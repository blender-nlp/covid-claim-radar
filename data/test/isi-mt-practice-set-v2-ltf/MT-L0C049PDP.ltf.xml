<LCTL_TEXT lang="ukr">
<DOC grammar="none" id="L0C049PDP" lang="ukr" raw_text_char_length="12799" raw_text_md5="0e61747cfa4ea1307553feac6bf2bd77" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="89" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Israeli Startup Claims SARS-Cov-2 Escaped from Wuhan Lab During Gain-of-Function Research</ORIGINAL_TEXT>
<TOKEN end_char="7" id="token-0-0" morph="none" pos="word" start_char="1">Israeli</TOKEN>
<TOKEN end_char="15" id="token-0-1" morph="none" pos="word" start_char="9">Startup</TOKEN>
<TOKEN end_char="22" id="token-0-2" morph="none" pos="word" start_char="17">Claims</TOKEN>
<TOKEN end_char="33" id="token-0-3" morph="none" pos="unknown" start_char="24">SARS-Cov-2</TOKEN>
<TOKEN end_char="41" id="token-0-4" morph="none" pos="word" start_char="35">Escaped</TOKEN>
<TOKEN end_char="46" id="token-0-5" morph="none" pos="word" start_char="43">from</TOKEN>
<TOKEN end_char="52" id="token-0-6" morph="none" pos="word" start_char="48">Wuhan</TOKEN>
<TOKEN end_char="56" id="token-0-7" morph="none" pos="word" start_char="54">Lab</TOKEN>
<TOKEN end_char="63" id="token-0-8" morph="none" pos="word" start_char="58">During</TOKEN>
<TOKEN end_char="80" id="token-0-9" morph="none" pos="unknown" start_char="65">Gain-of-Function</TOKEN>
<TOKEN end_char="89" id="token-0-10" morph="none" pos="word" start_char="82">Research</TOKEN>
</SEG>
<SEG end_char="287" id="segment-1" start_char="93">
<ORIGINAL_TEXT>An Israeli startup named Rootclaim has determined that there is an 81% chance that the SARS-cov-2 was accidentally released from the Wuhan bioresearch laboratory during gain-of-function research.</ORIGINAL_TEXT>
<TOKEN end_char="94" id="token-1-0" morph="none" pos="word" start_char="93">An</TOKEN>
<TOKEN end_char="102" id="token-1-1" morph="none" pos="word" start_char="96">Israeli</TOKEN>
<TOKEN end_char="110" id="token-1-2" morph="none" pos="word" start_char="104">startup</TOKEN>
<TOKEN end_char="116" id="token-1-3" morph="none" pos="word" start_char="112">named</TOKEN>
<TOKEN end_char="126" id="token-1-4" morph="none" pos="word" start_char="118">Rootclaim</TOKEN>
<TOKEN end_char="130" id="token-1-5" morph="none" pos="word" start_char="128">has</TOKEN>
<TOKEN end_char="141" id="token-1-6" morph="none" pos="word" start_char="132">determined</TOKEN>
<TOKEN end_char="146" id="token-1-7" morph="none" pos="word" start_char="143">that</TOKEN>
<TOKEN end_char="152" id="token-1-8" morph="none" pos="word" start_char="148">there</TOKEN>
<TOKEN end_char="155" id="token-1-9" morph="none" pos="word" start_char="154">is</TOKEN>
<TOKEN end_char="158" id="token-1-10" morph="none" pos="word" start_char="157">an</TOKEN>
<TOKEN end_char="161" id="token-1-11" morph="none" pos="word" start_char="160">81</TOKEN>
<TOKEN end_char="162" id="token-1-12" morph="none" pos="punct" start_char="162">%</TOKEN>
<TOKEN end_char="169" id="token-1-13" morph="none" pos="word" start_char="164">chance</TOKEN>
<TOKEN end_char="174" id="token-1-14" morph="none" pos="word" start_char="171">that</TOKEN>
<TOKEN end_char="178" id="token-1-15" morph="none" pos="word" start_char="176">the</TOKEN>
<TOKEN end_char="189" id="token-1-16" morph="none" pos="unknown" start_char="180">SARS-cov-2</TOKEN>
<TOKEN end_char="193" id="token-1-17" morph="none" pos="word" start_char="191">was</TOKEN>
<TOKEN end_char="206" id="token-1-18" morph="none" pos="word" start_char="195">accidentally</TOKEN>
<TOKEN end_char="215" id="token-1-19" morph="none" pos="word" start_char="208">released</TOKEN>
<TOKEN end_char="220" id="token-1-20" morph="none" pos="word" start_char="217">from</TOKEN>
<TOKEN end_char="224" id="token-1-21" morph="none" pos="word" start_char="222">the</TOKEN>
<TOKEN end_char="230" id="token-1-22" morph="none" pos="word" start_char="226">Wuhan</TOKEN>
<TOKEN end_char="242" id="token-1-23" morph="none" pos="word" start_char="232">bioresearch</TOKEN>
<TOKEN end_char="253" id="token-1-24" morph="none" pos="word" start_char="244">laboratory</TOKEN>
<TOKEN end_char="260" id="token-1-25" morph="none" pos="word" start_char="255">during</TOKEN>
<TOKEN end_char="277" id="token-1-26" morph="none" pos="unknown" start_char="262">gain-of-function</TOKEN>
<TOKEN end_char="286" id="token-1-27" morph="none" pos="word" start_char="279">research</TOKEN>
<TOKEN end_char="287" id="token-1-28" morph="none" pos="punct" start_char="287">.</TOKEN>
</SEG>
<SEG end_char="353" id="segment-2" start_char="290">
<ORIGINAL_TEXT>Rootclaim is, i'm assuming, an ai driven service that does this:</ORIGINAL_TEXT>
<TOKEN end_char="298" id="token-2-0" morph="none" pos="word" start_char="290">Rootclaim</TOKEN>
<TOKEN end_char="301" id="token-2-1" morph="none" pos="word" start_char="300">is</TOKEN>
<TOKEN end_char="302" id="token-2-2" morph="none" pos="punct" start_char="302">,</TOKEN>
<TOKEN end_char="306" id="token-2-3" morph="none" pos="word" start_char="304">i'm</TOKEN>
<TOKEN end_char="315" id="token-2-4" morph="none" pos="word" start_char="308">assuming</TOKEN>
<TOKEN end_char="316" id="token-2-5" morph="none" pos="punct" start_char="316">,</TOKEN>
<TOKEN end_char="319" id="token-2-6" morph="none" pos="word" start_char="318">an</TOKEN>
<TOKEN end_char="322" id="token-2-7" morph="none" pos="word" start_char="321">ai</TOKEN>
<TOKEN end_char="329" id="token-2-8" morph="none" pos="word" start_char="324">driven</TOKEN>
<TOKEN end_char="337" id="token-2-9" morph="none" pos="word" start_char="331">service</TOKEN>
<TOKEN end_char="342" id="token-2-10" morph="none" pos="word" start_char="339">that</TOKEN>
<TOKEN end_char="347" id="token-2-11" morph="none" pos="word" start_char="344">does</TOKEN>
<TOKEN end_char="352" id="token-2-12" morph="none" pos="word" start_char="349">this</TOKEN>
<TOKEN end_char="353" id="token-2-13" morph="none" pos="punct" start_char="353">:</TOKEN>
</SEG>
<SEG end_char="451" id="segment-3" start_char="356">
<ORIGINAL_TEXT>Rootclaim outperforms human reasoning by correcting for the biases and flaws of human intuition.</ORIGINAL_TEXT>
<TOKEN end_char="364" id="token-3-0" morph="none" pos="word" start_char="356">Rootclaim</TOKEN>
<TOKEN end_char="376" id="token-3-1" morph="none" pos="word" start_char="366">outperforms</TOKEN>
<TOKEN end_char="382" id="token-3-2" morph="none" pos="word" start_char="378">human</TOKEN>
<TOKEN end_char="392" id="token-3-3" morph="none" pos="word" start_char="384">reasoning</TOKEN>
<TOKEN end_char="395" id="token-3-4" morph="none" pos="word" start_char="394">by</TOKEN>
<TOKEN end_char="406" id="token-3-5" morph="none" pos="word" start_char="397">correcting</TOKEN>
<TOKEN end_char="410" id="token-3-6" morph="none" pos="word" start_char="408">for</TOKEN>
<TOKEN end_char="414" id="token-3-7" morph="none" pos="word" start_char="412">the</TOKEN>
<TOKEN end_char="421" id="token-3-8" morph="none" pos="word" start_char="416">biases</TOKEN>
<TOKEN end_char="425" id="token-3-9" morph="none" pos="word" start_char="423">and</TOKEN>
<TOKEN end_char="431" id="token-3-10" morph="none" pos="word" start_char="427">flaws</TOKEN>
<TOKEN end_char="434" id="token-3-11" morph="none" pos="word" start_char="433">of</TOKEN>
<TOKEN end_char="440" id="token-3-12" morph="none" pos="word" start_char="436">human</TOKEN>
<TOKEN end_char="450" id="token-3-13" morph="none" pos="word" start_char="442">intuition</TOKEN>
<TOKEN end_char="451" id="token-3-14" morph="none" pos="punct" start_char="451">.</TOKEN>
</SEG>
<SEG end_char="624" id="segment-4" start_char="453">
<ORIGINAL_TEXT>The platform integrates all available evidence, assesses it for credibility and uses probabilistic models to reach conclusions about the likelihood of competing hypotheses.</ORIGINAL_TEXT>
<TOKEN end_char="455" id="token-4-0" morph="none" pos="word" start_char="453">The</TOKEN>
<TOKEN end_char="464" id="token-4-1" morph="none" pos="word" start_char="457">platform</TOKEN>
<TOKEN end_char="475" id="token-4-2" morph="none" pos="word" start_char="466">integrates</TOKEN>
<TOKEN end_char="479" id="token-4-3" morph="none" pos="word" start_char="477">all</TOKEN>
<TOKEN end_char="489" id="token-4-4" morph="none" pos="word" start_char="481">available</TOKEN>
<TOKEN end_char="498" id="token-4-5" morph="none" pos="word" start_char="491">evidence</TOKEN>
<TOKEN end_char="499" id="token-4-6" morph="none" pos="punct" start_char="499">,</TOKEN>
<TOKEN end_char="508" id="token-4-7" morph="none" pos="word" start_char="501">assesses</TOKEN>
<TOKEN end_char="511" id="token-4-8" morph="none" pos="word" start_char="510">it</TOKEN>
<TOKEN end_char="515" id="token-4-9" morph="none" pos="word" start_char="513">for</TOKEN>
<TOKEN end_char="527" id="token-4-10" morph="none" pos="word" start_char="517">credibility</TOKEN>
<TOKEN end_char="531" id="token-4-11" morph="none" pos="word" start_char="529">and</TOKEN>
<TOKEN end_char="536" id="token-4-12" morph="none" pos="word" start_char="533">uses</TOKEN>
<TOKEN end_char="550" id="token-4-13" morph="none" pos="word" start_char="538">probabilistic</TOKEN>
<TOKEN end_char="557" id="token-4-14" morph="none" pos="word" start_char="552">models</TOKEN>
<TOKEN end_char="560" id="token-4-15" morph="none" pos="word" start_char="559">to</TOKEN>
<TOKEN end_char="566" id="token-4-16" morph="none" pos="word" start_char="562">reach</TOKEN>
<TOKEN end_char="578" id="token-4-17" morph="none" pos="word" start_char="568">conclusions</TOKEN>
<TOKEN end_char="584" id="token-4-18" morph="none" pos="word" start_char="580">about</TOKEN>
<TOKEN end_char="588" id="token-4-19" morph="none" pos="word" start_char="586">the</TOKEN>
<TOKEN end_char="599" id="token-4-20" morph="none" pos="word" start_char="590">likelihood</TOKEN>
<TOKEN end_char="602" id="token-4-21" morph="none" pos="word" start_char="601">of</TOKEN>
<TOKEN end_char="612" id="token-4-22" morph="none" pos="word" start_char="604">competing</TOKEN>
<TOKEN end_char="623" id="token-4-23" morph="none" pos="word" start_char="614">hypotheses</TOKEN>
<TOKEN end_char="624" id="token-4-24" morph="none" pos="punct" start_char="624">.</TOKEN>
</SEG>
<SEG end_char="730" id="segment-5" start_char="626">
<ORIGINAL_TEXT>Its conclusions represent the best available understanding of the complexity and uncertainty in our world</ORIGINAL_TEXT>
<TOKEN end_char="628" id="token-5-0" morph="none" pos="word" start_char="626">Its</TOKEN>
<TOKEN end_char="640" id="token-5-1" morph="none" pos="word" start_char="630">conclusions</TOKEN>
<TOKEN end_char="650" id="token-5-2" morph="none" pos="word" start_char="642">represent</TOKEN>
<TOKEN end_char="654" id="token-5-3" morph="none" pos="word" start_char="652">the</TOKEN>
<TOKEN end_char="659" id="token-5-4" morph="none" pos="word" start_char="656">best</TOKEN>
<TOKEN end_char="669" id="token-5-5" morph="none" pos="word" start_char="661">available</TOKEN>
<TOKEN end_char="683" id="token-5-6" morph="none" pos="word" start_char="671">understanding</TOKEN>
<TOKEN end_char="686" id="token-5-7" morph="none" pos="word" start_char="685">of</TOKEN>
<TOKEN end_char="690" id="token-5-8" morph="none" pos="word" start_char="688">the</TOKEN>
<TOKEN end_char="701" id="token-5-9" morph="none" pos="word" start_char="692">complexity</TOKEN>
<TOKEN end_char="705" id="token-5-10" morph="none" pos="word" start_char="703">and</TOKEN>
<TOKEN end_char="717" id="token-5-11" morph="none" pos="word" start_char="707">uncertainty</TOKEN>
<TOKEN end_char="720" id="token-5-12" morph="none" pos="word" start_char="719">in</TOKEN>
<TOKEN end_char="724" id="token-5-13" morph="none" pos="word" start_char="722">our</TOKEN>
<TOKEN end_char="730" id="token-5-14" morph="none" pos="word" start_char="726">world</TOKEN>
</SEG>
<SEG end_char="754" id="segment-6" start_char="733">
<ORIGINAL_TEXT>According to rootclaim</ORIGINAL_TEXT>
<TOKEN end_char="741" id="token-6-0" morph="none" pos="word" start_char="733">According</TOKEN>
<TOKEN end_char="744" id="token-6-1" morph="none" pos="word" start_char="743">to</TOKEN>
<TOKEN end_char="754" id="token-6-2" morph="none" pos="word" start_char="746">rootclaim</TOKEN>
</SEG>
<SEG end_char="776" id="segment-7" start_char="757">
<ORIGINAL_TEXT>www.rootclaim.com...</ORIGINAL_TEXT>
<TOKEN end_char="776" id="token-7-0" morph="none" pos="url" start_char="757">www.rootclaim.com...</TOKEN>
</SEG>
<SEG end_char="864" id="segment-8" start_char="779">
<ORIGINAL_TEXT>The virus was developed during gain-of-function research and was released by accident.</ORIGINAL_TEXT>
<TOKEN end_char="781" id="token-8-0" morph="none" pos="word" start_char="779">The</TOKEN>
<TOKEN end_char="787" id="token-8-1" morph="none" pos="word" start_char="783">virus</TOKEN>
<TOKEN end_char="791" id="token-8-2" morph="none" pos="word" start_char="789">was</TOKEN>
<TOKEN end_char="801" id="token-8-3" morph="none" pos="word" start_char="793">developed</TOKEN>
<TOKEN end_char="808" id="token-8-4" morph="none" pos="word" start_char="803">during</TOKEN>
<TOKEN end_char="825" id="token-8-5" morph="none" pos="unknown" start_char="810">gain-of-function</TOKEN>
<TOKEN end_char="834" id="token-8-6" morph="none" pos="word" start_char="827">research</TOKEN>
<TOKEN end_char="838" id="token-8-7" morph="none" pos="word" start_char="836">and</TOKEN>
<TOKEN end_char="842" id="token-8-8" morph="none" pos="word" start_char="840">was</TOKEN>
<TOKEN end_char="851" id="token-8-9" morph="none" pos="word" start_char="844">released</TOKEN>
<TOKEN end_char="854" id="token-8-10" morph="none" pos="word" start_char="853">by</TOKEN>
<TOKEN end_char="863" id="token-8-11" morph="none" pos="word" start_char="856">accident</TOKEN>
<TOKEN end_char="864" id="token-8-12" morph="none" pos="punct" start_char="864">.</TOKEN>
</SEG>
<SEG end_char="1009" id="segment-9" start_char="866">
<ORIGINAL_TEXT>(81% probability) Hypotheses Considered 1 81% Lab escape: The virus was developed during gain-of-function research and was released by accident.</ORIGINAL_TEXT>
<TOKEN end_char="866" id="token-9-0" morph="none" pos="punct" start_char="866">(</TOKEN>
<TOKEN end_char="868" id="token-9-1" morph="none" pos="word" start_char="867">81</TOKEN>
<TOKEN end_char="869" id="token-9-2" morph="none" pos="punct" start_char="869">%</TOKEN>
<TOKEN end_char="881" id="token-9-3" morph="none" pos="word" start_char="871">probability</TOKEN>
<TOKEN end_char="882" id="token-9-4" morph="none" pos="punct" start_char="882">)</TOKEN>
<TOKEN end_char="893" id="token-9-5" morph="none" pos="word" start_char="884">Hypotheses</TOKEN>
<TOKEN end_char="904" id="token-9-6" morph="none" pos="word" start_char="895">Considered</TOKEN>
<TOKEN end_char="906" id="token-9-7" morph="none" pos="word" start_char="906">1</TOKEN>
<TOKEN end_char="909" id="token-9-8" morph="none" pos="word" start_char="908">81</TOKEN>
<TOKEN end_char="910" id="token-9-9" morph="none" pos="punct" start_char="910">%</TOKEN>
<TOKEN end_char="914" id="token-9-10" morph="none" pos="word" start_char="912">Lab</TOKEN>
<TOKEN end_char="921" id="token-9-11" morph="none" pos="word" start_char="916">escape</TOKEN>
<TOKEN end_char="922" id="token-9-12" morph="none" pos="punct" start_char="922">:</TOKEN>
<TOKEN end_char="926" id="token-9-13" morph="none" pos="word" start_char="924">The</TOKEN>
<TOKEN end_char="932" id="token-9-14" morph="none" pos="word" start_char="928">virus</TOKEN>
<TOKEN end_char="936" id="token-9-15" morph="none" pos="word" start_char="934">was</TOKEN>
<TOKEN end_char="946" id="token-9-16" morph="none" pos="word" start_char="938">developed</TOKEN>
<TOKEN end_char="953" id="token-9-17" morph="none" pos="word" start_char="948">during</TOKEN>
<TOKEN end_char="970" id="token-9-18" morph="none" pos="unknown" start_char="955">gain-of-function</TOKEN>
<TOKEN end_char="979" id="token-9-19" morph="none" pos="word" start_char="972">research</TOKEN>
<TOKEN end_char="983" id="token-9-20" morph="none" pos="word" start_char="981">and</TOKEN>
<TOKEN end_char="987" id="token-9-21" morph="none" pos="word" start_char="985">was</TOKEN>
<TOKEN end_char="996" id="token-9-22" morph="none" pos="word" start_char="989">released</TOKEN>
<TOKEN end_char="999" id="token-9-23" morph="none" pos="word" start_char="998">by</TOKEN>
<TOKEN end_char="1008" id="token-9-24" morph="none" pos="word" start_char="1001">accident</TOKEN>
<TOKEN end_char="1009" id="token-9-25" morph="none" pos="punct" start_char="1009">.</TOKEN>
</SEG>
<SEG end_char="1097" id="segment-10" start_char="1011">
<ORIGINAL_TEXT>2 16% Zoonotic: The virus evolved in nature and was transmitted to humans zoonotically.</ORIGINAL_TEXT>
<TOKEN end_char="1011" id="token-10-0" morph="none" pos="word" start_char="1011">2</TOKEN>
<TOKEN end_char="1014" id="token-10-1" morph="none" pos="word" start_char="1013">16</TOKEN>
<TOKEN end_char="1015" id="token-10-2" morph="none" pos="punct" start_char="1015">%</TOKEN>
<TOKEN end_char="1024" id="token-10-3" morph="none" pos="word" start_char="1017">Zoonotic</TOKEN>
<TOKEN end_char="1025" id="token-10-4" morph="none" pos="punct" start_char="1025">:</TOKEN>
<TOKEN end_char="1029" id="token-10-5" morph="none" pos="word" start_char="1027">The</TOKEN>
<TOKEN end_char="1035" id="token-10-6" morph="none" pos="word" start_char="1031">virus</TOKEN>
<TOKEN end_char="1043" id="token-10-7" morph="none" pos="word" start_char="1037">evolved</TOKEN>
<TOKEN end_char="1046" id="token-10-8" morph="none" pos="word" start_char="1045">in</TOKEN>
<TOKEN end_char="1053" id="token-10-9" morph="none" pos="word" start_char="1048">nature</TOKEN>
<TOKEN end_char="1057" id="token-10-10" morph="none" pos="word" start_char="1055">and</TOKEN>
<TOKEN end_char="1061" id="token-10-11" morph="none" pos="word" start_char="1059">was</TOKEN>
<TOKEN end_char="1073" id="token-10-12" morph="none" pos="word" start_char="1063">transmitted</TOKEN>
<TOKEN end_char="1076" id="token-10-13" morph="none" pos="word" start_char="1075">to</TOKEN>
<TOKEN end_char="1083" id="token-10-14" morph="none" pos="word" start_char="1078">humans</TOKEN>
<TOKEN end_char="1096" id="token-10-15" morph="none" pos="word" start_char="1085">zoonotically</TOKEN>
<TOKEN end_char="1097" id="token-10-16" morph="none" pos="punct" start_char="1097">.</TOKEN>
</SEG>
<SEG end_char="1198" id="segment-11" start_char="1099">
<ORIGINAL_TEXT>3 2.8% Bioweapon: The virus was genetically engineered as a bioweapon and was deliberately released.</ORIGINAL_TEXT>
<TOKEN end_char="1099" id="token-11-0" morph="none" pos="word" start_char="1099">3</TOKEN>
<TOKEN end_char="1103" id="token-11-1" morph="none" pos="unknown" start_char="1101">2.8</TOKEN>
<TOKEN end_char="1104" id="token-11-2" morph="none" pos="punct" start_char="1104">%</TOKEN>
<TOKEN end_char="1114" id="token-11-3" morph="none" pos="word" start_char="1106">Bioweapon</TOKEN>
<TOKEN end_char="1115" id="token-11-4" morph="none" pos="punct" start_char="1115">:</TOKEN>
<TOKEN end_char="1119" id="token-11-5" morph="none" pos="word" start_char="1117">The</TOKEN>
<TOKEN end_char="1125" id="token-11-6" morph="none" pos="word" start_char="1121">virus</TOKEN>
<TOKEN end_char="1129" id="token-11-7" morph="none" pos="word" start_char="1127">was</TOKEN>
<TOKEN end_char="1141" id="token-11-8" morph="none" pos="word" start_char="1131">genetically</TOKEN>
<TOKEN end_char="1152" id="token-11-9" morph="none" pos="word" start_char="1143">engineered</TOKEN>
<TOKEN end_char="1155" id="token-11-10" morph="none" pos="word" start_char="1154">as</TOKEN>
<TOKEN end_char="1157" id="token-11-11" morph="none" pos="word" start_char="1157">a</TOKEN>
<TOKEN end_char="1167" id="token-11-12" morph="none" pos="word" start_char="1159">bioweapon</TOKEN>
<TOKEN end_char="1171" id="token-11-13" morph="none" pos="word" start_char="1169">and</TOKEN>
<TOKEN end_char="1175" id="token-11-14" morph="none" pos="word" start_char="1173">was</TOKEN>
<TOKEN end_char="1188" id="token-11-15" morph="none" pos="word" start_char="1177">deliberately</TOKEN>
<TOKEN end_char="1197" id="token-11-16" morph="none" pos="word" start_char="1190">released</TOKEN>
<TOKEN end_char="1198" id="token-11-17" morph="none" pos="punct" start_char="1198">.</TOKEN>
</SEG>
<SEG end_char="1333" id="segment-12" start_char="1200">
<ORIGINAL_TEXT>When a novel coronavirus was first identified in late 2019, the assumption was that, like most epidemics, it was of a zoonotic source.</ORIGINAL_TEXT>
<TOKEN end_char="1203" id="token-12-0" morph="none" pos="word" start_char="1200">When</TOKEN>
<TOKEN end_char="1205" id="token-12-1" morph="none" pos="word" start_char="1205">a</TOKEN>
<TOKEN end_char="1211" id="token-12-2" morph="none" pos="word" start_char="1207">novel</TOKEN>
<TOKEN end_char="1223" id="token-12-3" morph="none" pos="word" start_char="1213">coronavirus</TOKEN>
<TOKEN end_char="1227" id="token-12-4" morph="none" pos="word" start_char="1225">was</TOKEN>
<TOKEN end_char="1233" id="token-12-5" morph="none" pos="word" start_char="1229">first</TOKEN>
<TOKEN end_char="1244" id="token-12-6" morph="none" pos="word" start_char="1235">identified</TOKEN>
<TOKEN end_char="1247" id="token-12-7" morph="none" pos="word" start_char="1246">in</TOKEN>
<TOKEN end_char="1252" id="token-12-8" morph="none" pos="word" start_char="1249">late</TOKEN>
<TOKEN end_char="1257" id="token-12-9" morph="none" pos="word" start_char="1254">2019</TOKEN>
<TOKEN end_char="1258" id="token-12-10" morph="none" pos="punct" start_char="1258">,</TOKEN>
<TOKEN end_char="1262" id="token-12-11" morph="none" pos="word" start_char="1260">the</TOKEN>
<TOKEN end_char="1273" id="token-12-12" morph="none" pos="word" start_char="1264">assumption</TOKEN>
<TOKEN end_char="1277" id="token-12-13" morph="none" pos="word" start_char="1275">was</TOKEN>
<TOKEN end_char="1282" id="token-12-14" morph="none" pos="word" start_char="1279">that</TOKEN>
<TOKEN end_char="1283" id="token-12-15" morph="none" pos="punct" start_char="1283">,</TOKEN>
<TOKEN end_char="1288" id="token-12-16" morph="none" pos="word" start_char="1285">like</TOKEN>
<TOKEN end_char="1293" id="token-12-17" morph="none" pos="word" start_char="1290">most</TOKEN>
<TOKEN end_char="1303" id="token-12-18" morph="none" pos="word" start_char="1295">epidemics</TOKEN>
<TOKEN end_char="1304" id="token-12-19" morph="none" pos="punct" start_char="1304">,</TOKEN>
<TOKEN end_char="1307" id="token-12-20" morph="none" pos="word" start_char="1306">it</TOKEN>
<TOKEN end_char="1311" id="token-12-21" morph="none" pos="word" start_char="1309">was</TOKEN>
<TOKEN end_char="1314" id="token-12-22" morph="none" pos="word" start_char="1313">of</TOKEN>
<TOKEN end_char="1316" id="token-12-23" morph="none" pos="word" start_char="1316">a</TOKEN>
<TOKEN end_char="1325" id="token-12-24" morph="none" pos="word" start_char="1318">zoonotic</TOKEN>
<TOKEN end_char="1332" id="token-12-25" morph="none" pos="word" start_char="1327">source</TOKEN>
<TOKEN end_char="1333" id="token-12-26" morph="none" pos="punct" start_char="1333">.</TOKEN>
</SEG>
<SEG end_char="1464" id="segment-13" start_char="1335">
<ORIGINAL_TEXT>A few studies, including one published in the prestigious Nature magazine, concluded that the virus is not a laboratory construct.</ORIGINAL_TEXT>
<TOKEN end_char="1335" id="token-13-0" morph="none" pos="word" start_char="1335">A</TOKEN>
<TOKEN end_char="1339" id="token-13-1" morph="none" pos="word" start_char="1337">few</TOKEN>
<TOKEN end_char="1347" id="token-13-2" morph="none" pos="word" start_char="1341">studies</TOKEN>
<TOKEN end_char="1348" id="token-13-3" morph="none" pos="punct" start_char="1348">,</TOKEN>
<TOKEN end_char="1358" id="token-13-4" morph="none" pos="word" start_char="1350">including</TOKEN>
<TOKEN end_char="1362" id="token-13-5" morph="none" pos="word" start_char="1360">one</TOKEN>
<TOKEN end_char="1372" id="token-13-6" morph="none" pos="word" start_char="1364">published</TOKEN>
<TOKEN end_char="1375" id="token-13-7" morph="none" pos="word" start_char="1374">in</TOKEN>
<TOKEN end_char="1379" id="token-13-8" morph="none" pos="word" start_char="1377">the</TOKEN>
<TOKEN end_char="1391" id="token-13-9" morph="none" pos="word" start_char="1381">prestigious</TOKEN>
<TOKEN end_char="1398" id="token-13-10" morph="none" pos="word" start_char="1393">Nature</TOKEN>
<TOKEN end_char="1407" id="token-13-11" morph="none" pos="word" start_char="1400">magazine</TOKEN>
<TOKEN end_char="1408" id="token-13-12" morph="none" pos="punct" start_char="1408">,</TOKEN>
<TOKEN end_char="1418" id="token-13-13" morph="none" pos="word" start_char="1410">concluded</TOKEN>
<TOKEN end_char="1423" id="token-13-14" morph="none" pos="word" start_char="1420">that</TOKEN>
<TOKEN end_char="1427" id="token-13-15" morph="none" pos="word" start_char="1425">the</TOKEN>
<TOKEN end_char="1433" id="token-13-16" morph="none" pos="word" start_char="1429">virus</TOKEN>
<TOKEN end_char="1436" id="token-13-17" morph="none" pos="word" start_char="1435">is</TOKEN>
<TOKEN end_char="1440" id="token-13-18" morph="none" pos="word" start_char="1438">not</TOKEN>
<TOKEN end_char="1442" id="token-13-19" morph="none" pos="word" start_char="1442">a</TOKEN>
<TOKEN end_char="1453" id="token-13-20" morph="none" pos="word" start_char="1444">laboratory</TOKEN>
<TOKEN end_char="1463" id="token-13-21" morph="none" pos="word" start_char="1455">construct</TOKEN>
<TOKEN end_char="1464" id="token-13-22" morph="none" pos="punct" start_char="1464">.</TOKEN>
</SEG>
<SEG end_char="1636" id="segment-14" start_char="1466">
<ORIGINAL_TEXT>Today, claiming a non-zoonotic origin is widely considered a conspiracy theory, and indeed many such claims are easily refutable without requiring probabilistic inference.</ORIGINAL_TEXT>
<TOKEN end_char="1470" id="token-14-0" morph="none" pos="word" start_char="1466">Today</TOKEN>
<TOKEN end_char="1471" id="token-14-1" morph="none" pos="punct" start_char="1471">,</TOKEN>
<TOKEN end_char="1480" id="token-14-2" morph="none" pos="word" start_char="1473">claiming</TOKEN>
<TOKEN end_char="1482" id="token-14-3" morph="none" pos="word" start_char="1482">a</TOKEN>
<TOKEN end_char="1495" id="token-14-4" morph="none" pos="unknown" start_char="1484">non-zoonotic</TOKEN>
<TOKEN end_char="1502" id="token-14-5" morph="none" pos="word" start_char="1497">origin</TOKEN>
<TOKEN end_char="1505" id="token-14-6" morph="none" pos="word" start_char="1504">is</TOKEN>
<TOKEN end_char="1512" id="token-14-7" morph="none" pos="word" start_char="1507">widely</TOKEN>
<TOKEN end_char="1523" id="token-14-8" morph="none" pos="word" start_char="1514">considered</TOKEN>
<TOKEN end_char="1525" id="token-14-9" morph="none" pos="word" start_char="1525">a</TOKEN>
<TOKEN end_char="1536" id="token-14-10" morph="none" pos="word" start_char="1527">conspiracy</TOKEN>
<TOKEN end_char="1543" id="token-14-11" morph="none" pos="word" start_char="1538">theory</TOKEN>
<TOKEN end_char="1544" id="token-14-12" morph="none" pos="punct" start_char="1544">,</TOKEN>
<TOKEN end_char="1548" id="token-14-13" morph="none" pos="word" start_char="1546">and</TOKEN>
<TOKEN end_char="1555" id="token-14-14" morph="none" pos="word" start_char="1550">indeed</TOKEN>
<TOKEN end_char="1560" id="token-14-15" morph="none" pos="word" start_char="1557">many</TOKEN>
<TOKEN end_char="1565" id="token-14-16" morph="none" pos="word" start_char="1562">such</TOKEN>
<TOKEN end_char="1572" id="token-14-17" morph="none" pos="word" start_char="1567">claims</TOKEN>
<TOKEN end_char="1576" id="token-14-18" morph="none" pos="word" start_char="1574">are</TOKEN>
<TOKEN end_char="1583" id="token-14-19" morph="none" pos="word" start_char="1578">easily</TOKEN>
<TOKEN end_char="1593" id="token-14-20" morph="none" pos="word" start_char="1585">refutable</TOKEN>
<TOKEN end_char="1601" id="token-14-21" morph="none" pos="word" start_char="1595">without</TOKEN>
<TOKEN end_char="1611" id="token-14-22" morph="none" pos="word" start_char="1603">requiring</TOKEN>
<TOKEN end_char="1625" id="token-14-23" morph="none" pos="word" start_char="1613">probabilistic</TOKEN>
<TOKEN end_char="1635" id="token-14-24" morph="none" pos="word" start_char="1627">inference</TOKEN>
<TOKEN end_char="1636" id="token-14-25" morph="none" pos="punct" start_char="1636">.</TOKEN>
</SEG>
<SEG end_char="1852" id="segment-15" start_char="1638">
<ORIGINAL_TEXT>However, the possibility of a lab escape does require serious examination, especially when considering the proximity of the source to a major coronavirus lab and several unusual findings in the genome of SARS-CoV-2.</ORIGINAL_TEXT>
<TOKEN end_char="1644" id="token-15-0" morph="none" pos="word" start_char="1638">However</TOKEN>
<TOKEN end_char="1645" id="token-15-1" morph="none" pos="punct" start_char="1645">,</TOKEN>
<TOKEN end_char="1649" id="token-15-2" morph="none" pos="word" start_char="1647">the</TOKEN>
<TOKEN end_char="1661" id="token-15-3" morph="none" pos="word" start_char="1651">possibility</TOKEN>
<TOKEN end_char="1664" id="token-15-4" morph="none" pos="word" start_char="1663">of</TOKEN>
<TOKEN end_char="1666" id="token-15-5" morph="none" pos="word" start_char="1666">a</TOKEN>
<TOKEN end_char="1670" id="token-15-6" morph="none" pos="word" start_char="1668">lab</TOKEN>
<TOKEN end_char="1677" id="token-15-7" morph="none" pos="word" start_char="1672">escape</TOKEN>
<TOKEN end_char="1682" id="token-15-8" morph="none" pos="word" start_char="1679">does</TOKEN>
<TOKEN end_char="1690" id="token-15-9" morph="none" pos="word" start_char="1684">require</TOKEN>
<TOKEN end_char="1698" id="token-15-10" morph="none" pos="word" start_char="1692">serious</TOKEN>
<TOKEN end_char="1710" id="token-15-11" morph="none" pos="word" start_char="1700">examination</TOKEN>
<TOKEN end_char="1711" id="token-15-12" morph="none" pos="punct" start_char="1711">,</TOKEN>
<TOKEN end_char="1722" id="token-15-13" morph="none" pos="word" start_char="1713">especially</TOKEN>
<TOKEN end_char="1727" id="token-15-14" morph="none" pos="word" start_char="1724">when</TOKEN>
<TOKEN end_char="1739" id="token-15-15" morph="none" pos="word" start_char="1729">considering</TOKEN>
<TOKEN end_char="1743" id="token-15-16" morph="none" pos="word" start_char="1741">the</TOKEN>
<TOKEN end_char="1753" id="token-15-17" morph="none" pos="word" start_char="1745">proximity</TOKEN>
<TOKEN end_char="1756" id="token-15-18" morph="none" pos="word" start_char="1755">of</TOKEN>
<TOKEN end_char="1760" id="token-15-19" morph="none" pos="word" start_char="1758">the</TOKEN>
<TOKEN end_char="1767" id="token-15-20" morph="none" pos="word" start_char="1762">source</TOKEN>
<TOKEN end_char="1770" id="token-15-21" morph="none" pos="word" start_char="1769">to</TOKEN>
<TOKEN end_char="1772" id="token-15-22" morph="none" pos="word" start_char="1772">a</TOKEN>
<TOKEN end_char="1778" id="token-15-23" morph="none" pos="word" start_char="1774">major</TOKEN>
<TOKEN end_char="1790" id="token-15-24" morph="none" pos="word" start_char="1780">coronavirus</TOKEN>
<TOKEN end_char="1794" id="token-15-25" morph="none" pos="word" start_char="1792">lab</TOKEN>
<TOKEN end_char="1798" id="token-15-26" morph="none" pos="word" start_char="1796">and</TOKEN>
<TOKEN end_char="1806" id="token-15-27" morph="none" pos="word" start_char="1800">several</TOKEN>
<TOKEN end_char="1814" id="token-15-28" morph="none" pos="word" start_char="1808">unusual</TOKEN>
<TOKEN end_char="1823" id="token-15-29" morph="none" pos="word" start_char="1816">findings</TOKEN>
<TOKEN end_char="1826" id="token-15-30" morph="none" pos="word" start_char="1825">in</TOKEN>
<TOKEN end_char="1830" id="token-15-31" morph="none" pos="word" start_char="1828">the</TOKEN>
<TOKEN end_char="1837" id="token-15-32" morph="none" pos="word" start_char="1832">genome</TOKEN>
<TOKEN end_char="1840" id="token-15-33" morph="none" pos="word" start_char="1839">of</TOKEN>
<TOKEN end_char="1851" id="token-15-34" morph="none" pos="unknown" start_char="1842">SARS-CoV-2</TOKEN>
<TOKEN end_char="1852" id="token-15-35" morph="none" pos="punct" start_char="1852">.</TOKEN>
</SEG>
<SEG end_char="2005" id="segment-16" start_char="1854">
<ORIGINAL_TEXT>Due to the complexities of weighing an unlikely lab origin against findings that are unlikely for a zoonotic source, a probabilistic analysis is needed.</ORIGINAL_TEXT>
<TOKEN end_char="1856" id="token-16-0" morph="none" pos="word" start_char="1854">Due</TOKEN>
<TOKEN end_char="1859" id="token-16-1" morph="none" pos="word" start_char="1858">to</TOKEN>
<TOKEN end_char="1863" id="token-16-2" morph="none" pos="word" start_char="1861">the</TOKEN>
<TOKEN end_char="1876" id="token-16-3" morph="none" pos="word" start_char="1865">complexities</TOKEN>
<TOKEN end_char="1879" id="token-16-4" morph="none" pos="word" start_char="1878">of</TOKEN>
<TOKEN end_char="1888" id="token-16-5" morph="none" pos="word" start_char="1881">weighing</TOKEN>
<TOKEN end_char="1891" id="token-16-6" morph="none" pos="word" start_char="1890">an</TOKEN>
<TOKEN end_char="1900" id="token-16-7" morph="none" pos="word" start_char="1893">unlikely</TOKEN>
<TOKEN end_char="1904" id="token-16-8" morph="none" pos="word" start_char="1902">lab</TOKEN>
<TOKEN end_char="1911" id="token-16-9" morph="none" pos="word" start_char="1906">origin</TOKEN>
<TOKEN end_char="1919" id="token-16-10" morph="none" pos="word" start_char="1913">against</TOKEN>
<TOKEN end_char="1928" id="token-16-11" morph="none" pos="word" start_char="1921">findings</TOKEN>
<TOKEN end_char="1933" id="token-16-12" morph="none" pos="word" start_char="1930">that</TOKEN>
<TOKEN end_char="1937" id="token-16-13" morph="none" pos="word" start_char="1935">are</TOKEN>
<TOKEN end_char="1946" id="token-16-14" morph="none" pos="word" start_char="1939">unlikely</TOKEN>
<TOKEN end_char="1950" id="token-16-15" morph="none" pos="word" start_char="1948">for</TOKEN>
<TOKEN end_char="1952" id="token-16-16" morph="none" pos="word" start_char="1952">a</TOKEN>
<TOKEN end_char="1961" id="token-16-17" morph="none" pos="word" start_char="1954">zoonotic</TOKEN>
<TOKEN end_char="1968" id="token-16-18" morph="none" pos="word" start_char="1963">source</TOKEN>
<TOKEN end_char="1969" id="token-16-19" morph="none" pos="punct" start_char="1969">,</TOKEN>
<TOKEN end_char="1971" id="token-16-20" morph="none" pos="word" start_char="1971">a</TOKEN>
<TOKEN end_char="1985" id="token-16-21" morph="none" pos="word" start_char="1973">probabilistic</TOKEN>
<TOKEN end_char="1994" id="token-16-22" morph="none" pos="word" start_char="1987">analysis</TOKEN>
<TOKEN end_char="1997" id="token-16-23" morph="none" pos="word" start_char="1996">is</TOKEN>
<TOKEN end_char="2004" id="token-16-24" morph="none" pos="word" start_char="1999">needed</TOKEN>
<TOKEN end_char="2005" id="token-16-25" morph="none" pos="punct" start_char="2005">.</TOKEN>
</SEG>
<SEG end_char="2087" id="segment-17" start_char="2008">
<ORIGINAL_TEXT>I'm sure this will likely be dismissed by the majority of media and many people.</ORIGINAL_TEXT>
<TOKEN end_char="2010" id="token-17-0" morph="none" pos="word" start_char="2008">I'm</TOKEN>
<TOKEN end_char="2015" id="token-17-1" morph="none" pos="word" start_char="2012">sure</TOKEN>
<TOKEN end_char="2020" id="token-17-2" morph="none" pos="word" start_char="2017">this</TOKEN>
<TOKEN end_char="2025" id="token-17-3" morph="none" pos="word" start_char="2022">will</TOKEN>
<TOKEN end_char="2032" id="token-17-4" morph="none" pos="word" start_char="2027">likely</TOKEN>
<TOKEN end_char="2035" id="token-17-5" morph="none" pos="word" start_char="2034">be</TOKEN>
<TOKEN end_char="2045" id="token-17-6" morph="none" pos="word" start_char="2037">dismissed</TOKEN>
<TOKEN end_char="2048" id="token-17-7" morph="none" pos="word" start_char="2047">by</TOKEN>
<TOKEN end_char="2052" id="token-17-8" morph="none" pos="word" start_char="2050">the</TOKEN>
<TOKEN end_char="2061" id="token-17-9" morph="none" pos="word" start_char="2054">majority</TOKEN>
<TOKEN end_char="2064" id="token-17-10" morph="none" pos="word" start_char="2063">of</TOKEN>
<TOKEN end_char="2070" id="token-17-11" morph="none" pos="word" start_char="2066">media</TOKEN>
<TOKEN end_char="2074" id="token-17-12" morph="none" pos="word" start_char="2072">and</TOKEN>
<TOKEN end_char="2079" id="token-17-13" morph="none" pos="word" start_char="2076">many</TOKEN>
<TOKEN end_char="2086" id="token-17-14" morph="none" pos="word" start_char="2081">people</TOKEN>
<TOKEN end_char="2087" id="token-17-15" morph="none" pos="punct" start_char="2087">.</TOKEN>
</SEG>
<SEG end_char="2135" id="segment-18" start_char="2089">
<ORIGINAL_TEXT>But it seems like it's worth looking more into.</ORIGINAL_TEXT>
<TOKEN end_char="2091" id="token-18-0" morph="none" pos="word" start_char="2089">But</TOKEN>
<TOKEN end_char="2094" id="token-18-1" morph="none" pos="word" start_char="2093">it</TOKEN>
<TOKEN end_char="2100" id="token-18-2" morph="none" pos="word" start_char="2096">seems</TOKEN>
<TOKEN end_char="2105" id="token-18-3" morph="none" pos="word" start_char="2102">like</TOKEN>
<TOKEN end_char="2110" id="token-18-4" morph="none" pos="word" start_char="2107">it's</TOKEN>
<TOKEN end_char="2116" id="token-18-5" morph="none" pos="word" start_char="2112">worth</TOKEN>
<TOKEN end_char="2124" id="token-18-6" morph="none" pos="word" start_char="2118">looking</TOKEN>
<TOKEN end_char="2129" id="token-18-7" morph="none" pos="word" start_char="2126">more</TOKEN>
<TOKEN end_char="2134" id="token-18-8" morph="none" pos="word" start_char="2131">into</TOKEN>
<TOKEN end_char="2135" id="token-18-9" morph="none" pos="punct" start_char="2135">.</TOKEN>
</SEG>
<SEG end_char="2202" id="segment-19" start_char="2137">
<ORIGINAL_TEXT>The link goes more into the evidence behind the eventual decision.</ORIGINAL_TEXT>
<TOKEN end_char="2139" id="token-19-0" morph="none" pos="word" start_char="2137">The</TOKEN>
<TOKEN end_char="2144" id="token-19-1" morph="none" pos="word" start_char="2141">link</TOKEN>
<TOKEN end_char="2149" id="token-19-2" morph="none" pos="word" start_char="2146">goes</TOKEN>
<TOKEN end_char="2154" id="token-19-3" morph="none" pos="word" start_char="2151">more</TOKEN>
<TOKEN end_char="2159" id="token-19-4" morph="none" pos="word" start_char="2156">into</TOKEN>
<TOKEN end_char="2163" id="token-19-5" morph="none" pos="word" start_char="2161">the</TOKEN>
<TOKEN end_char="2172" id="token-19-6" morph="none" pos="word" start_char="2165">evidence</TOKEN>
<TOKEN end_char="2179" id="token-19-7" morph="none" pos="word" start_char="2174">behind</TOKEN>
<TOKEN end_char="2183" id="token-19-8" morph="none" pos="word" start_char="2181">the</TOKEN>
<TOKEN end_char="2192" id="token-19-9" morph="none" pos="word" start_char="2185">eventual</TOKEN>
<TOKEN end_char="2201" id="token-19-10" morph="none" pos="word" start_char="2194">decision</TOKEN>
<TOKEN end_char="2202" id="token-19-11" morph="none" pos="punct" start_char="2202">.</TOKEN>
</SEG>
<SEG end_char="2252" id="segment-20" start_char="2205">
<ORIGINAL_TEXT>ETA: a bit of info on gain of function research.</ORIGINAL_TEXT>
<TOKEN end_char="2207" id="token-20-0" morph="none" pos="word" start_char="2205">ETA</TOKEN>
<TOKEN end_char="2208" id="token-20-1" morph="none" pos="punct" start_char="2208">:</TOKEN>
<TOKEN end_char="2210" id="token-20-2" morph="none" pos="word" start_char="2210">a</TOKEN>
<TOKEN end_char="2214" id="token-20-3" morph="none" pos="word" start_char="2212">bit</TOKEN>
<TOKEN end_char="2217" id="token-20-4" morph="none" pos="word" start_char="2216">of</TOKEN>
<TOKEN end_char="2222" id="token-20-5" morph="none" pos="word" start_char="2219">info</TOKEN>
<TOKEN end_char="2225" id="token-20-6" morph="none" pos="word" start_char="2224">on</TOKEN>
<TOKEN end_char="2230" id="token-20-7" morph="none" pos="word" start_char="2227">gain</TOKEN>
<TOKEN end_char="2233" id="token-20-8" morph="none" pos="word" start_char="2232">of</TOKEN>
<TOKEN end_char="2242" id="token-20-9" morph="none" pos="word" start_char="2235">function</TOKEN>
<TOKEN end_char="2251" id="token-20-10" morph="none" pos="word" start_char="2244">research</TOKEN>
<TOKEN end_char="2252" id="token-20-11" morph="none" pos="punct" start_char="2252">.</TOKEN>
</SEG>
<SEG end_char="2277" id="segment-21" start_char="2255">
<ORIGINAL_TEXT>www.ncbi.nlm.nih.gov...</ORIGINAL_TEXT>
<TOKEN end_char="2277" id="token-21-0" morph="none" pos="url" start_char="2255">www.ncbi.nlm.nih.gov...</TOKEN>
<TRANSLATED_TEXT>www.ncbi.nlm.nih.gov.</TRANSLATED_TEXT><DETECTED_LANGUAGE>hr</DETECTED_LANGUAGE></SEG>
<SEG end_char="2561" id="segment-22" start_char="2280">
<ORIGINAL_TEXT>Subbarao explained that routine virological methods involve experiments that aim to produce a gain of a desired function, such as higher yields for vaccine strains, but often also lead to loss of function, such as loss of the ability for a virus to replicate well, as a consequence.</ORIGINAL_TEXT>
<TOKEN end_char="2287" id="token-22-0" morph="none" pos="word" start_char="2280">Subbarao</TOKEN>
<TOKEN end_char="2297" id="token-22-1" morph="none" pos="word" start_char="2289">explained</TOKEN>
<TOKEN end_char="2302" id="token-22-2" morph="none" pos="word" start_char="2299">that</TOKEN>
<TOKEN end_char="2310" id="token-22-3" morph="none" pos="word" start_char="2304">routine</TOKEN>
<TOKEN end_char="2322" id="token-22-4" morph="none" pos="word" start_char="2312">virological</TOKEN>
<TOKEN end_char="2330" id="token-22-5" morph="none" pos="word" start_char="2324">methods</TOKEN>
<TOKEN end_char="2338" id="token-22-6" morph="none" pos="word" start_char="2332">involve</TOKEN>
<TOKEN end_char="2350" id="token-22-7" morph="none" pos="word" start_char="2340">experiments</TOKEN>
<TOKEN end_char="2355" id="token-22-8" morph="none" pos="word" start_char="2352">that</TOKEN>
<TOKEN end_char="2359" id="token-22-9" morph="none" pos="word" start_char="2357">aim</TOKEN>
<TOKEN end_char="2362" id="token-22-10" morph="none" pos="word" start_char="2361">to</TOKEN>
<TOKEN end_char="2370" id="token-22-11" morph="none" pos="word" start_char="2364">produce</TOKEN>
<TOKEN end_char="2372" id="token-22-12" morph="none" pos="word" start_char="2372">a</TOKEN>
<TOKEN end_char="2377" id="token-22-13" morph="none" pos="word" start_char="2374">gain</TOKEN>
<TOKEN end_char="2380" id="token-22-14" morph="none" pos="word" start_char="2379">of</TOKEN>
<TOKEN end_char="2382" id="token-22-15" morph="none" pos="word" start_char="2382">a</TOKEN>
<TOKEN end_char="2390" id="token-22-16" morph="none" pos="word" start_char="2384">desired</TOKEN>
<TOKEN end_char="2399" id="token-22-17" morph="none" pos="word" start_char="2392">function</TOKEN>
<TOKEN end_char="2400" id="token-22-18" morph="none" pos="punct" start_char="2400">,</TOKEN>
<TOKEN end_char="2405" id="token-22-19" morph="none" pos="word" start_char="2402">such</TOKEN>
<TOKEN end_char="2408" id="token-22-20" morph="none" pos="word" start_char="2407">as</TOKEN>
<TOKEN end_char="2415" id="token-22-21" morph="none" pos="word" start_char="2410">higher</TOKEN>
<TOKEN end_char="2422" id="token-22-22" morph="none" pos="word" start_char="2417">yields</TOKEN>
<TOKEN end_char="2426" id="token-22-23" morph="none" pos="word" start_char="2424">for</TOKEN>
<TOKEN end_char="2434" id="token-22-24" morph="none" pos="word" start_char="2428">vaccine</TOKEN>
<TOKEN end_char="2442" id="token-22-25" morph="none" pos="word" start_char="2436">strains</TOKEN>
<TOKEN end_char="2443" id="token-22-26" morph="none" pos="punct" start_char="2443">,</TOKEN>
<TOKEN end_char="2447" id="token-22-27" morph="none" pos="word" start_char="2445">but</TOKEN>
<TOKEN end_char="2453" id="token-22-28" morph="none" pos="word" start_char="2449">often</TOKEN>
<TOKEN end_char="2458" id="token-22-29" morph="none" pos="word" start_char="2455">also</TOKEN>
<TOKEN end_char="2463" id="token-22-30" morph="none" pos="word" start_char="2460">lead</TOKEN>
<TOKEN end_char="2466" id="token-22-31" morph="none" pos="word" start_char="2465">to</TOKEN>
<TOKEN end_char="2471" id="token-22-32" morph="none" pos="word" start_char="2468">loss</TOKEN>
<TOKEN end_char="2474" id="token-22-33" morph="none" pos="word" start_char="2473">of</TOKEN>
<TOKEN end_char="2483" id="token-22-34" morph="none" pos="word" start_char="2476">function</TOKEN>
<TOKEN end_char="2484" id="token-22-35" morph="none" pos="punct" start_char="2484">,</TOKEN>
<TOKEN end_char="2489" id="token-22-36" morph="none" pos="word" start_char="2486">such</TOKEN>
<TOKEN end_char="2492" id="token-22-37" morph="none" pos="word" start_char="2491">as</TOKEN>
<TOKEN end_char="2497" id="token-22-38" morph="none" pos="word" start_char="2494">loss</TOKEN>
<TOKEN end_char="2500" id="token-22-39" morph="none" pos="word" start_char="2499">of</TOKEN>
<TOKEN end_char="2504" id="token-22-40" morph="none" pos="word" start_char="2502">the</TOKEN>
<TOKEN end_char="2512" id="token-22-41" morph="none" pos="word" start_char="2506">ability</TOKEN>
<TOKEN end_char="2516" id="token-22-42" morph="none" pos="word" start_char="2514">for</TOKEN>
<TOKEN end_char="2518" id="token-22-43" morph="none" pos="word" start_char="2518">a</TOKEN>
<TOKEN end_char="2524" id="token-22-44" morph="none" pos="word" start_char="2520">virus</TOKEN>
<TOKEN end_char="2527" id="token-22-45" morph="none" pos="word" start_char="2526">to</TOKEN>
<TOKEN end_char="2537" id="token-22-46" morph="none" pos="word" start_char="2529">replicate</TOKEN>
<TOKEN end_char="2542" id="token-22-47" morph="none" pos="word" start_char="2539">well</TOKEN>
<TOKEN end_char="2543" id="token-22-48" morph="none" pos="punct" start_char="2543">,</TOKEN>
<TOKEN end_char="2546" id="token-22-49" morph="none" pos="word" start_char="2545">as</TOKEN>
<TOKEN end_char="2548" id="token-22-50" morph="none" pos="word" start_char="2548">a</TOKEN>
<TOKEN end_char="2560" id="token-22-51" morph="none" pos="word" start_char="2550">consequence</TOKEN>
<TOKEN end_char="2561" id="token-22-52" morph="none" pos="punct" start_char="2561">.</TOKEN>
</SEG>
<SEG end_char="2806" id="segment-23" start_char="2563">
<ORIGINAL_TEXT>In other words, any selection process involving an alteration of genotypes and their resulting phenotypes is considered a type of Gain-of-Function (GoF) research, even if the U.S. policy is intended to apply to only a small subset of such work.</ORIGINAL_TEXT>
<TOKEN end_char="2564" id="token-23-0" morph="none" pos="word" start_char="2563">In</TOKEN>
<TOKEN end_char="2570" id="token-23-1" morph="none" pos="word" start_char="2566">other</TOKEN>
<TOKEN end_char="2576" id="token-23-2" morph="none" pos="word" start_char="2572">words</TOKEN>
<TOKEN end_char="2577" id="token-23-3" morph="none" pos="punct" start_char="2577">,</TOKEN>
<TOKEN end_char="2581" id="token-23-4" morph="none" pos="word" start_char="2579">any</TOKEN>
<TOKEN end_char="2591" id="token-23-5" morph="none" pos="word" start_char="2583">selection</TOKEN>
<TOKEN end_char="2599" id="token-23-6" morph="none" pos="word" start_char="2593">process</TOKEN>
<TOKEN end_char="2609" id="token-23-7" morph="none" pos="word" start_char="2601">involving</TOKEN>
<TOKEN end_char="2612" id="token-23-8" morph="none" pos="word" start_char="2611">an</TOKEN>
<TOKEN end_char="2623" id="token-23-9" morph="none" pos="word" start_char="2614">alteration</TOKEN>
<TOKEN end_char="2626" id="token-23-10" morph="none" pos="word" start_char="2625">of</TOKEN>
<TOKEN end_char="2636" id="token-23-11" morph="none" pos="word" start_char="2628">genotypes</TOKEN>
<TOKEN end_char="2640" id="token-23-12" morph="none" pos="word" start_char="2638">and</TOKEN>
<TOKEN end_char="2646" id="token-23-13" morph="none" pos="word" start_char="2642">their</TOKEN>
<TOKEN end_char="2656" id="token-23-14" morph="none" pos="word" start_char="2648">resulting</TOKEN>
<TOKEN end_char="2667" id="token-23-15" morph="none" pos="word" start_char="2658">phenotypes</TOKEN>
<TOKEN end_char="2670" id="token-23-16" morph="none" pos="word" start_char="2669">is</TOKEN>
<TOKEN end_char="2681" id="token-23-17" morph="none" pos="word" start_char="2672">considered</TOKEN>
<TOKEN end_char="2683" id="token-23-18" morph="none" pos="word" start_char="2683">a</TOKEN>
<TOKEN end_char="2688" id="token-23-19" morph="none" pos="word" start_char="2685">type</TOKEN>
<TOKEN end_char="2691" id="token-23-20" morph="none" pos="word" start_char="2690">of</TOKEN>
<TOKEN end_char="2708" id="token-23-21" morph="none" pos="unknown" start_char="2693">Gain-of-Function</TOKEN>
<TOKEN end_char="2710" id="token-23-22" morph="none" pos="punct" start_char="2710">(</TOKEN>
<TOKEN end_char="2713" id="token-23-23" morph="none" pos="word" start_char="2711">GoF</TOKEN>
<TOKEN end_char="2714" id="token-23-24" morph="none" pos="punct" start_char="2714">)</TOKEN>
<TOKEN end_char="2723" id="token-23-25" morph="none" pos="word" start_char="2716">research</TOKEN>
<TOKEN end_char="2724" id="token-23-26" morph="none" pos="punct" start_char="2724">,</TOKEN>
<TOKEN end_char="2729" id="token-23-27" morph="none" pos="word" start_char="2726">even</TOKEN>
<TOKEN end_char="2732" id="token-23-28" morph="none" pos="word" start_char="2731">if</TOKEN>
<TOKEN end_char="2736" id="token-23-29" morph="none" pos="word" start_char="2734">the</TOKEN>
<TOKEN end_char="2740" id="token-23-30" morph="none" pos="unknown" start_char="2738">U.S</TOKEN>
<TOKEN end_char="2741" id="token-23-31" morph="none" pos="punct" start_char="2741">.</TOKEN>
<TOKEN end_char="2748" id="token-23-32" morph="none" pos="word" start_char="2743">policy</TOKEN>
<TOKEN end_char="2751" id="token-23-33" morph="none" pos="word" start_char="2750">is</TOKEN>
<TOKEN end_char="2760" id="token-23-34" morph="none" pos="word" start_char="2753">intended</TOKEN>
<TOKEN end_char="2763" id="token-23-35" morph="none" pos="word" start_char="2762">to</TOKEN>
<TOKEN end_char="2769" id="token-23-36" morph="none" pos="word" start_char="2765">apply</TOKEN>
<TOKEN end_char="2772" id="token-23-37" morph="none" pos="word" start_char="2771">to</TOKEN>
<TOKEN end_char="2777" id="token-23-38" morph="none" pos="word" start_char="2774">only</TOKEN>
<TOKEN end_char="2779" id="token-23-39" morph="none" pos="word" start_char="2779">a</TOKEN>
<TOKEN end_char="2785" id="token-23-40" morph="none" pos="word" start_char="2781">small</TOKEN>
<TOKEN end_char="2792" id="token-23-41" morph="none" pos="word" start_char="2787">subset</TOKEN>
<TOKEN end_char="2795" id="token-23-42" morph="none" pos="word" start_char="2794">of</TOKEN>
<TOKEN end_char="2800" id="token-23-43" morph="none" pos="word" start_char="2797">such</TOKEN>
<TOKEN end_char="2805" id="token-23-44" morph="none" pos="word" start_char="2802">work</TOKEN>
<TOKEN end_char="2806" id="token-23-45" morph="none" pos="punct" start_char="2806">.</TOKEN>
</SEG>
<SEG end_char="3023" id="segment-24" start_char="2808">
<ORIGINAL_TEXT>Subbarao emphasized that such experiments in virology are fundamental to understanding the biology, ecology, and pathogenesis of viruses and added that much basic knowledge is still lacking for SARS-CoV and MERS-CoV.</ORIGINAL_TEXT>
<TOKEN end_char="2815" id="token-24-0" morph="none" pos="word" start_char="2808">Subbarao</TOKEN>
<TOKEN end_char="2826" id="token-24-1" morph="none" pos="word" start_char="2817">emphasized</TOKEN>
<TOKEN end_char="2831" id="token-24-2" morph="none" pos="word" start_char="2828">that</TOKEN>
<TOKEN end_char="2836" id="token-24-3" morph="none" pos="word" start_char="2833">such</TOKEN>
<TOKEN end_char="2848" id="token-24-4" morph="none" pos="word" start_char="2838">experiments</TOKEN>
<TOKEN end_char="2851" id="token-24-5" morph="none" pos="word" start_char="2850">in</TOKEN>
<TOKEN end_char="2860" id="token-24-6" morph="none" pos="word" start_char="2853">virology</TOKEN>
<TOKEN end_char="2864" id="token-24-7" morph="none" pos="word" start_char="2862">are</TOKEN>
<TOKEN end_char="2876" id="token-24-8" morph="none" pos="word" start_char="2866">fundamental</TOKEN>
<TOKEN end_char="2879" id="token-24-9" morph="none" pos="word" start_char="2878">to</TOKEN>
<TOKEN end_char="2893" id="token-24-10" morph="none" pos="word" start_char="2881">understanding</TOKEN>
<TOKEN end_char="2897" id="token-24-11" morph="none" pos="word" start_char="2895">the</TOKEN>
<TOKEN end_char="2905" id="token-24-12" morph="none" pos="word" start_char="2899">biology</TOKEN>
<TOKEN end_char="2906" id="token-24-13" morph="none" pos="punct" start_char="2906">,</TOKEN>
<TOKEN end_char="2914" id="token-24-14" morph="none" pos="word" start_char="2908">ecology</TOKEN>
<TOKEN end_char="2915" id="token-24-15" morph="none" pos="punct" start_char="2915">,</TOKEN>
<TOKEN end_char="2919" id="token-24-16" morph="none" pos="word" start_char="2917">and</TOKEN>
<TOKEN end_char="2932" id="token-24-17" morph="none" pos="word" start_char="2921">pathogenesis</TOKEN>
<TOKEN end_char="2935" id="token-24-18" morph="none" pos="word" start_char="2934">of</TOKEN>
<TOKEN end_char="2943" id="token-24-19" morph="none" pos="word" start_char="2937">viruses</TOKEN>
<TOKEN end_char="2947" id="token-24-20" morph="none" pos="word" start_char="2945">and</TOKEN>
<TOKEN end_char="2953" id="token-24-21" morph="none" pos="word" start_char="2949">added</TOKEN>
<TOKEN end_char="2958" id="token-24-22" morph="none" pos="word" start_char="2955">that</TOKEN>
<TOKEN end_char="2963" id="token-24-23" morph="none" pos="word" start_char="2960">much</TOKEN>
<TOKEN end_char="2969" id="token-24-24" morph="none" pos="word" start_char="2965">basic</TOKEN>
<TOKEN end_char="2979" id="token-24-25" morph="none" pos="word" start_char="2971">knowledge</TOKEN>
<TOKEN end_char="2982" id="token-24-26" morph="none" pos="word" start_char="2981">is</TOKEN>
<TOKEN end_char="2988" id="token-24-27" morph="none" pos="word" start_char="2984">still</TOKEN>
<TOKEN end_char="2996" id="token-24-28" morph="none" pos="word" start_char="2990">lacking</TOKEN>
<TOKEN end_char="3000" id="token-24-29" morph="none" pos="word" start_char="2998">for</TOKEN>
<TOKEN end_char="3009" id="token-24-30" morph="none" pos="unknown" start_char="3002">SARS-CoV</TOKEN>
<TOKEN end_char="3013" id="token-24-31" morph="none" pos="word" start_char="3011">and</TOKEN>
<TOKEN end_char="3022" id="token-24-32" morph="none" pos="unknown" start_char="3015">MERS-CoV</TOKEN>
<TOKEN end_char="3023" id="token-24-33" morph="none" pos="punct" start_char="3023">.</TOKEN>
</SEG>
<SEG end_char="3266" id="segment-25" start_char="3025">
<ORIGINAL_TEXT>Subbarao introduced the key questions that virologists ask at all stages of research on the emergence or re-emergence of a virus and specifically adapted these general questions to the three viruses of interest in the symposium (see Box 3-1).</ORIGINAL_TEXT>
<TOKEN end_char="3032" id="token-25-0" morph="none" pos="word" start_char="3025">Subbarao</TOKEN>
<TOKEN end_char="3043" id="token-25-1" morph="none" pos="word" start_char="3034">introduced</TOKEN>
<TOKEN end_char="3047" id="token-25-2" morph="none" pos="word" start_char="3045">the</TOKEN>
<TOKEN end_char="3051" id="token-25-3" morph="none" pos="word" start_char="3049">key</TOKEN>
<TOKEN end_char="3061" id="token-25-4" morph="none" pos="word" start_char="3053">questions</TOKEN>
<TOKEN end_char="3066" id="token-25-5" morph="none" pos="word" start_char="3063">that</TOKEN>
<TOKEN end_char="3078" id="token-25-6" morph="none" pos="word" start_char="3068">virologists</TOKEN>
<TOKEN end_char="3082" id="token-25-7" morph="none" pos="word" start_char="3080">ask</TOKEN>
<TOKEN end_char="3085" id="token-25-8" morph="none" pos="word" start_char="3084">at</TOKEN>
<TOKEN end_char="3089" id="token-25-9" morph="none" pos="word" start_char="3087">all</TOKEN>
<TOKEN end_char="3096" id="token-25-10" morph="none" pos="word" start_char="3091">stages</TOKEN>
<TOKEN end_char="3099" id="token-25-11" morph="none" pos="word" start_char="3098">of</TOKEN>
<TOKEN end_char="3108" id="token-25-12" morph="none" pos="word" start_char="3101">research</TOKEN>
<TOKEN end_char="3111" id="token-25-13" morph="none" pos="word" start_char="3110">on</TOKEN>
<TOKEN end_char="3115" id="token-25-14" morph="none" pos="word" start_char="3113">the</TOKEN>
<TOKEN end_char="3125" id="token-25-15" morph="none" pos="word" start_char="3117">emergence</TOKEN>
<TOKEN end_char="3128" id="token-25-16" morph="none" pos="word" start_char="3127">or</TOKEN>
<TOKEN end_char="3141" id="token-25-17" morph="none" pos="unknown" start_char="3130">re-emergence</TOKEN>
<TOKEN end_char="3144" id="token-25-18" morph="none" pos="word" start_char="3143">of</TOKEN>
<TOKEN end_char="3146" id="token-25-19" morph="none" pos="word" start_char="3146">a</TOKEN>
<TOKEN end_char="3152" id="token-25-20" morph="none" pos="word" start_char="3148">virus</TOKEN>
<TOKEN end_char="3156" id="token-25-21" morph="none" pos="word" start_char="3154">and</TOKEN>
<TOKEN end_char="3169" id="token-25-22" morph="none" pos="word" start_char="3158">specifically</TOKEN>
<TOKEN end_char="3177" id="token-25-23" morph="none" pos="word" start_char="3171">adapted</TOKEN>
<TOKEN end_char="3183" id="token-25-24" morph="none" pos="word" start_char="3179">these</TOKEN>
<TOKEN end_char="3191" id="token-25-25" morph="none" pos="word" start_char="3185">general</TOKEN>
<TOKEN end_char="3201" id="token-25-26" morph="none" pos="word" start_char="3193">questions</TOKEN>
<TOKEN end_char="3204" id="token-25-27" morph="none" pos="word" start_char="3203">to</TOKEN>
<TOKEN end_char="3208" id="token-25-28" morph="none" pos="word" start_char="3206">the</TOKEN>
<TOKEN end_char="3214" id="token-25-29" morph="none" pos="word" start_char="3210">three</TOKEN>
<TOKEN end_char="3222" id="token-25-30" morph="none" pos="word" start_char="3216">viruses</TOKEN>
<TOKEN end_char="3225" id="token-25-31" morph="none" pos="word" start_char="3224">of</TOKEN>
<TOKEN end_char="3234" id="token-25-32" morph="none" pos="word" start_char="3227">interest</TOKEN>
<TOKEN end_char="3237" id="token-25-33" morph="none" pos="word" start_char="3236">in</TOKEN>
<TOKEN end_char="3241" id="token-25-34" morph="none" pos="word" start_char="3239">the</TOKEN>
<TOKEN end_char="3251" id="token-25-35" morph="none" pos="word" start_char="3243">symposium</TOKEN>
<TOKEN end_char="3253" id="token-25-36" morph="none" pos="punct" start_char="3253">(</TOKEN>
<TOKEN end_char="3256" id="token-25-37" morph="none" pos="word" start_char="3254">see</TOKEN>
<TOKEN end_char="3260" id="token-25-38" morph="none" pos="word" start_char="3258">Box</TOKEN>
<TOKEN end_char="3264" id="token-25-39" morph="none" pos="unknown" start_char="3262">3-1</TOKEN>
<TOKEN end_char="3266" id="token-25-40" morph="none" pos="punct" start_char="3265">).</TOKEN>
</SEG>
<SEG end_char="3437" id="segment-26" start_char="3268">
<ORIGINAL_TEXT>To answer these questions, virologists use gain- and loss-of-function experiments to understand the genetic makeup of viruses and the specifics of virus-host interaction.</ORIGINAL_TEXT>
<TOKEN end_char="3269" id="token-26-0" morph="none" pos="word" start_char="3268">To</TOKEN>
<TOKEN end_char="3276" id="token-26-1" morph="none" pos="word" start_char="3271">answer</TOKEN>
<TOKEN end_char="3282" id="token-26-2" morph="none" pos="word" start_char="3278">these</TOKEN>
<TOKEN end_char="3292" id="token-26-3" morph="none" pos="word" start_char="3284">questions</TOKEN>
<TOKEN end_char="3293" id="token-26-4" morph="none" pos="punct" start_char="3293">,</TOKEN>
<TOKEN end_char="3305" id="token-26-5" morph="none" pos="word" start_char="3295">virologists</TOKEN>
<TOKEN end_char="3309" id="token-26-6" morph="none" pos="word" start_char="3307">use</TOKEN>
<TOKEN end_char="3314" id="token-26-7" morph="none" pos="word" start_char="3311">gain</TOKEN>
<TOKEN end_char="3315" id="token-26-8" morph="none" pos="punct" start_char="3315">-</TOKEN>
<TOKEN end_char="3319" id="token-26-9" morph="none" pos="word" start_char="3317">and</TOKEN>
<TOKEN end_char="3336" id="token-26-10" morph="none" pos="unknown" start_char="3321">loss-of-function</TOKEN>
<TOKEN end_char="3348" id="token-26-11" morph="none" pos="word" start_char="3338">experiments</TOKEN>
<TOKEN end_char="3351" id="token-26-12" morph="none" pos="word" start_char="3350">to</TOKEN>
<TOKEN end_char="3362" id="token-26-13" morph="none" pos="word" start_char="3353">understand</TOKEN>
<TOKEN end_char="3366" id="token-26-14" morph="none" pos="word" start_char="3364">the</TOKEN>
<TOKEN end_char="3374" id="token-26-15" morph="none" pos="word" start_char="3368">genetic</TOKEN>
<TOKEN end_char="3381" id="token-26-16" morph="none" pos="word" start_char="3376">makeup</TOKEN>
<TOKEN end_char="3384" id="token-26-17" morph="none" pos="word" start_char="3383">of</TOKEN>
<TOKEN end_char="3392" id="token-26-18" morph="none" pos="word" start_char="3386">viruses</TOKEN>
<TOKEN end_char="3396" id="token-26-19" morph="none" pos="word" start_char="3394">and</TOKEN>
<TOKEN end_char="3400" id="token-26-20" morph="none" pos="word" start_char="3398">the</TOKEN>
<TOKEN end_char="3410" id="token-26-21" morph="none" pos="word" start_char="3402">specifics</TOKEN>
<TOKEN end_char="3413" id="token-26-22" morph="none" pos="word" start_char="3412">of</TOKEN>
<TOKEN end_char="3424" id="token-26-23" morph="none" pos="unknown" start_char="3415">virus-host</TOKEN>
<TOKEN end_char="3436" id="token-26-24" morph="none" pos="word" start_char="3426">interaction</TOKEN>
<TOKEN end_char="3437" id="token-26-25" morph="none" pos="punct" start_char="3437">.</TOKEN>
</SEG>
<SEG end_char="3722" id="segment-27" start_char="3439">
<ORIGINAL_TEXT>For instance, researchers now have advanced molecular technologies, such as reverse genetics, which allow them to produce de novo recombinant viruses from cloned cDNA, and deep sequencing that are critical for studying how viruses escape the host immune system and antiviral controls.</ORIGINAL_TEXT>
<TOKEN end_char="3441" id="token-27-0" morph="none" pos="word" start_char="3439">For</TOKEN>
<TOKEN end_char="3450" id="token-27-1" morph="none" pos="word" start_char="3443">instance</TOKEN>
<TOKEN end_char="3451" id="token-27-2" morph="none" pos="punct" start_char="3451">,</TOKEN>
<TOKEN end_char="3463" id="token-27-3" morph="none" pos="word" start_char="3453">researchers</TOKEN>
<TOKEN end_char="3467" id="token-27-4" morph="none" pos="word" start_char="3465">now</TOKEN>
<TOKEN end_char="3472" id="token-27-5" morph="none" pos="word" start_char="3469">have</TOKEN>
<TOKEN end_char="3481" id="token-27-6" morph="none" pos="word" start_char="3474">advanced</TOKEN>
<TOKEN end_char="3491" id="token-27-7" morph="none" pos="word" start_char="3483">molecular</TOKEN>
<TOKEN end_char="3504" id="token-27-8" morph="none" pos="word" start_char="3493">technologies</TOKEN>
<TOKEN end_char="3505" id="token-27-9" morph="none" pos="punct" start_char="3505">,</TOKEN>
<TOKEN end_char="3510" id="token-27-10" morph="none" pos="word" start_char="3507">such</TOKEN>
<TOKEN end_char="3513" id="token-27-11" morph="none" pos="word" start_char="3512">as</TOKEN>
<TOKEN end_char="3521" id="token-27-12" morph="none" pos="word" start_char="3515">reverse</TOKEN>
<TOKEN end_char="3530" id="token-27-13" morph="none" pos="word" start_char="3523">genetics</TOKEN>
<TOKEN end_char="3531" id="token-27-14" morph="none" pos="punct" start_char="3531">,</TOKEN>
<TOKEN end_char="3537" id="token-27-15" morph="none" pos="word" start_char="3533">which</TOKEN>
<TOKEN end_char="3543" id="token-27-16" morph="none" pos="word" start_char="3539">allow</TOKEN>
<TOKEN end_char="3548" id="token-27-17" morph="none" pos="word" start_char="3545">them</TOKEN>
<TOKEN end_char="3551" id="token-27-18" morph="none" pos="word" start_char="3550">to</TOKEN>
<TOKEN end_char="3559" id="token-27-19" morph="none" pos="word" start_char="3553">produce</TOKEN>
<TOKEN end_char="3562" id="token-27-20" morph="none" pos="word" start_char="3561">de</TOKEN>
<TOKEN end_char="3567" id="token-27-21" morph="none" pos="word" start_char="3564">novo</TOKEN>
<TOKEN end_char="3579" id="token-27-22" morph="none" pos="word" start_char="3569">recombinant</TOKEN>
<TOKEN end_char="3587" id="token-27-23" morph="none" pos="word" start_char="3581">viruses</TOKEN>
<TOKEN end_char="3592" id="token-27-24" morph="none" pos="word" start_char="3589">from</TOKEN>
<TOKEN end_char="3599" id="token-27-25" morph="none" pos="word" start_char="3594">cloned</TOKEN>
<TOKEN end_char="3604" id="token-27-26" morph="none" pos="word" start_char="3601">cDNA</TOKEN>
<TOKEN end_char="3605" id="token-27-27" morph="none" pos="punct" start_char="3605">,</TOKEN>
<TOKEN end_char="3609" id="token-27-28" morph="none" pos="word" start_char="3607">and</TOKEN>
<TOKEN end_char="3614" id="token-27-29" morph="none" pos="word" start_char="3611">deep</TOKEN>
<TOKEN end_char="3625" id="token-27-30" morph="none" pos="word" start_char="3616">sequencing</TOKEN>
<TOKEN end_char="3630" id="token-27-31" morph="none" pos="word" start_char="3627">that</TOKEN>
<TOKEN end_char="3634" id="token-27-32" morph="none" pos="word" start_char="3632">are</TOKEN>
<TOKEN end_char="3643" id="token-27-33" morph="none" pos="word" start_char="3636">critical</TOKEN>
<TOKEN end_char="3647" id="token-27-34" morph="none" pos="word" start_char="3645">for</TOKEN>
<TOKEN end_char="3656" id="token-27-35" morph="none" pos="word" start_char="3649">studying</TOKEN>
<TOKEN end_char="3660" id="token-27-36" morph="none" pos="word" start_char="3658">how</TOKEN>
<TOKEN end_char="3668" id="token-27-37" morph="none" pos="word" start_char="3662">viruses</TOKEN>
<TOKEN end_char="3675" id="token-27-38" morph="none" pos="word" start_char="3670">escape</TOKEN>
<TOKEN end_char="3679" id="token-27-39" morph="none" pos="word" start_char="3677">the</TOKEN>
<TOKEN end_char="3684" id="token-27-40" morph="none" pos="word" start_char="3681">host</TOKEN>
<TOKEN end_char="3691" id="token-27-41" morph="none" pos="word" start_char="3686">immune</TOKEN>
<TOKEN end_char="3698" id="token-27-42" morph="none" pos="word" start_char="3693">system</TOKEN>
<TOKEN end_char="3702" id="token-27-43" morph="none" pos="word" start_char="3700">and</TOKEN>
<TOKEN end_char="3712" id="token-27-44" morph="none" pos="word" start_char="3704">antiviral</TOKEN>
<TOKEN end_char="3721" id="token-27-45" morph="none" pos="word" start_char="3714">controls</TOKEN>
<TOKEN end_char="3722" id="token-27-46" morph="none" pos="punct" start_char="3722">.</TOKEN>
</SEG>
<SEG end_char="3888" id="segment-28" start_char="3724">
<ORIGINAL_TEXT>Researchers also use targeted host or viral genome modification using small interfering RNA or the bacterial CRISPR-associated protein-9 nuclease as an editing tool.</ORIGINAL_TEXT>
<TOKEN end_char="3734" id="token-28-0" morph="none" pos="word" start_char="3724">Researchers</TOKEN>
<TOKEN end_char="3739" id="token-28-1" morph="none" pos="word" start_char="3736">also</TOKEN>
<TOKEN end_char="3743" id="token-28-2" morph="none" pos="word" start_char="3741">use</TOKEN>
<TOKEN end_char="3752" id="token-28-3" morph="none" pos="word" start_char="3745">targeted</TOKEN>
<TOKEN end_char="3757" id="token-28-4" morph="none" pos="word" start_char="3754">host</TOKEN>
<TOKEN end_char="3760" id="token-28-5" morph="none" pos="word" start_char="3759">or</TOKEN>
<TOKEN end_char="3766" id="token-28-6" morph="none" pos="word" start_char="3762">viral</TOKEN>
<TOKEN end_char="3773" id="token-28-7" morph="none" pos="word" start_char="3768">genome</TOKEN>
<TOKEN end_char="3786" id="token-28-8" morph="none" pos="word" start_char="3775">modification</TOKEN>
<TOKEN end_char="3792" id="token-28-9" morph="none" pos="word" start_char="3788">using</TOKEN>
<TOKEN end_char="3798" id="token-28-10" morph="none" pos="word" start_char="3794">small</TOKEN>
<TOKEN end_char="3810" id="token-28-11" morph="none" pos="word" start_char="3800">interfering</TOKEN>
<TOKEN end_char="3814" id="token-28-12" morph="none" pos="word" start_char="3812">RNA</TOKEN>
<TOKEN end_char="3817" id="token-28-13" morph="none" pos="word" start_char="3816">or</TOKEN>
<TOKEN end_char="3821" id="token-28-14" morph="none" pos="word" start_char="3819">the</TOKEN>
<TOKEN end_char="3831" id="token-28-15" morph="none" pos="word" start_char="3823">bacterial</TOKEN>
<TOKEN end_char="3849" id="token-28-16" morph="none" pos="unknown" start_char="3833">CRISPR-associated</TOKEN>
<TOKEN end_char="3859" id="token-28-17" morph="none" pos="unknown" start_char="3851">protein-9</TOKEN>
<TOKEN end_char="3868" id="token-28-18" morph="none" pos="word" start_char="3861">nuclease</TOKEN>
<TOKEN end_char="3871" id="token-28-19" morph="none" pos="word" start_char="3870">as</TOKEN>
<TOKEN end_char="3874" id="token-28-20" morph="none" pos="word" start_char="3873">an</TOKEN>
<TOKEN end_char="3882" id="token-28-21" morph="none" pos="word" start_char="3876">editing</TOKEN>
<TOKEN end_char="3887" id="token-28-22" morph="none" pos="word" start_char="3884">tool</TOKEN>
<TOKEN end_char="3888" id="token-28-23" morph="none" pos="punct" start_char="3888">.</TOKEN>
</SEG>
<SEG end_char="3944" id="segment-29" start_char="3891">
<ORIGINAL_TEXT>edit on 30/12/2020 by dug88 because: (no reason given)</ORIGINAL_TEXT>
<TOKEN end_char="3894" id="token-29-0" morph="none" pos="word" start_char="3891">edit</TOKEN>
<TOKEN end_char="3897" id="token-29-1" morph="none" pos="word" start_char="3896">on</TOKEN>
<TOKEN end_char="3908" id="token-29-2" morph="none" pos="unknown" start_char="3899">30/12/2020</TOKEN>
<TOKEN end_char="3911" id="token-29-3" morph="none" pos="word" start_char="3910">by</TOKEN>
<TOKEN end_char="3917" id="token-29-4" morph="none" pos="word" start_char="3913">dug88</TOKEN>
<TOKEN end_char="3925" id="token-29-5" morph="none" pos="word" start_char="3919">because</TOKEN>
<TOKEN end_char="3926" id="token-29-6" morph="none" pos="punct" start_char="3926">:</TOKEN>
<TOKEN end_char="3928" id="token-29-7" morph="none" pos="punct" start_char="3928">(</TOKEN>
<TOKEN end_char="3930" id="token-29-8" morph="none" pos="word" start_char="3929">no</TOKEN>
<TOKEN end_char="3937" id="token-29-9" morph="none" pos="word" start_char="3932">reason</TOKEN>
<TOKEN end_char="3943" id="token-29-10" morph="none" pos="word" start_char="3939">given</TOKEN>
<TOKEN end_char="3944" id="token-29-11" morph="none" pos="punct" start_char="3944">)</TOKEN>
</SEG>
<SEG end_char="4175" id="segment-30" start_char="3949">
<ORIGINAL_TEXT>originally posted by: dug88 An Israeli startup named Rootclaim has determined that there is an 81% chance that the SARS-cov-2 was accidentally released from the Wuhan bioresearch laboratory during gain-of-function research. ...</ORIGINAL_TEXT>
<TOKEN end_char="3958" id="token-30-0" morph="none" pos="word" start_char="3949">originally</TOKEN>
<TOKEN end_char="3965" id="token-30-1" morph="none" pos="word" start_char="3960">posted</TOKEN>
<TOKEN end_char="3968" id="token-30-2" morph="none" pos="word" start_char="3967">by</TOKEN>
<TOKEN end_char="3969" id="token-30-3" morph="none" pos="punct" start_char="3969">:</TOKEN>
<TOKEN end_char="3975" id="token-30-4" morph="none" pos="word" start_char="3971">dug88</TOKEN>
<TOKEN end_char="3978" id="token-30-5" morph="none" pos="word" start_char="3977">An</TOKEN>
<TOKEN end_char="3986" id="token-30-6" morph="none" pos="word" start_char="3980">Israeli</TOKEN>
<TOKEN end_char="3994" id="token-30-7" morph="none" pos="word" start_char="3988">startup</TOKEN>
<TOKEN end_char="4000" id="token-30-8" morph="none" pos="word" start_char="3996">named</TOKEN>
<TOKEN end_char="4010" id="token-30-9" morph="none" pos="word" start_char="4002">Rootclaim</TOKEN>
<TOKEN end_char="4014" id="token-30-10" morph="none" pos="word" start_char="4012">has</TOKEN>
<TOKEN end_char="4025" id="token-30-11" morph="none" pos="word" start_char="4016">determined</TOKEN>
<TOKEN end_char="4030" id="token-30-12" morph="none" pos="word" start_char="4027">that</TOKEN>
<TOKEN end_char="4036" id="token-30-13" morph="none" pos="word" start_char="4032">there</TOKEN>
<TOKEN end_char="4039" id="token-30-14" morph="none" pos="word" start_char="4038">is</TOKEN>
<TOKEN end_char="4042" id="token-30-15" morph="none" pos="word" start_char="4041">an</TOKEN>
<TOKEN end_char="4045" id="token-30-16" morph="none" pos="word" start_char="4044">81</TOKEN>
<TOKEN end_char="4046" id="token-30-17" morph="none" pos="punct" start_char="4046">%</TOKEN>
<TOKEN end_char="4053" id="token-30-18" morph="none" pos="word" start_char="4048">chance</TOKEN>
<TOKEN end_char="4058" id="token-30-19" morph="none" pos="word" start_char="4055">that</TOKEN>
<TOKEN end_char="4062" id="token-30-20" morph="none" pos="word" start_char="4060">the</TOKEN>
<TOKEN end_char="4073" id="token-30-21" morph="none" pos="unknown" start_char="4064">SARS-cov-2</TOKEN>
<TOKEN end_char="4077" id="token-30-22" morph="none" pos="word" start_char="4075">was</TOKEN>
<TOKEN end_char="4090" id="token-30-23" morph="none" pos="word" start_char="4079">accidentally</TOKEN>
<TOKEN end_char="4099" id="token-30-24" morph="none" pos="word" start_char="4092">released</TOKEN>
<TOKEN end_char="4104" id="token-30-25" morph="none" pos="word" start_char="4101">from</TOKEN>
<TOKEN end_char="4108" id="token-30-26" morph="none" pos="word" start_char="4106">the</TOKEN>
<TOKEN end_char="4114" id="token-30-27" morph="none" pos="word" start_char="4110">Wuhan</TOKEN>
<TOKEN end_char="4126" id="token-30-28" morph="none" pos="word" start_char="4116">bioresearch</TOKEN>
<TOKEN end_char="4137" id="token-30-29" morph="none" pos="word" start_char="4128">laboratory</TOKEN>
<TOKEN end_char="4144" id="token-30-30" morph="none" pos="word" start_char="4139">during</TOKEN>
<TOKEN end_char="4161" id="token-30-31" morph="none" pos="unknown" start_char="4146">gain-of-function</TOKEN>
<TOKEN end_char="4170" id="token-30-32" morph="none" pos="word" start_char="4163">research</TOKEN>
<TOKEN end_char="4171" id="token-30-33" morph="none" pos="punct" start_char="4171">.</TOKEN>
<TOKEN end_char="4175" id="token-30-34" morph="none" pos="punct" start_char="4173">...</TOKEN>
</SEG>
<SEG end_char="4216" id="segment-31" start_char="4178">
<ORIGINAL_TEXT>Didn't need no damn AI to tell me that!</ORIGINAL_TEXT>
<TOKEN end_char="4183" id="token-31-0" morph="none" pos="word" start_char="4178">Didn't</TOKEN>
<TOKEN end_char="4188" id="token-31-1" morph="none" pos="word" start_char="4185">need</TOKEN>
<TOKEN end_char="4191" id="token-31-2" morph="none" pos="word" start_char="4190">no</TOKEN>
<TOKEN end_char="4196" id="token-31-3" morph="none" pos="word" start_char="4193">damn</TOKEN>
<TOKEN end_char="4199" id="token-31-4" morph="none" pos="word" start_char="4198">AI</TOKEN>
<TOKEN end_char="4202" id="token-31-5" morph="none" pos="word" start_char="4201">to</TOKEN>
<TOKEN end_char="4207" id="token-31-6" morph="none" pos="word" start_char="4204">tell</TOKEN>
<TOKEN end_char="4210" id="token-31-7" morph="none" pos="word" start_char="4209">me</TOKEN>
<TOKEN end_char="4215" id="token-31-8" morph="none" pos="word" start_char="4212">that</TOKEN>
<TOKEN end_char="4216" id="token-31-9" morph="none" pos="punct" start_char="4216">!</TOKEN>
</SEG>
<SEG end_char="4283" id="segment-32" start_char="4219">
<ORIGINAL_TEXT>That is, if it's actually all as terrible as they claim it to be.</ORIGINAL_TEXT>
<TOKEN end_char="4222" id="token-32-0" morph="none" pos="word" start_char="4219">That</TOKEN>
<TOKEN end_char="4225" id="token-32-1" morph="none" pos="word" start_char="4224">is</TOKEN>
<TOKEN end_char="4226" id="token-32-2" morph="none" pos="punct" start_char="4226">,</TOKEN>
<TOKEN end_char="4229" id="token-32-3" morph="none" pos="word" start_char="4228">if</TOKEN>
<TOKEN end_char="4234" id="token-32-4" morph="none" pos="word" start_char="4231">it's</TOKEN>
<TOKEN end_char="4243" id="token-32-5" morph="none" pos="word" start_char="4236">actually</TOKEN>
<TOKEN end_char="4247" id="token-32-6" morph="none" pos="word" start_char="4245">all</TOKEN>
<TOKEN end_char="4250" id="token-32-7" morph="none" pos="word" start_char="4249">as</TOKEN>
<TOKEN end_char="4259" id="token-32-8" morph="none" pos="word" start_char="4252">terrible</TOKEN>
<TOKEN end_char="4262" id="token-32-9" morph="none" pos="word" start_char="4261">as</TOKEN>
<TOKEN end_char="4267" id="token-32-10" morph="none" pos="word" start_char="4264">they</TOKEN>
<TOKEN end_char="4273" id="token-32-11" morph="none" pos="word" start_char="4269">claim</TOKEN>
<TOKEN end_char="4276" id="token-32-12" morph="none" pos="word" start_char="4275">it</TOKEN>
<TOKEN end_char="4279" id="token-32-13" morph="none" pos="word" start_char="4278">to</TOKEN>
<TOKEN end_char="4282" id="token-32-14" morph="none" pos="word" start_char="4281">be</TOKEN>
<TOKEN end_char="4283" id="token-32-15" morph="none" pos="punct" start_char="4283">.</TOKEN>
</SEG>
<SEG end_char="4285" id="segment-33" start_char="4285">
<ORIGINAL_TEXT>:</ORIGINAL_TEXT>
<TOKEN end_char="4285" id="token-33-0" morph="none" pos="punct" start_char="4285">:</TOKEN>
</SEG>
<SEG end_char="4341" id="segment-34" start_char="4288">
<ORIGINAL_TEXT>edit on 2020 12 30 by incoserv because: clarification.</ORIGINAL_TEXT>
<TOKEN end_char="4291" id="token-34-0" morph="none" pos="word" start_char="4288">edit</TOKEN>
<TOKEN end_char="4294" id="token-34-1" morph="none" pos="word" start_char="4293">on</TOKEN>
<TOKEN end_char="4299" id="token-34-2" morph="none" pos="word" start_char="4296">2020</TOKEN>
<TOKEN end_char="4302" id="token-34-3" morph="none" pos="word" start_char="4301">12</TOKEN>
<TOKEN end_char="4305" id="token-34-4" morph="none" pos="word" start_char="4304">30</TOKEN>
<TOKEN end_char="4308" id="token-34-5" morph="none" pos="word" start_char="4307">by</TOKEN>
<TOKEN end_char="4317" id="token-34-6" morph="none" pos="word" start_char="4310">incoserv</TOKEN>
<TOKEN end_char="4325" id="token-34-7" morph="none" pos="word" start_char="4319">because</TOKEN>
<TOKEN end_char="4326" id="token-34-8" morph="none" pos="punct" start_char="4326">:</TOKEN>
<TOKEN end_char="4340" id="token-34-9" morph="none" pos="word" start_char="4328">clarification</TOKEN>
<TOKEN end_char="4341" id="token-34-10" morph="none" pos="punct" start_char="4341">.</TOKEN>
</SEG>
<SEG end_char="4362" id="segment-35" start_char="4346">
<ORIGINAL_TEXT>a reply to: dug88</ORIGINAL_TEXT>
<TOKEN end_char="4346" id="token-35-0" morph="none" pos="word" start_char="4346">a</TOKEN>
<TOKEN end_char="4352" id="token-35-1" morph="none" pos="word" start_char="4348">reply</TOKEN>
<TOKEN end_char="4355" id="token-35-2" morph="none" pos="word" start_char="4354">to</TOKEN>
<TOKEN end_char="4356" id="token-35-3" morph="none" pos="punct" start_char="4356">:</TOKEN>
<TOKEN end_char="4362" id="token-35-4" morph="none" pos="word" start_char="4358">dug88</TOKEN>
</SEG>
<SEG end_char="4434" id="segment-36" start_char="4365">
<ORIGINAL_TEXT>To be honest dug88, humans got there first....and quite some time ago.</ORIGINAL_TEXT>
<TOKEN end_char="4366" id="token-36-0" morph="none" pos="word" start_char="4365">To</TOKEN>
<TOKEN end_char="4369" id="token-36-1" morph="none" pos="word" start_char="4368">be</TOKEN>
<TOKEN end_char="4376" id="token-36-2" morph="none" pos="word" start_char="4371">honest</TOKEN>
<TOKEN end_char="4382" id="token-36-3" morph="none" pos="word" start_char="4378">dug88</TOKEN>
<TOKEN end_char="4383" id="token-36-4" morph="none" pos="punct" start_char="4383">,</TOKEN>
<TOKEN end_char="4390" id="token-36-5" morph="none" pos="word" start_char="4385">humans</TOKEN>
<TOKEN end_char="4394" id="token-36-6" morph="none" pos="word" start_char="4392">got</TOKEN>
<TOKEN end_char="4400" id="token-36-7" morph="none" pos="word" start_char="4396">there</TOKEN>
<TOKEN end_char="4413" id="token-36-8" morph="none" pos="unknown" start_char="4402">first....and</TOKEN>
<TOKEN end_char="4419" id="token-36-9" morph="none" pos="word" start_char="4415">quite</TOKEN>
<TOKEN end_char="4424" id="token-36-10" morph="none" pos="word" start_char="4421">some</TOKEN>
<TOKEN end_char="4429" id="token-36-11" morph="none" pos="word" start_char="4426">time</TOKEN>
<TOKEN end_char="4433" id="token-36-12" morph="none" pos="word" start_char="4431">ago</TOKEN>
<TOKEN end_char="4434" id="token-36-13" morph="none" pos="punct" start_char="4434">.</TOKEN>
</SEG>
<SEG end_char="4614" id="segment-37" start_char="4437">
<ORIGINAL_TEXT>Nonetheless, this is significant and very important in demonstrating that when Bill Gates (of hell) says there will be more pandemics, we know where to start pointing the finger.</ORIGINAL_TEXT>
<TOKEN end_char="4447" id="token-37-0" morph="none" pos="word" start_char="4437">Nonetheless</TOKEN>
<TOKEN end_char="4448" id="token-37-1" morph="none" pos="punct" start_char="4448">,</TOKEN>
<TOKEN end_char="4453" id="token-37-2" morph="none" pos="word" start_char="4450">this</TOKEN>
<TOKEN end_char="4456" id="token-37-3" morph="none" pos="word" start_char="4455">is</TOKEN>
<TOKEN end_char="4468" id="token-37-4" morph="none" pos="word" start_char="4458">significant</TOKEN>
<TOKEN end_char="4472" id="token-37-5" morph="none" pos="word" start_char="4470">and</TOKEN>
<TOKEN end_char="4477" id="token-37-6" morph="none" pos="word" start_char="4474">very</TOKEN>
<TOKEN end_char="4487" id="token-37-7" morph="none" pos="word" start_char="4479">important</TOKEN>
<TOKEN end_char="4490" id="token-37-8" morph="none" pos="word" start_char="4489">in</TOKEN>
<TOKEN end_char="4504" id="token-37-9" morph="none" pos="word" start_char="4492">demonstrating</TOKEN>
<TOKEN end_char="4509" id="token-37-10" morph="none" pos="word" start_char="4506">that</TOKEN>
<TOKEN end_char="4514" id="token-37-11" morph="none" pos="word" start_char="4511">when</TOKEN>
<TOKEN end_char="4519" id="token-37-12" morph="none" pos="word" start_char="4516">Bill</TOKEN>
<TOKEN end_char="4525" id="token-37-13" morph="none" pos="word" start_char="4521">Gates</TOKEN>
<TOKEN end_char="4527" id="token-37-14" morph="none" pos="punct" start_char="4527">(</TOKEN>
<TOKEN end_char="4529" id="token-37-15" morph="none" pos="word" start_char="4528">of</TOKEN>
<TOKEN end_char="4534" id="token-37-16" morph="none" pos="word" start_char="4531">hell</TOKEN>
<TOKEN end_char="4535" id="token-37-17" morph="none" pos="punct" start_char="4535">)</TOKEN>
<TOKEN end_char="4540" id="token-37-18" morph="none" pos="word" start_char="4537">says</TOKEN>
<TOKEN end_char="4546" id="token-37-19" morph="none" pos="word" start_char="4542">there</TOKEN>
<TOKEN end_char="4551" id="token-37-20" morph="none" pos="word" start_char="4548">will</TOKEN>
<TOKEN end_char="4554" id="token-37-21" morph="none" pos="word" start_char="4553">be</TOKEN>
<TOKEN end_char="4559" id="token-37-22" morph="none" pos="word" start_char="4556">more</TOKEN>
<TOKEN end_char="4569" id="token-37-23" morph="none" pos="word" start_char="4561">pandemics</TOKEN>
<TOKEN end_char="4570" id="token-37-24" morph="none" pos="punct" start_char="4570">,</TOKEN>
<TOKEN end_char="4573" id="token-37-25" morph="none" pos="word" start_char="4572">we</TOKEN>
<TOKEN end_char="4578" id="token-37-26" morph="none" pos="word" start_char="4575">know</TOKEN>
<TOKEN end_char="4584" id="token-37-27" morph="none" pos="word" start_char="4580">where</TOKEN>
<TOKEN end_char="4587" id="token-37-28" morph="none" pos="word" start_char="4586">to</TOKEN>
<TOKEN end_char="4593" id="token-37-29" morph="none" pos="word" start_char="4589">start</TOKEN>
<TOKEN end_char="4602" id="token-37-30" morph="none" pos="word" start_char="4595">pointing</TOKEN>
<TOKEN end_char="4606" id="token-37-31" morph="none" pos="word" start_char="4604">the</TOKEN>
<TOKEN end_char="4613" id="token-37-32" morph="none" pos="word" start_char="4608">finger</TOKEN>
<TOKEN end_char="4614" id="token-37-33" morph="none" pos="punct" start_char="4614">.</TOKEN>
</SEG>
<SEG end_char="4666" id="segment-38" start_char="4617">
<ORIGINAL_TEXT>Never forget Sars-CoV-2 Is The Tip Of The Iceberg.</ORIGINAL_TEXT>
<TOKEN end_char="4621" id="token-38-0" morph="none" pos="word" start_char="4617">Never</TOKEN>
<TOKEN end_char="4628" id="token-38-1" morph="none" pos="word" start_char="4623">forget</TOKEN>
<TOKEN end_char="4639" id="token-38-2" morph="none" pos="unknown" start_char="4630">Sars-CoV-2</TOKEN>
<TOKEN end_char="4642" id="token-38-3" morph="none" pos="word" start_char="4641">Is</TOKEN>
<TOKEN end_char="4646" id="token-38-4" morph="none" pos="word" start_char="4644">The</TOKEN>
<TOKEN end_char="4650" id="token-38-5" morph="none" pos="word" start_char="4648">Tip</TOKEN>
<TOKEN end_char="4653" id="token-38-6" morph="none" pos="word" start_char="4652">Of</TOKEN>
<TOKEN end_char="4657" id="token-38-7" morph="none" pos="word" start_char="4655">The</TOKEN>
<TOKEN end_char="4665" id="token-38-8" morph="none" pos="word" start_char="4659">Iceberg</TOKEN>
<TOKEN end_char="4666" id="token-38-9" morph="none" pos="punct" start_char="4666">.</TOKEN>
</SEG>
<SEG end_char="4786" id="segment-39" start_char="4670">
<ORIGINAL_TEXT>If it accidently leaked from China, they owe the rest of the world for the losses that will set us all back a decade.</ORIGINAL_TEXT>
<TOKEN end_char="4671" id="token-39-0" morph="none" pos="word" start_char="4670">If</TOKEN>
<TOKEN end_char="4674" id="token-39-1" morph="none" pos="word" start_char="4673">it</TOKEN>
<TOKEN end_char="4685" id="token-39-2" morph="none" pos="word" start_char="4676">accidently</TOKEN>
<TOKEN end_char="4692" id="token-39-3" morph="none" pos="word" start_char="4687">leaked</TOKEN>
<TOKEN end_char="4697" id="token-39-4" morph="none" pos="word" start_char="4694">from</TOKEN>
<TOKEN end_char="4703" id="token-39-5" morph="none" pos="word" start_char="4699">China</TOKEN>
<TOKEN end_char="4704" id="token-39-6" morph="none" pos="punct" start_char="4704">,</TOKEN>
<TOKEN end_char="4709" id="token-39-7" morph="none" pos="word" start_char="4706">they</TOKEN>
<TOKEN end_char="4713" id="token-39-8" morph="none" pos="word" start_char="4711">owe</TOKEN>
<TOKEN end_char="4717" id="token-39-9" morph="none" pos="word" start_char="4715">the</TOKEN>
<TOKEN end_char="4722" id="token-39-10" morph="none" pos="word" start_char="4719">rest</TOKEN>
<TOKEN end_char="4725" id="token-39-11" morph="none" pos="word" start_char="4724">of</TOKEN>
<TOKEN end_char="4729" id="token-39-12" morph="none" pos="word" start_char="4727">the</TOKEN>
<TOKEN end_char="4735" id="token-39-13" morph="none" pos="word" start_char="4731">world</TOKEN>
<TOKEN end_char="4739" id="token-39-14" morph="none" pos="word" start_char="4737">for</TOKEN>
<TOKEN end_char="4743" id="token-39-15" morph="none" pos="word" start_char="4741">the</TOKEN>
<TOKEN end_char="4750" id="token-39-16" morph="none" pos="word" start_char="4745">losses</TOKEN>
<TOKEN end_char="4755" id="token-39-17" morph="none" pos="word" start_char="4752">that</TOKEN>
<TOKEN end_char="4760" id="token-39-18" morph="none" pos="word" start_char="4757">will</TOKEN>
<TOKEN end_char="4764" id="token-39-19" morph="none" pos="word" start_char="4762">set</TOKEN>
<TOKEN end_char="4767" id="token-39-20" morph="none" pos="word" start_char="4766">us</TOKEN>
<TOKEN end_char="4771" id="token-39-21" morph="none" pos="word" start_char="4769">all</TOKEN>
<TOKEN end_char="4776" id="token-39-22" morph="none" pos="word" start_char="4773">back</TOKEN>
<TOKEN end_char="4778" id="token-39-23" morph="none" pos="word" start_char="4778">a</TOKEN>
<TOKEN end_char="4785" id="token-39-24" morph="none" pos="word" start_char="4780">decade</TOKEN>
<TOKEN end_char="4786" id="token-39-25" morph="none" pos="punct" start_char="4786">.</TOKEN>
</SEG>
<SEG end_char="4927" id="segment-40" start_char="4790">
<ORIGINAL_TEXT>The idea that a bat 800 miles away from Wuhan caused a outbreak of a novel corona virus at a wet market and not the Bio lab is just insane</ORIGINAL_TEXT>
<TOKEN end_char="4792" id="token-40-0" morph="none" pos="word" start_char="4790">The</TOKEN>
<TOKEN end_char="4797" id="token-40-1" morph="none" pos="word" start_char="4794">idea</TOKEN>
<TOKEN end_char="4802" id="token-40-2" morph="none" pos="word" start_char="4799">that</TOKEN>
<TOKEN end_char="4804" id="token-40-3" morph="none" pos="word" start_char="4804">a</TOKEN>
<TOKEN end_char="4808" id="token-40-4" morph="none" pos="word" start_char="4806">bat</TOKEN>
<TOKEN end_char="4812" id="token-40-5" morph="none" pos="word" start_char="4810">800</TOKEN>
<TOKEN end_char="4818" id="token-40-6" morph="none" pos="word" start_char="4814">miles</TOKEN>
<TOKEN end_char="4823" id="token-40-7" morph="none" pos="word" start_char="4820">away</TOKEN>
<TOKEN end_char="4828" id="token-40-8" morph="none" pos="word" start_char="4825">from</TOKEN>
<TOKEN end_char="4834" id="token-40-9" morph="none" pos="word" start_char="4830">Wuhan</TOKEN>
<TOKEN end_char="4841" id="token-40-10" morph="none" pos="word" start_char="4836">caused</TOKEN>
<TOKEN end_char="4843" id="token-40-11" morph="none" pos="word" start_char="4843">a</TOKEN>
<TOKEN end_char="4852" id="token-40-12" morph="none" pos="word" start_char="4845">outbreak</TOKEN>
<TOKEN end_char="4855" id="token-40-13" morph="none" pos="word" start_char="4854">of</TOKEN>
<TOKEN end_char="4857" id="token-40-14" morph="none" pos="word" start_char="4857">a</TOKEN>
<TOKEN end_char="4863" id="token-40-15" morph="none" pos="word" start_char="4859">novel</TOKEN>
<TOKEN end_char="4870" id="token-40-16" morph="none" pos="word" start_char="4865">corona</TOKEN>
<TOKEN end_char="4876" id="token-40-17" morph="none" pos="word" start_char="4872">virus</TOKEN>
<TOKEN end_char="4879" id="token-40-18" morph="none" pos="word" start_char="4878">at</TOKEN>
<TOKEN end_char="4881" id="token-40-19" morph="none" pos="word" start_char="4881">a</TOKEN>
<TOKEN end_char="4885" id="token-40-20" morph="none" pos="word" start_char="4883">wet</TOKEN>
<TOKEN end_char="4892" id="token-40-21" morph="none" pos="word" start_char="4887">market</TOKEN>
<TOKEN end_char="4896" id="token-40-22" morph="none" pos="word" start_char="4894">and</TOKEN>
<TOKEN end_char="4900" id="token-40-23" morph="none" pos="word" start_char="4898">not</TOKEN>
<TOKEN end_char="4904" id="token-40-24" morph="none" pos="word" start_char="4902">the</TOKEN>
<TOKEN end_char="4908" id="token-40-25" morph="none" pos="word" start_char="4906">Bio</TOKEN>
<TOKEN end_char="4912" id="token-40-26" morph="none" pos="word" start_char="4910">lab</TOKEN>
<TOKEN end_char="4915" id="token-40-27" morph="none" pos="word" start_char="4914">is</TOKEN>
<TOKEN end_char="4920" id="token-40-28" morph="none" pos="word" start_char="4917">just</TOKEN>
<TOKEN end_char="4927" id="token-40-29" morph="none" pos="word" start_char="4922">insane</TOKEN>
</SEG>
<SEG end_char="5127" id="segment-41" start_char="4930">
<ORIGINAL_TEXT>So much points towds a accadental release from the Bio lab there is just no way a natraly occurring virus made that trip without leaving a trail of infected citys just as bad if not worse than Wuhan</ORIGINAL_TEXT>
<TOKEN end_char="4931" id="token-41-0" morph="none" pos="word" start_char="4930">So</TOKEN>
<TOKEN end_char="4936" id="token-41-1" morph="none" pos="word" start_char="4933">much</TOKEN>
<TOKEN end_char="4943" id="token-41-2" morph="none" pos="word" start_char="4938">points</TOKEN>
<TOKEN end_char="4949" id="token-41-3" morph="none" pos="word" start_char="4945">towds</TOKEN>
<TOKEN end_char="4951" id="token-41-4" morph="none" pos="word" start_char="4951">a</TOKEN>
<TOKEN end_char="4962" id="token-41-5" morph="none" pos="word" start_char="4953">accadental</TOKEN>
<TOKEN end_char="4970" id="token-41-6" morph="none" pos="word" start_char="4964">release</TOKEN>
<TOKEN end_char="4975" id="token-41-7" morph="none" pos="word" start_char="4972">from</TOKEN>
<TOKEN end_char="4979" id="token-41-8" morph="none" pos="word" start_char="4977">the</TOKEN>
<TOKEN end_char="4983" id="token-41-9" morph="none" pos="word" start_char="4981">Bio</TOKEN>
<TOKEN end_char="4987" id="token-41-10" morph="none" pos="word" start_char="4985">lab</TOKEN>
<TOKEN end_char="4993" id="token-41-11" morph="none" pos="word" start_char="4989">there</TOKEN>
<TOKEN end_char="4996" id="token-41-12" morph="none" pos="word" start_char="4995">is</TOKEN>
<TOKEN end_char="5001" id="token-41-13" morph="none" pos="word" start_char="4998">just</TOKEN>
<TOKEN end_char="5004" id="token-41-14" morph="none" pos="word" start_char="5003">no</TOKEN>
<TOKEN end_char="5008" id="token-41-15" morph="none" pos="word" start_char="5006">way</TOKEN>
<TOKEN end_char="5010" id="token-41-16" morph="none" pos="word" start_char="5010">a</TOKEN>
<TOKEN end_char="5018" id="token-41-17" morph="none" pos="word" start_char="5012">natraly</TOKEN>
<TOKEN end_char="5028" id="token-41-18" morph="none" pos="word" start_char="5020">occurring</TOKEN>
<TOKEN end_char="5034" id="token-41-19" morph="none" pos="word" start_char="5030">virus</TOKEN>
<TOKEN end_char="5039" id="token-41-20" morph="none" pos="word" start_char="5036">made</TOKEN>
<TOKEN end_char="5044" id="token-41-21" morph="none" pos="word" start_char="5041">that</TOKEN>
<TOKEN end_char="5049" id="token-41-22" morph="none" pos="word" start_char="5046">trip</TOKEN>
<TOKEN end_char="5057" id="token-41-23" morph="none" pos="word" start_char="5051">without</TOKEN>
<TOKEN end_char="5065" id="token-41-24" morph="none" pos="word" start_char="5059">leaving</TOKEN>
<TOKEN end_char="5067" id="token-41-25" morph="none" pos="word" start_char="5067">a</TOKEN>
<TOKEN end_char="5073" id="token-41-26" morph="none" pos="word" start_char="5069">trail</TOKEN>
<TOKEN end_char="5076" id="token-41-27" morph="none" pos="word" start_char="5075">of</TOKEN>
<TOKEN end_char="5085" id="token-41-28" morph="none" pos="word" start_char="5078">infected</TOKEN>
<TOKEN end_char="5091" id="token-41-29" morph="none" pos="word" start_char="5087">citys</TOKEN>
<TOKEN end_char="5096" id="token-41-30" morph="none" pos="word" start_char="5093">just</TOKEN>
<TOKEN end_char="5099" id="token-41-31" morph="none" pos="word" start_char="5098">as</TOKEN>
<TOKEN end_char="5103" id="token-41-32" morph="none" pos="word" start_char="5101">bad</TOKEN>
<TOKEN end_char="5106" id="token-41-33" morph="none" pos="word" start_char="5105">if</TOKEN>
<TOKEN end_char="5110" id="token-41-34" morph="none" pos="word" start_char="5108">not</TOKEN>
<TOKEN end_char="5116" id="token-41-35" morph="none" pos="word" start_char="5112">worse</TOKEN>
<TOKEN end_char="5121" id="token-41-36" morph="none" pos="word" start_char="5118">than</TOKEN>
<TOKEN end_char="5127" id="token-41-37" morph="none" pos="word" start_char="5123">Wuhan</TOKEN>
</SEG>
<SEG end_char="5188" id="segment-42" start_char="5130">
<ORIGINAL_TEXT>It came from that lab there is no other logical explanation</ORIGINAL_TEXT>
<TOKEN end_char="5131" id="token-42-0" morph="none" pos="word" start_char="5130">It</TOKEN>
<TOKEN end_char="5136" id="token-42-1" morph="none" pos="word" start_char="5133">came</TOKEN>
<TOKEN end_char="5141" id="token-42-2" morph="none" pos="word" start_char="5138">from</TOKEN>
<TOKEN end_char="5146" id="token-42-3" morph="none" pos="word" start_char="5143">that</TOKEN>
<TOKEN end_char="5150" id="token-42-4" morph="none" pos="word" start_char="5148">lab</TOKEN>
<TOKEN end_char="5156" id="token-42-5" morph="none" pos="word" start_char="5152">there</TOKEN>
<TOKEN end_char="5159" id="token-42-6" morph="none" pos="word" start_char="5158">is</TOKEN>
<TOKEN end_char="5162" id="token-42-7" morph="none" pos="word" start_char="5161">no</TOKEN>
<TOKEN end_char="5168" id="token-42-8" morph="none" pos="word" start_char="5164">other</TOKEN>
<TOKEN end_char="5176" id="token-42-9" morph="none" pos="word" start_char="5170">logical</TOKEN>
<TOKEN end_char="5188" id="token-42-10" morph="none" pos="word" start_char="5178">explanation</TOKEN>
</SEG>
<SEG end_char="5321" id="segment-43" start_char="5191">
<ORIGINAL_TEXT>I personally don't think what escaped was modified but I would not rule out the possibility tho I doubt we will ever know for shure</ORIGINAL_TEXT>
<TOKEN end_char="5191" id="token-43-0" morph="none" pos="word" start_char="5191">I</TOKEN>
<TOKEN end_char="5202" id="token-43-1" morph="none" pos="word" start_char="5193">personally</TOKEN>
<TOKEN end_char="5208" id="token-43-2" morph="none" pos="word" start_char="5204">don't</TOKEN>
<TOKEN end_char="5214" id="token-43-3" morph="none" pos="word" start_char="5210">think</TOKEN>
<TOKEN end_char="5219" id="token-43-4" morph="none" pos="word" start_char="5216">what</TOKEN>
<TOKEN end_char="5227" id="token-43-5" morph="none" pos="word" start_char="5221">escaped</TOKEN>
<TOKEN end_char="5231" id="token-43-6" morph="none" pos="word" start_char="5229">was</TOKEN>
<TOKEN end_char="5240" id="token-43-7" morph="none" pos="word" start_char="5233">modified</TOKEN>
<TOKEN end_char="5244" id="token-43-8" morph="none" pos="word" start_char="5242">but</TOKEN>
<TOKEN end_char="5246" id="token-43-9" morph="none" pos="word" start_char="5246">I</TOKEN>
<TOKEN end_char="5252" id="token-43-10" morph="none" pos="word" start_char="5248">would</TOKEN>
<TOKEN end_char="5256" id="token-43-11" morph="none" pos="word" start_char="5254">not</TOKEN>
<TOKEN end_char="5261" id="token-43-12" morph="none" pos="word" start_char="5258">rule</TOKEN>
<TOKEN end_char="5265" id="token-43-13" morph="none" pos="word" start_char="5263">out</TOKEN>
<TOKEN end_char="5269" id="token-43-14" morph="none" pos="word" start_char="5267">the</TOKEN>
<TOKEN end_char="5281" id="token-43-15" morph="none" pos="word" start_char="5271">possibility</TOKEN>
<TOKEN end_char="5285" id="token-43-16" morph="none" pos="word" start_char="5283">tho</TOKEN>
<TOKEN end_char="5287" id="token-43-17" morph="none" pos="word" start_char="5287">I</TOKEN>
<TOKEN end_char="5293" id="token-43-18" morph="none" pos="word" start_char="5289">doubt</TOKEN>
<TOKEN end_char="5296" id="token-43-19" morph="none" pos="word" start_char="5295">we</TOKEN>
<TOKEN end_char="5301" id="token-43-20" morph="none" pos="word" start_char="5298">will</TOKEN>
<TOKEN end_char="5306" id="token-43-21" morph="none" pos="word" start_char="5303">ever</TOKEN>
<TOKEN end_char="5311" id="token-43-22" morph="none" pos="word" start_char="5308">know</TOKEN>
<TOKEN end_char="5315" id="token-43-23" morph="none" pos="word" start_char="5313">for</TOKEN>
<TOKEN end_char="5321" id="token-43-24" morph="none" pos="word" start_char="5317">shure</TOKEN>
</SEG>
<SEG end_char="5341" id="segment-44" start_char="5325">
<ORIGINAL_TEXT>a reply to: dug88</ORIGINAL_TEXT>
<TOKEN end_char="5325" id="token-44-0" morph="none" pos="word" start_char="5325">a</TOKEN>
<TOKEN end_char="5331" id="token-44-1" morph="none" pos="word" start_char="5327">reply</TOKEN>
<TOKEN end_char="5334" id="token-44-2" morph="none" pos="word" start_char="5333">to</TOKEN>
<TOKEN end_char="5335" id="token-44-3" morph="none" pos="punct" start_char="5335">:</TOKEN>
<TOKEN end_char="5341" id="token-44-4" morph="none" pos="word" start_char="5337">dug88</TOKEN>
</SEG>
<SEG end_char="5399" id="segment-45" start_char="5344">
<ORIGINAL_TEXT>Thanks for posting this dug, but pardon my skepticism...</ORIGINAL_TEXT>
<TOKEN end_char="5349" id="token-45-0" morph="none" pos="word" start_char="5344">Thanks</TOKEN>
<TOKEN end_char="5353" id="token-45-1" morph="none" pos="word" start_char="5351">for</TOKEN>
<TOKEN end_char="5361" id="token-45-2" morph="none" pos="word" start_char="5355">posting</TOKEN>
<TOKEN end_char="5366" id="token-45-3" morph="none" pos="word" start_char="5363">this</TOKEN>
<TOKEN end_char="5370" id="token-45-4" morph="none" pos="word" start_char="5368">dug</TOKEN>
<TOKEN end_char="5371" id="token-45-5" morph="none" pos="punct" start_char="5371">,</TOKEN>
<TOKEN end_char="5375" id="token-45-6" morph="none" pos="word" start_char="5373">but</TOKEN>
<TOKEN end_char="5382" id="token-45-7" morph="none" pos="word" start_char="5377">pardon</TOKEN>
<TOKEN end_char="5385" id="token-45-8" morph="none" pos="word" start_char="5384">my</TOKEN>
<TOKEN end_char="5396" id="token-45-9" morph="none" pos="word" start_char="5387">skepticism</TOKEN>
<TOKEN end_char="5399" id="token-45-10" morph="none" pos="punct" start_char="5397">...</TOKEN>
</SEG>
<SEG end_char="5474" id="segment-46" start_char="5402">
<ORIGINAL_TEXT>An AI, that's gonna legitimatize our whack-a-doodle conspiracy-theories ?</ORIGINAL_TEXT>
<TOKEN end_char="5403" id="token-46-0" morph="none" pos="word" start_char="5402">An</TOKEN>
<TOKEN end_char="5406" id="token-46-1" morph="none" pos="word" start_char="5405">AI</TOKEN>
<TOKEN end_char="5407" id="token-46-2" morph="none" pos="punct" start_char="5407">,</TOKEN>
<TOKEN end_char="5414" id="token-46-3" morph="none" pos="word" start_char="5409">that's</TOKEN>
<TOKEN end_char="5420" id="token-46-4" morph="none" pos="word" start_char="5416">gonna</TOKEN>
<TOKEN end_char="5433" id="token-46-5" morph="none" pos="word" start_char="5422">legitimatize</TOKEN>
<TOKEN end_char="5437" id="token-46-6" morph="none" pos="word" start_char="5435">our</TOKEN>
<TOKEN end_char="5452" id="token-46-7" morph="none" pos="unknown" start_char="5439">whack-a-doodle</TOKEN>
<TOKEN end_char="5472" id="token-46-8" morph="none" pos="unknown" start_char="5454">conspiracy-theories</TOKEN>
<TOKEN end_char="5474" id="token-46-9" morph="none" pos="punct" start_char="5474">?</TOKEN>
</SEG>
<SEG end_char="5483" id="segment-47" start_char="5476">
<ORIGINAL_TEXT>Really ?</ORIGINAL_TEXT>
<TOKEN end_char="5481" id="token-47-0" morph="none" pos="word" start_char="5476">Really</TOKEN>
<TOKEN end_char="5483" id="token-47-1" morph="none" pos="punct" start_char="5483">?</TOKEN>
</SEG>
<SEG end_char="5567" id="segment-48" start_char="5486">
<ORIGINAL_TEXT>From the land, where many tech startups are fronted by 'former' Moss-head agents ?</ORIGINAL_TEXT>
<TOKEN end_char="5489" id="token-48-0" morph="none" pos="word" start_char="5486">From</TOKEN>
<TOKEN end_char="5493" id="token-48-1" morph="none" pos="word" start_char="5491">the</TOKEN>
<TOKEN end_char="5498" id="token-48-2" morph="none" pos="word" start_char="5495">land</TOKEN>
<TOKEN end_char="5499" id="token-48-3" morph="none" pos="punct" start_char="5499">,</TOKEN>
<TOKEN end_char="5505" id="token-48-4" morph="none" pos="word" start_char="5501">where</TOKEN>
<TOKEN end_char="5510" id="token-48-5" morph="none" pos="word" start_char="5507">many</TOKEN>
<TOKEN end_char="5515" id="token-48-6" morph="none" pos="word" start_char="5512">tech</TOKEN>
<TOKEN end_char="5524" id="token-48-7" morph="none" pos="word" start_char="5517">startups</TOKEN>
<TOKEN end_char="5528" id="token-48-8" morph="none" pos="word" start_char="5526">are</TOKEN>
<TOKEN end_char="5536" id="token-48-9" morph="none" pos="word" start_char="5530">fronted</TOKEN>
<TOKEN end_char="5539" id="token-48-10" morph="none" pos="word" start_char="5538">by</TOKEN>
<TOKEN end_char="5541" id="token-48-11" morph="none" pos="punct" start_char="5541">'</TOKEN>
<TOKEN end_char="5547" id="token-48-12" morph="none" pos="word" start_char="5542">former</TOKEN>
<TOKEN end_char="5548" id="token-48-13" morph="none" pos="punct" start_char="5548">'</TOKEN>
<TOKEN end_char="5558" id="token-48-14" morph="none" pos="unknown" start_char="5550">Moss-head</TOKEN>
<TOKEN end_char="5565" id="token-48-15" morph="none" pos="word" start_char="5560">agents</TOKEN>
<TOKEN end_char="5567" id="token-48-16" morph="none" pos="punct" start_char="5567">?</TOKEN>
</SEG>
<SEG end_char="5576" id="segment-49" start_char="5569">
<ORIGINAL_TEXT>Really ?</ORIGINAL_TEXT>
<TOKEN end_char="5574" id="token-49-0" morph="none" pos="word" start_char="5569">Really</TOKEN>
<TOKEN end_char="5576" id="token-49-1" morph="none" pos="punct" start_char="5576">?</TOKEN>
</SEG>
<SEG end_char="5637" id="segment-50" start_char="5579">
<ORIGINAL_TEXT>How soon until they use this, to justify attacks, or wars ?</ORIGINAL_TEXT>
<TOKEN end_char="5581" id="token-50-0" morph="none" pos="word" start_char="5579">How</TOKEN>
<TOKEN end_char="5586" id="token-50-1" morph="none" pos="word" start_char="5583">soon</TOKEN>
<TOKEN end_char="5592" id="token-50-2" morph="none" pos="word" start_char="5588">until</TOKEN>
<TOKEN end_char="5597" id="token-50-3" morph="none" pos="word" start_char="5594">they</TOKEN>
<TOKEN end_char="5601" id="token-50-4" morph="none" pos="word" start_char="5599">use</TOKEN>
<TOKEN end_char="5606" id="token-50-5" morph="none" pos="word" start_char="5603">this</TOKEN>
<TOKEN end_char="5607" id="token-50-6" morph="none" pos="punct" start_char="5607">,</TOKEN>
<TOKEN end_char="5610" id="token-50-7" morph="none" pos="word" start_char="5609">to</TOKEN>
<TOKEN end_char="5618" id="token-50-8" morph="none" pos="word" start_char="5612">justify</TOKEN>
<TOKEN end_char="5626" id="token-50-9" morph="none" pos="word" start_char="5620">attacks</TOKEN>
<TOKEN end_char="5627" id="token-50-10" morph="none" pos="punct" start_char="5627">,</TOKEN>
<TOKEN end_char="5630" id="token-50-11" morph="none" pos="word" start_char="5629">or</TOKEN>
<TOKEN end_char="5635" id="token-50-12" morph="none" pos="word" start_char="5632">wars</TOKEN>
<TOKEN end_char="5637" id="token-50-13" morph="none" pos="punct" start_char="5637">?</TOKEN>
</SEG>
<SEG end_char="5665" id="segment-51" start_char="5640">
<ORIGINAL_TEXT>Just doesn't feel right...</ORIGINAL_TEXT>
<TOKEN end_char="5643" id="token-51-0" morph="none" pos="word" start_char="5640">Just</TOKEN>
<TOKEN end_char="5651" id="token-51-1" morph="none" pos="word" start_char="5645">doesn't</TOKEN>
<TOKEN end_char="5656" id="token-51-2" morph="none" pos="word" start_char="5653">feel</TOKEN>
<TOKEN end_char="5662" id="token-51-3" morph="none" pos="word" start_char="5658">right</TOKEN>
<TOKEN end_char="5665" id="token-51-4" morph="none" pos="punct" start_char="5663">...</TOKEN>
</SEG>
<SEG end_char="5707" id="segment-52" start_char="5668">
<ORIGINAL_TEXT>edit on 30-12-2020 by Nothin because: sp</ORIGINAL_TEXT>
<TOKEN end_char="5671" id="token-52-0" morph="none" pos="word" start_char="5668">edit</TOKEN>
<TOKEN end_char="5674" id="token-52-1" morph="none" pos="word" start_char="5673">on</TOKEN>
<TOKEN end_char="5685" id="token-52-2" morph="none" pos="unknown" start_char="5676">30-12-2020</TOKEN>
<TOKEN end_char="5688" id="token-52-3" morph="none" pos="word" start_char="5687">by</TOKEN>
<TOKEN end_char="5695" id="token-52-4" morph="none" pos="word" start_char="5690">Nothin</TOKEN>
<TOKEN end_char="5703" id="token-52-5" morph="none" pos="word" start_char="5697">because</TOKEN>
<TOKEN end_char="5704" id="token-52-6" morph="none" pos="punct" start_char="5704">:</TOKEN>
<TOKEN end_char="5707" id="token-52-7" morph="none" pos="word" start_char="5706">sp</TOKEN>
</SEG>
<SEG end_char="5884" id="segment-53" start_char="5712">
<ORIGINAL_TEXT>originally posted by: CriticalStinker a reply to: dug88 If it accidently leaked from China, they owe the rest of the world for the losses that will set us all back a decade.</ORIGINAL_TEXT>
<TOKEN end_char="5721" id="token-53-0" morph="none" pos="word" start_char="5712">originally</TOKEN>
<TOKEN end_char="5728" id="token-53-1" morph="none" pos="word" start_char="5723">posted</TOKEN>
<TOKEN end_char="5731" id="token-53-2" morph="none" pos="word" start_char="5730">by</TOKEN>
<TOKEN end_char="5732" id="token-53-3" morph="none" pos="punct" start_char="5732">:</TOKEN>
<TOKEN end_char="5748" id="token-53-4" morph="none" pos="word" start_char="5734">CriticalStinker</TOKEN>
<TOKEN end_char="5750" id="token-53-5" morph="none" pos="word" start_char="5750">a</TOKEN>
<TOKEN end_char="5756" id="token-53-6" morph="none" pos="word" start_char="5752">reply</TOKEN>
<TOKEN end_char="5759" id="token-53-7" morph="none" pos="word" start_char="5758">to</TOKEN>
<TOKEN end_char="5760" id="token-53-8" morph="none" pos="punct" start_char="5760">:</TOKEN>
<TOKEN end_char="5766" id="token-53-9" morph="none" pos="word" start_char="5762">dug88</TOKEN>
<TOKEN end_char="5769" id="token-53-10" morph="none" pos="word" start_char="5768">If</TOKEN>
<TOKEN end_char="5772" id="token-53-11" morph="none" pos="word" start_char="5771">it</TOKEN>
<TOKEN end_char="5783" id="token-53-12" morph="none" pos="word" start_char="5774">accidently</TOKEN>
<TOKEN end_char="5790" id="token-53-13" morph="none" pos="word" start_char="5785">leaked</TOKEN>
<TOKEN end_char="5795" id="token-53-14" morph="none" pos="word" start_char="5792">from</TOKEN>
<TOKEN end_char="5801" id="token-53-15" morph="none" pos="word" start_char="5797">China</TOKEN>
<TOKEN end_char="5802" id="token-53-16" morph="none" pos="punct" start_char="5802">,</TOKEN>
<TOKEN end_char="5807" id="token-53-17" morph="none" pos="word" start_char="5804">they</TOKEN>
<TOKEN end_char="5811" id="token-53-18" morph="none" pos="word" start_char="5809">owe</TOKEN>
<TOKEN end_char="5815" id="token-53-19" morph="none" pos="word" start_char="5813">the</TOKEN>
<TOKEN end_char="5820" id="token-53-20" morph="none" pos="word" start_char="5817">rest</TOKEN>
<TOKEN end_char="5823" id="token-53-21" morph="none" pos="word" start_char="5822">of</TOKEN>
<TOKEN end_char="5827" id="token-53-22" morph="none" pos="word" start_char="5825">the</TOKEN>
<TOKEN end_char="5833" id="token-53-23" morph="none" pos="word" start_char="5829">world</TOKEN>
<TOKEN end_char="5837" id="token-53-24" morph="none" pos="word" start_char="5835">for</TOKEN>
<TOKEN end_char="5841" id="token-53-25" morph="none" pos="word" start_char="5839">the</TOKEN>
<TOKEN end_char="5848" id="token-53-26" morph="none" pos="word" start_char="5843">losses</TOKEN>
<TOKEN end_char="5853" id="token-53-27" morph="none" pos="word" start_char="5850">that</TOKEN>
<TOKEN end_char="5858" id="token-53-28" morph="none" pos="word" start_char="5855">will</TOKEN>
<TOKEN end_char="5862" id="token-53-29" morph="none" pos="word" start_char="5860">set</TOKEN>
<TOKEN end_char="5865" id="token-53-30" morph="none" pos="word" start_char="5864">us</TOKEN>
<TOKEN end_char="5869" id="token-53-31" morph="none" pos="word" start_char="5867">all</TOKEN>
<TOKEN end_char="5874" id="token-53-32" morph="none" pos="word" start_char="5871">back</TOKEN>
<TOKEN end_char="5876" id="token-53-33" morph="none" pos="word" start_char="5876">a</TOKEN>
<TOKEN end_char="5883" id="token-53-34" morph="none" pos="word" start_char="5878">decade</TOKEN>
<TOKEN end_char="5884" id="token-53-35" morph="none" pos="punct" start_char="5884">.</TOKEN>
</SEG>
<SEG end_char="5901" id="segment-54" start_char="5887">
<ORIGINAL_TEXT>Hold up though.</ORIGINAL_TEXT>
<TOKEN end_char="5890" id="token-54-0" morph="none" pos="word" start_char="5887">Hold</TOKEN>
<TOKEN end_char="5893" id="token-54-1" morph="none" pos="word" start_char="5892">up</TOKEN>
<TOKEN end_char="5900" id="token-54-2" morph="none" pos="word" start_char="5895">though</TOKEN>
<TOKEN end_char="5901" id="token-54-3" morph="none" pos="punct" start_char="5901">.</TOKEN>
</SEG>
<SEG end_char="5985" id="segment-55" start_char="5903">
<ORIGINAL_TEXT>If it was being studied at UNC Chapel Hills R lab for GoF capacity (circa 2014-16?</ORIGINAL_TEXT>
<TOKEN end_char="5904" id="token-55-0" morph="none" pos="word" start_char="5903">If</TOKEN>
<TOKEN end_char="5907" id="token-55-1" morph="none" pos="word" start_char="5906">it</TOKEN>
<TOKEN end_char="5911" id="token-55-2" morph="none" pos="word" start_char="5909">was</TOKEN>
<TOKEN end_char="5917" id="token-55-3" morph="none" pos="word" start_char="5913">being</TOKEN>
<TOKEN end_char="5925" id="token-55-4" morph="none" pos="word" start_char="5919">studied</TOKEN>
<TOKEN end_char="5928" id="token-55-5" morph="none" pos="word" start_char="5927">at</TOKEN>
<TOKEN end_char="5932" id="token-55-6" morph="none" pos="word" start_char="5930">UNC</TOKEN>
<TOKEN end_char="5939" id="token-55-7" morph="none" pos="word" start_char="5934">Chapel</TOKEN>
<TOKEN end_char="5946" id="token-55-8" morph="none" pos="word" start_char="5941">Hills</TOKEN>
<TOKEN end_char="5948" id="token-55-9" morph="none" pos="word" start_char="5948">R</TOKEN>
<TOKEN end_char="5952" id="token-55-10" morph="none" pos="word" start_char="5950">lab</TOKEN>
<TOKEN end_char="5956" id="token-55-11" morph="none" pos="word" start_char="5954">for</TOKEN>
<TOKEN end_char="5960" id="token-55-12" morph="none" pos="word" start_char="5958">GoF</TOKEN>
<TOKEN end_char="5969" id="token-55-13" morph="none" pos="word" start_char="5962">capacity</TOKEN>
<TOKEN end_char="5971" id="token-55-14" morph="none" pos="punct" start_char="5971">(</TOKEN>
<TOKEN end_char="5976" id="token-55-15" morph="none" pos="word" start_char="5972">circa</TOKEN>
<TOKEN end_char="5984" id="token-55-16" morph="none" pos="unknown" start_char="5978">2014-16</TOKEN>
<TOKEN end_char="5985" id="token-55-17" morph="none" pos="punct" start_char="5985">?</TOKEN>
</SEG>
<SEG end_char="6070" id="segment-56" start_char="5987">
<ORIGINAL_TEXT>)and when it was learned of the intention and capability, it was shut down there....</ORIGINAL_TEXT>
<TOKEN end_char="5987" id="token-56-0" morph="none" pos="punct" start_char="5987">)</TOKEN>
<TOKEN end_char="5990" id="token-56-1" morph="none" pos="word" start_char="5988">and</TOKEN>
<TOKEN end_char="5995" id="token-56-2" morph="none" pos="word" start_char="5992">when</TOKEN>
<TOKEN end_char="5998" id="token-56-3" morph="none" pos="word" start_char="5997">it</TOKEN>
<TOKEN end_char="6002" id="token-56-4" morph="none" pos="word" start_char="6000">was</TOKEN>
<TOKEN end_char="6010" id="token-56-5" morph="none" pos="word" start_char="6004">learned</TOKEN>
<TOKEN end_char="6013" id="token-56-6" morph="none" pos="word" start_char="6012">of</TOKEN>
<TOKEN end_char="6017" id="token-56-7" morph="none" pos="word" start_char="6015">the</TOKEN>
<TOKEN end_char="6027" id="token-56-8" morph="none" pos="word" start_char="6019">intention</TOKEN>
<TOKEN end_char="6031" id="token-56-9" morph="none" pos="word" start_char="6029">and</TOKEN>
<TOKEN end_char="6042" id="token-56-10" morph="none" pos="word" start_char="6033">capability</TOKEN>
<TOKEN end_char="6043" id="token-56-11" morph="none" pos="punct" start_char="6043">,</TOKEN>
<TOKEN end_char="6046" id="token-56-12" morph="none" pos="word" start_char="6045">it</TOKEN>
<TOKEN end_char="6050" id="token-56-13" morph="none" pos="word" start_char="6048">was</TOKEN>
<TOKEN end_char="6055" id="token-56-14" morph="none" pos="word" start_char="6052">shut</TOKEN>
<TOKEN end_char="6060" id="token-56-15" morph="none" pos="word" start_char="6057">down</TOKEN>
<TOKEN end_char="6066" id="token-56-16" morph="none" pos="word" start_char="6062">there</TOKEN>
<TOKEN end_char="6070" id="token-56-17" morph="none" pos="punct" start_char="6067">....</TOKEN>
</SEG>
<SEG end_char="6161" id="segment-57" start_char="6073">
<ORIGINAL_TEXT>So did we outsource our low-key Geneva List Bio-No-No hot potato to A BSL-4 lab in Wuhan?</ORIGINAL_TEXT>
<TOKEN end_char="6074" id="token-57-0" morph="none" pos="word" start_char="6073">So</TOKEN>
<TOKEN end_char="6078" id="token-57-1" morph="none" pos="word" start_char="6076">did</TOKEN>
<TOKEN end_char="6081" id="token-57-2" morph="none" pos="word" start_char="6080">we</TOKEN>
<TOKEN end_char="6091" id="token-57-3" morph="none" pos="word" start_char="6083">outsource</TOKEN>
<TOKEN end_char="6095" id="token-57-4" morph="none" pos="word" start_char="6093">our</TOKEN>
<TOKEN end_char="6103" id="token-57-5" morph="none" pos="unknown" start_char="6097">low-key</TOKEN>
<TOKEN end_char="6110" id="token-57-6" morph="none" pos="word" start_char="6105">Geneva</TOKEN>
<TOKEN end_char="6115" id="token-57-7" morph="none" pos="word" start_char="6112">List</TOKEN>
<TOKEN end_char="6125" id="token-57-8" morph="none" pos="unknown" start_char="6117">Bio-No-No</TOKEN>
<TOKEN end_char="6129" id="token-57-9" morph="none" pos="word" start_char="6127">hot</TOKEN>
<TOKEN end_char="6136" id="token-57-10" morph="none" pos="word" start_char="6131">potato</TOKEN>
<TOKEN end_char="6139" id="token-57-11" morph="none" pos="word" start_char="6138">to</TOKEN>
<TOKEN end_char="6141" id="token-57-12" morph="none" pos="word" start_char="6141">A</TOKEN>
<TOKEN end_char="6147" id="token-57-13" morph="none" pos="unknown" start_char="6143">BSL-4</TOKEN>
<TOKEN end_char="6151" id="token-57-14" morph="none" pos="word" start_char="6149">lab</TOKEN>
<TOKEN end_char="6154" id="token-57-15" morph="none" pos="word" start_char="6153">in</TOKEN>
<TOKEN end_char="6160" id="token-57-16" morph="none" pos="word" start_char="6156">Wuhan</TOKEN>
<TOKEN end_char="6161" id="token-57-17" morph="none" pos="punct" start_char="6161">?</TOKEN>
</SEG>
<SEG end_char="6192" id="segment-58" start_char="6163">
<ORIGINAL_TEXT>Plausible deniability and all?</ORIGINAL_TEXT>
<TOKEN end_char="6171" id="token-58-0" morph="none" pos="word" start_char="6163">Plausible</TOKEN>
<TOKEN end_char="6183" id="token-58-1" morph="none" pos="word" start_char="6173">deniability</TOKEN>
<TOKEN end_char="6187" id="token-58-2" morph="none" pos="word" start_char="6185">and</TOKEN>
<TOKEN end_char="6191" id="token-58-3" morph="none" pos="word" start_char="6189">all</TOKEN>
<TOKEN end_char="6192" id="token-58-4" morph="none" pos="punct" start_char="6192">?</TOKEN>
</SEG>
<SEG end_char="6246" id="segment-59" start_char="6195">
<ORIGINAL_TEXT>How many licks to the center of the lollipop indeed.</ORIGINAL_TEXT>
<TOKEN end_char="6197" id="token-59-0" morph="none" pos="word" start_char="6195">How</TOKEN>
<TOKEN end_char="6202" id="token-59-1" morph="none" pos="word" start_char="6199">many</TOKEN>
<TOKEN end_char="6208" id="token-59-2" morph="none" pos="word" start_char="6204">licks</TOKEN>
<TOKEN end_char="6211" id="token-59-3" morph="none" pos="word" start_char="6210">to</TOKEN>
<TOKEN end_char="6215" id="token-59-4" morph="none" pos="word" start_char="6213">the</TOKEN>
<TOKEN end_char="6222" id="token-59-5" morph="none" pos="word" start_char="6217">center</TOKEN>
<TOKEN end_char="6225" id="token-59-6" morph="none" pos="word" start_char="6224">of</TOKEN>
<TOKEN end_char="6229" id="token-59-7" morph="none" pos="word" start_char="6227">the</TOKEN>
<TOKEN end_char="6238" id="token-59-8" morph="none" pos="word" start_char="6231">lollipop</TOKEN>
<TOKEN end_char="6245" id="token-59-9" morph="none" pos="word" start_char="6240">indeed</TOKEN>
<TOKEN end_char="6246" id="token-59-10" morph="none" pos="punct" start_char="6246">.</TOKEN>
</SEG>
<SEG end_char="6292" id="segment-60" start_char="6249">
<ORIGINAL_TEXT>Also: dont lick lollipop, it may have Covid</ORIGINAL_TEXT>
<TOKEN end_char="6252" id="token-60-0" morph="none" pos="word" start_char="6249">Also</TOKEN>
<TOKEN end_char="6253" id="token-60-1" morph="none" pos="punct" start_char="6253">:</TOKEN>
<TOKEN end_char="6259" id="token-60-2" morph="none" pos="word" start_char="6255">dont</TOKEN>
<TOKEN end_char="6264" id="token-60-3" morph="none" pos="word" start_char="6261">lick</TOKEN>
<TOKEN end_char="6273" id="token-60-4" morph="none" pos="word" start_char="6266">lollipop</TOKEN>
<TOKEN end_char="6274" id="token-60-5" morph="none" pos="punct" start_char="6274">,</TOKEN>
<TOKEN end_char="6277" id="token-60-6" morph="none" pos="word" start_char="6276">it</TOKEN>
<TOKEN end_char="6281" id="token-60-7" morph="none" pos="word" start_char="6279">may</TOKEN>
<TOKEN end_char="6286" id="token-60-8" morph="none" pos="word" start_char="6283">have</TOKEN>
<TOKEN end_char="6292" id="token-60-9" morph="none" pos="word" start_char="6288">Covid</TOKEN>
</SEG>
<SEG end_char="6353" id="segment-61" start_char="6295">
<ORIGINAL_TEXT>edit on 30-12-2020 by slatesteam because: (no reason given)</ORIGINAL_TEXT>
<TOKEN end_char="6298" id="token-61-0" morph="none" pos="word" start_char="6295">edit</TOKEN>
<TOKEN end_char="6301" id="token-61-1" morph="none" pos="word" start_char="6300">on</TOKEN>
<TOKEN end_char="6312" id="token-61-2" morph="none" pos="unknown" start_char="6303">30-12-2020</TOKEN>
<TOKEN end_char="6315" id="token-61-3" morph="none" pos="word" start_char="6314">by</TOKEN>
<TOKEN end_char="6326" id="token-61-4" morph="none" pos="word" start_char="6317">slatesteam</TOKEN>
<TOKEN end_char="6334" id="token-61-5" morph="none" pos="word" start_char="6328">because</TOKEN>
<TOKEN end_char="6335" id="token-61-6" morph="none" pos="punct" start_char="6335">:</TOKEN>
<TOKEN end_char="6337" id="token-61-7" morph="none" pos="punct" start_char="6337">(</TOKEN>
<TOKEN end_char="6339" id="token-61-8" morph="none" pos="word" start_char="6338">no</TOKEN>
<TOKEN end_char="6346" id="token-61-9" morph="none" pos="word" start_char="6341">reason</TOKEN>
<TOKEN end_char="6352" id="token-61-10" morph="none" pos="word" start_char="6348">given</TOKEN>
<TOKEN end_char="6353" id="token-61-11" morph="none" pos="punct" start_char="6353">)</TOKEN>
</SEG>
<SEG end_char="6414" id="segment-62" start_char="6356">
<ORIGINAL_TEXT>edit on 30-12-2020 by slatesteam because: (no reason given)</ORIGINAL_TEXT>
<TOKEN end_char="6359" id="token-62-0" morph="none" pos="word" start_char="6356">edit</TOKEN>
<TOKEN end_char="6362" id="token-62-1" morph="none" pos="word" start_char="6361">on</TOKEN>
<TOKEN end_char="6373" id="token-62-2" morph="none" pos="unknown" start_char="6364">30-12-2020</TOKEN>
<TOKEN end_char="6376" id="token-62-3" morph="none" pos="word" start_char="6375">by</TOKEN>
<TOKEN end_char="6387" id="token-62-4" morph="none" pos="word" start_char="6378">slatesteam</TOKEN>
<TOKEN end_char="6395" id="token-62-5" morph="none" pos="word" start_char="6389">because</TOKEN>
<TOKEN end_char="6396" id="token-62-6" morph="none" pos="punct" start_char="6396">:</TOKEN>
<TOKEN end_char="6398" id="token-62-7" morph="none" pos="punct" start_char="6398">(</TOKEN>
<TOKEN end_char="6400" id="token-62-8" morph="none" pos="word" start_char="6399">no</TOKEN>
<TOKEN end_char="6407" id="token-62-9" morph="none" pos="word" start_char="6402">reason</TOKEN>
<TOKEN end_char="6413" id="token-62-10" morph="none" pos="word" start_char="6409">given</TOKEN>
<TOKEN end_char="6414" id="token-62-11" morph="none" pos="punct" start_char="6414">)</TOKEN>
</SEG>
<SEG end_char="6477" id="segment-63" start_char="6419">
<ORIGINAL_TEXT>edit on 30-12-2020 by slatesteam because: (no reason given)</ORIGINAL_TEXT>
<TOKEN end_char="6422" id="token-63-0" morph="none" pos="word" start_char="6419">edit</TOKEN>
<TOKEN end_char="6425" id="token-63-1" morph="none" pos="word" start_char="6424">on</TOKEN>
<TOKEN end_char="6436" id="token-63-2" morph="none" pos="unknown" start_char="6427">30-12-2020</TOKEN>
<TOKEN end_char="6439" id="token-63-3" morph="none" pos="word" start_char="6438">by</TOKEN>
<TOKEN end_char="6450" id="token-63-4" morph="none" pos="word" start_char="6441">slatesteam</TOKEN>
<TOKEN end_char="6458" id="token-63-5" morph="none" pos="word" start_char="6452">because</TOKEN>
<TOKEN end_char="6459" id="token-63-6" morph="none" pos="punct" start_char="6459">:</TOKEN>
<TOKEN end_char="6461" id="token-63-7" morph="none" pos="punct" start_char="6461">(</TOKEN>
<TOKEN end_char="6463" id="token-63-8" morph="none" pos="word" start_char="6462">no</TOKEN>
<TOKEN end_char="6470" id="token-63-9" morph="none" pos="word" start_char="6465">reason</TOKEN>
<TOKEN end_char="6476" id="token-63-10" morph="none" pos="word" start_char="6472">given</TOKEN>
<TOKEN end_char="6477" id="token-63-11" morph="none" pos="punct" start_char="6477">)</TOKEN>
</SEG>
<SEG end_char="6498" id="segment-64" start_char="6481">
<ORIGINAL_TEXT>a reply to: Nothin</ORIGINAL_TEXT>
<TOKEN end_char="6481" id="token-64-0" morph="none" pos="word" start_char="6481">a</TOKEN>
<TOKEN end_char="6487" id="token-64-1" morph="none" pos="word" start_char="6483">reply</TOKEN>
<TOKEN end_char="6490" id="token-64-2" morph="none" pos="word" start_char="6489">to</TOKEN>
<TOKEN end_char="6491" id="token-64-3" morph="none" pos="punct" start_char="6491">:</TOKEN>
<TOKEN end_char="6498" id="token-64-4" morph="none" pos="word" start_char="6493">Nothin</TOKEN>
</SEG>
<SEG end_char="6597" id="segment-65" start_char="6501">
<ORIGINAL_TEXT>They do have their evidence and reasoning available for scrutiny should you choose to look at it.</ORIGINAL_TEXT>
<TOKEN end_char="6504" id="token-65-0" morph="none" pos="word" start_char="6501">They</TOKEN>
<TOKEN end_char="6507" id="token-65-1" morph="none" pos="word" start_char="6506">do</TOKEN>
<TOKEN end_char="6512" id="token-65-2" morph="none" pos="word" start_char="6509">have</TOKEN>
<TOKEN end_char="6518" id="token-65-3" morph="none" pos="word" start_char="6514">their</TOKEN>
<TOKEN end_char="6527" id="token-65-4" morph="none" pos="word" start_char="6520">evidence</TOKEN>
<TOKEN end_char="6531" id="token-65-5" morph="none" pos="word" start_char="6529">and</TOKEN>
<TOKEN end_char="6541" id="token-65-6" morph="none" pos="word" start_char="6533">reasoning</TOKEN>
<TOKEN end_char="6551" id="token-65-7" morph="none" pos="word" start_char="6543">available</TOKEN>
<TOKEN end_char="6555" id="token-65-8" morph="none" pos="word" start_char="6553">for</TOKEN>
<TOKEN end_char="6564" id="token-65-9" morph="none" pos="word" start_char="6557">scrutiny</TOKEN>
<TOKEN end_char="6571" id="token-65-10" morph="none" pos="word" start_char="6566">should</TOKEN>
<TOKEN end_char="6575" id="token-65-11" morph="none" pos="word" start_char="6573">you</TOKEN>
<TOKEN end_char="6582" id="token-65-12" morph="none" pos="word" start_char="6577">choose</TOKEN>
<TOKEN end_char="6585" id="token-65-13" morph="none" pos="word" start_char="6584">to</TOKEN>
<TOKEN end_char="6590" id="token-65-14" morph="none" pos="word" start_char="6587">look</TOKEN>
<TOKEN end_char="6593" id="token-65-15" morph="none" pos="word" start_char="6592">at</TOKEN>
<TOKEN end_char="6596" id="token-65-16" morph="none" pos="word" start_char="6595">it</TOKEN>
<TOKEN end_char="6597" id="token-65-17" morph="none" pos="punct" start_char="6597">.</TOKEN>
</SEG>
<SEG end_char="6665" id="segment-66" start_char="6599">
<ORIGINAL_TEXT>I wouldn't bother waiting for the cnn summary, I doubt it's coming.</ORIGINAL_TEXT>
<TOKEN end_char="6599" id="token-66-0" morph="none" pos="word" start_char="6599">I</TOKEN>
<TOKEN end_char="6608" id="token-66-1" morph="none" pos="word" start_char="6601">wouldn't</TOKEN>
<TOKEN end_char="6615" id="token-66-2" morph="none" pos="word" start_char="6610">bother</TOKEN>
<TOKEN end_char="6623" id="token-66-3" morph="none" pos="word" start_char="6617">waiting</TOKEN>
<TOKEN end_char="6627" id="token-66-4" morph="none" pos="word" start_char="6625">for</TOKEN>
<TOKEN end_char="6631" id="token-66-5" morph="none" pos="word" start_char="6629">the</TOKEN>
<TOKEN end_char="6635" id="token-66-6" morph="none" pos="word" start_char="6633">cnn</TOKEN>
<TOKEN end_char="6643" id="token-66-7" morph="none" pos="word" start_char="6637">summary</TOKEN>
<TOKEN end_char="6644" id="token-66-8" morph="none" pos="punct" start_char="6644">,</TOKEN>
<TOKEN end_char="6646" id="token-66-9" morph="none" pos="word" start_char="6646">I</TOKEN>
<TOKEN end_char="6652" id="token-66-10" morph="none" pos="word" start_char="6648">doubt</TOKEN>
<TOKEN end_char="6657" id="token-66-11" morph="none" pos="word" start_char="6654">it's</TOKEN>
<TOKEN end_char="6664" id="token-66-12" morph="none" pos="word" start_char="6659">coming</TOKEN>
<TOKEN end_char="6665" id="token-66-13" morph="none" pos="punct" start_char="6665">.</TOKEN>
</SEG>
<SEG end_char="6721" id="segment-67" start_char="6668">
<ORIGINAL_TEXT>edit on 30/12/2020 by dug88 because: (no reason given)</ORIGINAL_TEXT>
<TOKEN end_char="6671" id="token-67-0" morph="none" pos="word" start_char="6668">edit</TOKEN>
<TOKEN end_char="6674" id="token-67-1" morph="none" pos="word" start_char="6673">on</TOKEN>
<TOKEN end_char="6685" id="token-67-2" morph="none" pos="unknown" start_char="6676">30/12/2020</TOKEN>
<TOKEN end_char="6688" id="token-67-3" morph="none" pos="word" start_char="6687">by</TOKEN>
<TOKEN end_char="6694" id="token-67-4" morph="none" pos="word" start_char="6690">dug88</TOKEN>
<TOKEN end_char="6702" id="token-67-5" morph="none" pos="word" start_char="6696">because</TOKEN>
<TOKEN end_char="6703" id="token-67-6" morph="none" pos="punct" start_char="6703">:</TOKEN>
<TOKEN end_char="6705" id="token-67-7" morph="none" pos="punct" start_char="6705">(</TOKEN>
<TOKEN end_char="6707" id="token-67-8" morph="none" pos="word" start_char="6706">no</TOKEN>
<TOKEN end_char="6714" id="token-67-9" morph="none" pos="word" start_char="6709">reason</TOKEN>
<TOKEN end_char="6720" id="token-67-10" morph="none" pos="word" start_char="6716">given</TOKEN>
<TOKEN end_char="6721" id="token-67-11" morph="none" pos="punct" start_char="6721">)</TOKEN>
</SEG>
<SEG end_char="6741" id="segment-68" start_char="6725">
<ORIGINAL_TEXT>a reply to: dug88</ORIGINAL_TEXT>
<TOKEN end_char="6725" id="token-68-0" morph="none" pos="word" start_char="6725">a</TOKEN>
<TOKEN end_char="6731" id="token-68-1" morph="none" pos="word" start_char="6727">reply</TOKEN>
<TOKEN end_char="6734" id="token-68-2" morph="none" pos="word" start_char="6733">to</TOKEN>
<TOKEN end_char="6735" id="token-68-3" morph="none" pos="punct" start_char="6735">:</TOKEN>
<TOKEN end_char="6741" id="token-68-4" morph="none" pos="word" start_char="6737">dug88</TOKEN>
</SEG>
<SEG end_char="6890" id="segment-69" start_char="6744">
<ORIGINAL_TEXT>We could perhaps find the evidence and reasoning that Alexa uses as well, but still wouldn't trust her for the recipe to boil-water, or make toast.</ORIGINAL_TEXT>
<TOKEN end_char="6745" id="token-69-0" morph="none" pos="word" start_char="6744">We</TOKEN>
<TOKEN end_char="6751" id="token-69-1" morph="none" pos="word" start_char="6747">could</TOKEN>
<TOKEN end_char="6759" id="token-69-2" morph="none" pos="word" start_char="6753">perhaps</TOKEN>
<TOKEN end_char="6764" id="token-69-3" morph="none" pos="word" start_char="6761">find</TOKEN>
<TOKEN end_char="6768" id="token-69-4" morph="none" pos="word" start_char="6766">the</TOKEN>
<TOKEN end_char="6777" id="token-69-5" morph="none" pos="word" start_char="6770">evidence</TOKEN>
<TOKEN end_char="6781" id="token-69-6" morph="none" pos="word" start_char="6779">and</TOKEN>
<TOKEN end_char="6791" id="token-69-7" morph="none" pos="word" start_char="6783">reasoning</TOKEN>
<TOKEN end_char="6796" id="token-69-8" morph="none" pos="word" start_char="6793">that</TOKEN>
<TOKEN end_char="6802" id="token-69-9" morph="none" pos="word" start_char="6798">Alexa</TOKEN>
<TOKEN end_char="6807" id="token-69-10" morph="none" pos="word" start_char="6804">uses</TOKEN>
<TOKEN end_char="6810" id="token-69-11" morph="none" pos="word" start_char="6809">as</TOKEN>
<TOKEN end_char="6815" id="token-69-12" morph="none" pos="word" start_char="6812">well</TOKEN>
<TOKEN end_char="6816" id="token-69-13" morph="none" pos="punct" start_char="6816">,</TOKEN>
<TOKEN end_char="6820" id="token-69-14" morph="none" pos="word" start_char="6818">but</TOKEN>
<TOKEN end_char="6826" id="token-69-15" morph="none" pos="word" start_char="6822">still</TOKEN>
<TOKEN end_char="6835" id="token-69-16" morph="none" pos="word" start_char="6828">wouldn't</TOKEN>
<TOKEN end_char="6841" id="token-69-17" morph="none" pos="word" start_char="6837">trust</TOKEN>
<TOKEN end_char="6845" id="token-69-18" morph="none" pos="word" start_char="6843">her</TOKEN>
<TOKEN end_char="6849" id="token-69-19" morph="none" pos="word" start_char="6847">for</TOKEN>
<TOKEN end_char="6853" id="token-69-20" morph="none" pos="word" start_char="6851">the</TOKEN>
<TOKEN end_char="6860" id="token-69-21" morph="none" pos="word" start_char="6855">recipe</TOKEN>
<TOKEN end_char="6863" id="token-69-22" morph="none" pos="word" start_char="6862">to</TOKEN>
<TOKEN end_char="6874" id="token-69-23" morph="none" pos="unknown" start_char="6865">boil-water</TOKEN>
<TOKEN end_char="6875" id="token-69-24" morph="none" pos="punct" start_char="6875">,</TOKEN>
<TOKEN end_char="6878" id="token-69-25" morph="none" pos="word" start_char="6877">or</TOKEN>
<TOKEN end_char="6883" id="token-69-26" morph="none" pos="word" start_char="6880">make</TOKEN>
<TOKEN end_char="6889" id="token-69-27" morph="none" pos="word" start_char="6885">toast</TOKEN>
<TOKEN end_char="6890" id="token-69-28" morph="none" pos="punct" start_char="6890">.</TOKEN>
</SEG>
<SEG end_char="6919" id="segment-70" start_char="6893">
<ORIGINAL_TEXT>It just don't feel right...</ORIGINAL_TEXT>
<TOKEN end_char="6894" id="token-70-0" morph="none" pos="word" start_char="6893">It</TOKEN>
<TOKEN end_char="6899" id="token-70-1" morph="none" pos="word" start_char="6896">just</TOKEN>
<TOKEN end_char="6905" id="token-70-2" morph="none" pos="word" start_char="6901">don't</TOKEN>
<TOKEN end_char="6910" id="token-70-3" morph="none" pos="word" start_char="6907">feel</TOKEN>
<TOKEN end_char="6916" id="token-70-4" morph="none" pos="word" start_char="6912">right</TOKEN>
<TOKEN end_char="6919" id="token-70-5" morph="none" pos="punct" start_char="6917">...</TOKEN>
</SEG>
<SEG end_char="7146" id="segment-71" start_char="6924">
<ORIGINAL_TEXT>originally posted by: dug88 An Israeli startup named Rootclaim has determined that there is an 81% chance that the SARS-cov-2 was accidentally released from the Wuhan bioresearch laboratory during gain-of-function research.</ORIGINAL_TEXT>
<TOKEN end_char="6933" id="token-71-0" morph="none" pos="word" start_char="6924">originally</TOKEN>
<TOKEN end_char="6940" id="token-71-1" morph="none" pos="word" start_char="6935">posted</TOKEN>
<TOKEN end_char="6943" id="token-71-2" morph="none" pos="word" start_char="6942">by</TOKEN>
<TOKEN end_char="6944" id="token-71-3" morph="none" pos="punct" start_char="6944">:</TOKEN>
<TOKEN end_char="6950" id="token-71-4" morph="none" pos="word" start_char="6946">dug88</TOKEN>
<TOKEN end_char="6953" id="token-71-5" morph="none" pos="word" start_char="6952">An</TOKEN>
<TOKEN end_char="6961" id="token-71-6" morph="none" pos="word" start_char="6955">Israeli</TOKEN>
<TOKEN end_char="6969" id="token-71-7" morph="none" pos="word" start_char="6963">startup</TOKEN>
<TOKEN end_char="6975" id="token-71-8" morph="none" pos="word" start_char="6971">named</TOKEN>
<TOKEN end_char="6985" id="token-71-9" morph="none" pos="word" start_char="6977">Rootclaim</TOKEN>
<TOKEN end_char="6989" id="token-71-10" morph="none" pos="word" start_char="6987">has</TOKEN>
<TOKEN end_char="7000" id="token-71-11" morph="none" pos="word" start_char="6991">determined</TOKEN>
<TOKEN end_char="7005" id="token-71-12" morph="none" pos="word" start_char="7002">that</TOKEN>
<TOKEN end_char="7011" id="token-71-13" morph="none" pos="word" start_char="7007">there</TOKEN>
<TOKEN end_char="7014" id="token-71-14" morph="none" pos="word" start_char="7013">is</TOKEN>
<TOKEN end_char="7017" id="token-71-15" morph="none" pos="word" start_char="7016">an</TOKEN>
<TOKEN end_char="7020" id="token-71-16" morph="none" pos="word" start_char="7019">81</TOKEN>
<TOKEN end_char="7021" id="token-71-17" morph="none" pos="punct" start_char="7021">%</TOKEN>
<TOKEN end_char="7028" id="token-71-18" morph="none" pos="word" start_char="7023">chance</TOKEN>
<TOKEN end_char="7033" id="token-71-19" morph="none" pos="word" start_char="7030">that</TOKEN>
<TOKEN end_char="7037" id="token-71-20" morph="none" pos="word" start_char="7035">the</TOKEN>
<TOKEN end_char="7048" id="token-71-21" morph="none" pos="unknown" start_char="7039">SARS-cov-2</TOKEN>
<TOKEN end_char="7052" id="token-71-22" morph="none" pos="word" start_char="7050">was</TOKEN>
<TOKEN end_char="7065" id="token-71-23" morph="none" pos="word" start_char="7054">accidentally</TOKEN>
<TOKEN end_char="7074" id="token-71-24" morph="none" pos="word" start_char="7067">released</TOKEN>
<TOKEN end_char="7079" id="token-71-25" morph="none" pos="word" start_char="7076">from</TOKEN>
<TOKEN end_char="7083" id="token-71-26" morph="none" pos="word" start_char="7081">the</TOKEN>
<TOKEN end_char="7089" id="token-71-27" morph="none" pos="word" start_char="7085">Wuhan</TOKEN>
<TOKEN end_char="7101" id="token-71-28" morph="none" pos="word" start_char="7091">bioresearch</TOKEN>
<TOKEN end_char="7112" id="token-71-29" morph="none" pos="word" start_char="7103">laboratory</TOKEN>
<TOKEN end_char="7119" id="token-71-30" morph="none" pos="word" start_char="7114">during</TOKEN>
<TOKEN end_char="7136" id="token-71-31" morph="none" pos="unknown" start_char="7121">gain-of-function</TOKEN>
<TOKEN end_char="7145" id="token-71-32" morph="none" pos="word" start_char="7138">research</TOKEN>
<TOKEN end_char="7146" id="token-71-33" morph="none" pos="punct" start_char="7146">.</TOKEN>
</SEG>
<SEG end_char="7255" id="segment-72" start_char="7148">
<ORIGINAL_TEXT>Rootclaim is, i'm assuming, an ai driven service that does this: According to rootclaim www.rootclaim.com...</ORIGINAL_TEXT>
<TOKEN end_char="7156" id="token-72-0" morph="none" pos="word" start_char="7148">Rootclaim</TOKEN>
<TOKEN end_char="7159" id="token-72-1" morph="none" pos="word" start_char="7158">is</TOKEN>
<TOKEN end_char="7160" id="token-72-2" morph="none" pos="punct" start_char="7160">,</TOKEN>
<TOKEN end_char="7164" id="token-72-3" morph="none" pos="word" start_char="7162">i'm</TOKEN>
<TOKEN end_char="7173" id="token-72-4" morph="none" pos="word" start_char="7166">assuming</TOKEN>
<TOKEN end_char="7174" id="token-72-5" morph="none" pos="punct" start_char="7174">,</TOKEN>
<TOKEN end_char="7177" id="token-72-6" morph="none" pos="word" start_char="7176">an</TOKEN>
<TOKEN end_char="7180" id="token-72-7" morph="none" pos="word" start_char="7179">ai</TOKEN>
<TOKEN end_char="7187" id="token-72-8" morph="none" pos="word" start_char="7182">driven</TOKEN>
<TOKEN end_char="7195" id="token-72-9" morph="none" pos="word" start_char="7189">service</TOKEN>
<TOKEN end_char="7200" id="token-72-10" morph="none" pos="word" start_char="7197">that</TOKEN>
<TOKEN end_char="7205" id="token-72-11" morph="none" pos="word" start_char="7202">does</TOKEN>
<TOKEN end_char="7210" id="token-72-12" morph="none" pos="word" start_char="7207">this</TOKEN>
<TOKEN end_char="7211" id="token-72-13" morph="none" pos="punct" start_char="7211">:</TOKEN>
<TOKEN end_char="7221" id="token-72-14" morph="none" pos="word" start_char="7213">According</TOKEN>
<TOKEN end_char="7224" id="token-72-15" morph="none" pos="word" start_char="7223">to</TOKEN>
<TOKEN end_char="7234" id="token-72-16" morph="none" pos="word" start_char="7226">rootclaim</TOKEN>
<TOKEN end_char="7255" id="token-72-17" morph="none" pos="url" start_char="7236">www.rootclaim.com...</TOKEN>
</SEG>
<SEG end_char="7336" id="segment-73" start_char="7257">
<ORIGINAL_TEXT>I'm sure this will likely be dismissed by the majority of media and many people.</ORIGINAL_TEXT>
<TOKEN end_char="7259" id="token-73-0" morph="none" pos="word" start_char="7257">I'm</TOKEN>
<TOKEN end_char="7264" id="token-73-1" morph="none" pos="word" start_char="7261">sure</TOKEN>
<TOKEN end_char="7269" id="token-73-2" morph="none" pos="word" start_char="7266">this</TOKEN>
<TOKEN end_char="7274" id="token-73-3" morph="none" pos="word" start_char="7271">will</TOKEN>
<TOKEN end_char="7281" id="token-73-4" morph="none" pos="word" start_char="7276">likely</TOKEN>
<TOKEN end_char="7284" id="token-73-5" morph="none" pos="word" start_char="7283">be</TOKEN>
<TOKEN end_char="7294" id="token-73-6" morph="none" pos="word" start_char="7286">dismissed</TOKEN>
<TOKEN end_char="7297" id="token-73-7" morph="none" pos="word" start_char="7296">by</TOKEN>
<TOKEN end_char="7301" id="token-73-8" morph="none" pos="word" start_char="7299">the</TOKEN>
<TOKEN end_char="7310" id="token-73-9" morph="none" pos="word" start_char="7303">majority</TOKEN>
<TOKEN end_char="7313" id="token-73-10" morph="none" pos="word" start_char="7312">of</TOKEN>
<TOKEN end_char="7319" id="token-73-11" morph="none" pos="word" start_char="7315">media</TOKEN>
<TOKEN end_char="7323" id="token-73-12" morph="none" pos="word" start_char="7321">and</TOKEN>
<TOKEN end_char="7328" id="token-73-13" morph="none" pos="word" start_char="7325">many</TOKEN>
<TOKEN end_char="7335" id="token-73-14" morph="none" pos="word" start_char="7330">people</TOKEN>
<TOKEN end_char="7336" id="token-73-15" morph="none" pos="punct" start_char="7336">.</TOKEN>
</SEG>
<SEG end_char="7384" id="segment-74" start_char="7338">
<ORIGINAL_TEXT>But it seems like it's worth looking more into.</ORIGINAL_TEXT>
<TOKEN end_char="7340" id="token-74-0" morph="none" pos="word" start_char="7338">But</TOKEN>
<TOKEN end_char="7343" id="token-74-1" morph="none" pos="word" start_char="7342">it</TOKEN>
<TOKEN end_char="7349" id="token-74-2" morph="none" pos="word" start_char="7345">seems</TOKEN>
<TOKEN end_char="7354" id="token-74-3" morph="none" pos="word" start_char="7351">like</TOKEN>
<TOKEN end_char="7359" id="token-74-4" morph="none" pos="word" start_char="7356">it's</TOKEN>
<TOKEN end_char="7365" id="token-74-5" morph="none" pos="word" start_char="7361">worth</TOKEN>
<TOKEN end_char="7373" id="token-74-6" morph="none" pos="word" start_char="7367">looking</TOKEN>
<TOKEN end_char="7378" id="token-74-7" morph="none" pos="word" start_char="7375">more</TOKEN>
<TOKEN end_char="7383" id="token-74-8" morph="none" pos="word" start_char="7380">into</TOKEN>
<TOKEN end_char="7384" id="token-74-9" morph="none" pos="punct" start_char="7384">.</TOKEN>
</SEG>
<SEG end_char="7451" id="segment-75" start_char="7386">
<ORIGINAL_TEXT>The link goes more into the evidence behind the eventual decision.</ORIGINAL_TEXT>
<TOKEN end_char="7388" id="token-75-0" morph="none" pos="word" start_char="7386">The</TOKEN>
<TOKEN end_char="7393" id="token-75-1" morph="none" pos="word" start_char="7390">link</TOKEN>
<TOKEN end_char="7398" id="token-75-2" morph="none" pos="word" start_char="7395">goes</TOKEN>
<TOKEN end_char="7403" id="token-75-3" morph="none" pos="word" start_char="7400">more</TOKEN>
<TOKEN end_char="7408" id="token-75-4" morph="none" pos="word" start_char="7405">into</TOKEN>
<TOKEN end_char="7412" id="token-75-5" morph="none" pos="word" start_char="7410">the</TOKEN>
<TOKEN end_char="7421" id="token-75-6" morph="none" pos="word" start_char="7414">evidence</TOKEN>
<TOKEN end_char="7428" id="token-75-7" morph="none" pos="word" start_char="7423">behind</TOKEN>
<TOKEN end_char="7432" id="token-75-8" morph="none" pos="word" start_char="7430">the</TOKEN>
<TOKEN end_char="7441" id="token-75-9" morph="none" pos="word" start_char="7434">eventual</TOKEN>
<TOKEN end_char="7450" id="token-75-10" morph="none" pos="word" start_char="7443">decision</TOKEN>
<TOKEN end_char="7451" id="token-75-11" morph="none" pos="punct" start_char="7451">.</TOKEN>
</SEG>
<SEG end_char="7500" id="segment-76" start_char="7453">
<ORIGINAL_TEXT>ETA: a bit of info on gain of function research.</ORIGINAL_TEXT>
<TOKEN end_char="7455" id="token-76-0" morph="none" pos="word" start_char="7453">ETA</TOKEN>
<TOKEN end_char="7456" id="token-76-1" morph="none" pos="punct" start_char="7456">:</TOKEN>
<TOKEN end_char="7458" id="token-76-2" morph="none" pos="word" start_char="7458">a</TOKEN>
<TOKEN end_char="7462" id="token-76-3" morph="none" pos="word" start_char="7460">bit</TOKEN>
<TOKEN end_char="7465" id="token-76-4" morph="none" pos="word" start_char="7464">of</TOKEN>
<TOKEN end_char="7470" id="token-76-5" morph="none" pos="word" start_char="7467">info</TOKEN>
<TOKEN end_char="7473" id="token-76-6" morph="none" pos="word" start_char="7472">on</TOKEN>
<TOKEN end_char="7478" id="token-76-7" morph="none" pos="word" start_char="7475">gain</TOKEN>
<TOKEN end_char="7481" id="token-76-8" morph="none" pos="word" start_char="7480">of</TOKEN>
<TOKEN end_char="7490" id="token-76-9" morph="none" pos="word" start_char="7483">function</TOKEN>
<TOKEN end_char="7499" id="token-76-10" morph="none" pos="word" start_char="7492">research</TOKEN>
<TOKEN end_char="7500" id="token-76-11" morph="none" pos="punct" start_char="7500">.</TOKEN>
</SEG>
<SEG end_char="7524" id="segment-77" start_char="7502">
<ORIGINAL_TEXT>www.ncbi.nlm.nih.gov...</ORIGINAL_TEXT>
<TOKEN end_char="7524" id="token-77-0" morph="none" pos="url" start_char="7502">www.ncbi.nlm.nih.gov...</TOKEN>
<TRANSLATED_TEXT>www.ncbi.nlm.nih.gov.</TRANSLATED_TEXT><DETECTED_LANGUAGE>hr</DETECTED_LANGUAGE></SEG>
<SEG end_char="7609" id="segment-78" start_char="7527">
<ORIGINAL_TEXT>Most of us who have any damn sense or ability to reason knew this about a year ago.</ORIGINAL_TEXT>
<TOKEN end_char="7530" id="token-78-0" morph="none" pos="word" start_char="7527">Most</TOKEN>
<TOKEN end_char="7533" id="token-78-1" morph="none" pos="word" start_char="7532">of</TOKEN>
<TOKEN end_char="7536" id="token-78-2" morph="none" pos="word" start_char="7535">us</TOKEN>
<TOKEN end_char="7540" id="token-78-3" morph="none" pos="word" start_char="7538">who</TOKEN>
<TOKEN end_char="7545" id="token-78-4" morph="none" pos="word" start_char="7542">have</TOKEN>
<TOKEN end_char="7549" id="token-78-5" morph="none" pos="word" start_char="7547">any</TOKEN>
<TOKEN end_char="7554" id="token-78-6" morph="none" pos="word" start_char="7551">damn</TOKEN>
<TOKEN end_char="7560" id="token-78-7" morph="none" pos="word" start_char="7556">sense</TOKEN>
<TOKEN end_char="7563" id="token-78-8" morph="none" pos="word" start_char="7562">or</TOKEN>
<TOKEN end_char="7571" id="token-78-9" morph="none" pos="word" start_char="7565">ability</TOKEN>
<TOKEN end_char="7574" id="token-78-10" morph="none" pos="word" start_char="7573">to</TOKEN>
<TOKEN end_char="7581" id="token-78-11" morph="none" pos="word" start_char="7576">reason</TOKEN>
<TOKEN end_char="7586" id="token-78-12" morph="none" pos="word" start_char="7583">knew</TOKEN>
<TOKEN end_char="7591" id="token-78-13" morph="none" pos="word" start_char="7588">this</TOKEN>
<TOKEN end_char="7597" id="token-78-14" morph="none" pos="word" start_char="7593">about</TOKEN>
<TOKEN end_char="7599" id="token-78-15" morph="none" pos="word" start_char="7599">a</TOKEN>
<TOKEN end_char="7604" id="token-78-16" morph="none" pos="word" start_char="7601">year</TOKEN>
<TOKEN end_char="7608" id="token-78-17" morph="none" pos="word" start_char="7606">ago</TOKEN>
<TOKEN end_char="7609" id="token-78-18" morph="none" pos="punct" start_char="7609">.</TOKEN>
</SEG>
<SEG end_char="7635" id="segment-79" start_char="7614">
<ORIGINAL_TEXT>a reply to: slatesteam</ORIGINAL_TEXT>
<TOKEN end_char="7614" id="token-79-0" morph="none" pos="word" start_char="7614">a</TOKEN>
<TOKEN end_char="7620" id="token-79-1" morph="none" pos="word" start_char="7616">reply</TOKEN>
<TOKEN end_char="7623" id="token-79-2" morph="none" pos="word" start_char="7622">to</TOKEN>
<TOKEN end_char="7624" id="token-79-3" morph="none" pos="punct" start_char="7624">:</TOKEN>
<TOKEN end_char="7635" id="token-79-4" morph="none" pos="word" start_char="7626">slatesteam</TOKEN>
</SEG>
<SEG end_char="7670" id="segment-80" start_char="7638">
<ORIGINAL_TEXT>Your point is purely speculative.</ORIGINAL_TEXT>
<TOKEN end_char="7641" id="token-80-0" morph="none" pos="word" start_char="7638">Your</TOKEN>
<TOKEN end_char="7647" id="token-80-1" morph="none" pos="word" start_char="7643">point</TOKEN>
<TOKEN end_char="7650" id="token-80-2" morph="none" pos="word" start_char="7649">is</TOKEN>
<TOKEN end_char="7657" id="token-80-3" morph="none" pos="word" start_char="7652">purely</TOKEN>
<TOKEN end_char="7669" id="token-80-4" morph="none" pos="word" start_char="7659">speculative</TOKEN>
<TOKEN end_char="7670" id="token-80-5" morph="none" pos="punct" start_char="7670">.</TOKEN>
</SEG>
<SEG end_char="7730" id="segment-81" start_char="7673">
<ORIGINAL_TEXT>If that was the case, I'd expect more rhetoric from China.</ORIGINAL_TEXT>
<TOKEN end_char="7674" id="token-81-0" morph="none" pos="word" start_char="7673">If</TOKEN>
<TOKEN end_char="7679" id="token-81-1" morph="none" pos="word" start_char="7676">that</TOKEN>
<TOKEN end_char="7683" id="token-81-2" morph="none" pos="word" start_char="7681">was</TOKEN>
<TOKEN end_char="7687" id="token-81-3" morph="none" pos="word" start_char="7685">the</TOKEN>
<TOKEN end_char="7692" id="token-81-4" morph="none" pos="word" start_char="7689">case</TOKEN>
<TOKEN end_char="7693" id="token-81-5" morph="none" pos="punct" start_char="7693">,</TOKEN>
<TOKEN end_char="7697" id="token-81-6" morph="none" pos="word" start_char="7695">I'd</TOKEN>
<TOKEN end_char="7704" id="token-81-7" morph="none" pos="word" start_char="7699">expect</TOKEN>
<TOKEN end_char="7709" id="token-81-8" morph="none" pos="word" start_char="7706">more</TOKEN>
<TOKEN end_char="7718" id="token-81-9" morph="none" pos="word" start_char="7711">rhetoric</TOKEN>
<TOKEN end_char="7723" id="token-81-10" morph="none" pos="word" start_char="7720">from</TOKEN>
<TOKEN end_char="7729" id="token-81-11" morph="none" pos="word" start_char="7725">China</TOKEN>
<TOKEN end_char="7730" id="token-81-12" morph="none" pos="punct" start_char="7730">.</TOKEN>
</SEG>
<SEG end_char="7854" id="segment-82" start_char="7735">
<ORIGINAL_TEXT>originally posted by: Nothin a reply to: dug88 An AI, that's gonna legitimatize our whack-a-doodle conspiracy-theories ?</ORIGINAL_TEXT>
<TOKEN end_char="7744" id="token-82-0" morph="none" pos="word" start_char="7735">originally</TOKEN>
<TOKEN end_char="7751" id="token-82-1" morph="none" pos="word" start_char="7746">posted</TOKEN>
<TOKEN end_char="7754" id="token-82-2" morph="none" pos="word" start_char="7753">by</TOKEN>
<TOKEN end_char="7755" id="token-82-3" morph="none" pos="punct" start_char="7755">:</TOKEN>
<TOKEN end_char="7762" id="token-82-4" morph="none" pos="word" start_char="7757">Nothin</TOKEN>
<TOKEN end_char="7764" id="token-82-5" morph="none" pos="word" start_char="7764">a</TOKEN>
<TOKEN end_char="7770" id="token-82-6" morph="none" pos="word" start_char="7766">reply</TOKEN>
<TOKEN end_char="7773" id="token-82-7" morph="none" pos="word" start_char="7772">to</TOKEN>
<TOKEN end_char="7774" id="token-82-8" morph="none" pos="punct" start_char="7774">:</TOKEN>
<TOKEN end_char="7780" id="token-82-9" morph="none" pos="word" start_char="7776">dug88</TOKEN>
<TOKEN end_char="7783" id="token-82-10" morph="none" pos="word" start_char="7782">An</TOKEN>
<TOKEN end_char="7786" id="token-82-11" morph="none" pos="word" start_char="7785">AI</TOKEN>
<TOKEN end_char="7787" id="token-82-12" morph="none" pos="punct" start_char="7787">,</TOKEN>
<TOKEN end_char="7794" id="token-82-13" morph="none" pos="word" start_char="7789">that's</TOKEN>
<TOKEN end_char="7800" id="token-82-14" morph="none" pos="word" start_char="7796">gonna</TOKEN>
<TOKEN end_char="7813" id="token-82-15" morph="none" pos="word" start_char="7802">legitimatize</TOKEN>
<TOKEN end_char="7817" id="token-82-16" morph="none" pos="word" start_char="7815">our</TOKEN>
<TOKEN end_char="7832" id="token-82-17" morph="none" pos="unknown" start_char="7819">whack-a-doodle</TOKEN>
<TOKEN end_char="7852" id="token-82-18" morph="none" pos="unknown" start_char="7834">conspiracy-theories</TOKEN>
<TOKEN end_char="7854" id="token-82-19" morph="none" pos="punct" start_char="7854">?</TOKEN>
</SEG>
<SEG end_char="7863" id="segment-83" start_char="7856">
<ORIGINAL_TEXT>Really ?</ORIGINAL_TEXT>
<TOKEN end_char="7861" id="token-83-0" morph="none" pos="word" start_char="7856">Really</TOKEN>
<TOKEN end_char="7863" id="token-83-1" morph="none" pos="punct" start_char="7863">?</TOKEN>
</SEG>
<SEG end_char="7960" id="segment-84" start_char="7866">
<ORIGINAL_TEXT>Well AI are good at giving probabilities but we can't reach conclusions based on probabilities.</ORIGINAL_TEXT>
<TOKEN end_char="7869" id="token-84-0" morph="none" pos="word" start_char="7866">Well</TOKEN>
<TOKEN end_char="7872" id="token-84-1" morph="none" pos="word" start_char="7871">AI</TOKEN>
<TOKEN end_char="7876" id="token-84-2" morph="none" pos="word" start_char="7874">are</TOKEN>
<TOKEN end_char="7881" id="token-84-3" morph="none" pos="word" start_char="7878">good</TOKEN>
<TOKEN end_char="7884" id="token-84-4" morph="none" pos="word" start_char="7883">at</TOKEN>
<TOKEN end_char="7891" id="token-84-5" morph="none" pos="word" start_char="7886">giving</TOKEN>
<TOKEN end_char="7905" id="token-84-6" morph="none" pos="word" start_char="7893">probabilities</TOKEN>
<TOKEN end_char="7909" id="token-84-7" morph="none" pos="word" start_char="7907">but</TOKEN>
<TOKEN end_char="7912" id="token-84-8" morph="none" pos="word" start_char="7911">we</TOKEN>
<TOKEN end_char="7918" id="token-84-9" morph="none" pos="word" start_char="7914">can't</TOKEN>
<TOKEN end_char="7924" id="token-84-10" morph="none" pos="word" start_char="7920">reach</TOKEN>
<TOKEN end_char="7936" id="token-84-11" morph="none" pos="word" start_char="7926">conclusions</TOKEN>
<TOKEN end_char="7942" id="token-84-12" morph="none" pos="word" start_char="7938">based</TOKEN>
<TOKEN end_char="7945" id="token-84-13" morph="none" pos="word" start_char="7944">on</TOKEN>
<TOKEN end_char="7959" id="token-84-14" morph="none" pos="word" start_char="7947">probabilities</TOKEN>
<TOKEN end_char="7960" id="token-84-15" morph="none" pos="punct" start_char="7960">.</TOKEN>
</SEG>
<SEG end_char="8137" id="segment-85" start_char="7962">
<ORIGINAL_TEXT>However I do find it much more probable that Covid-19 was released from a lab considering that the Wuhan lab already created a SARS-Cov hybrid capable of infecting human cells.</ORIGINAL_TEXT>
<TOKEN end_char="7968" id="token-85-0" morph="none" pos="word" start_char="7962">However</TOKEN>
<TOKEN end_char="7970" id="token-85-1" morph="none" pos="word" start_char="7970">I</TOKEN>
<TOKEN end_char="7973" id="token-85-2" morph="none" pos="word" start_char="7972">do</TOKEN>
<TOKEN end_char="7978" id="token-85-3" morph="none" pos="word" start_char="7975">find</TOKEN>
<TOKEN end_char="7981" id="token-85-4" morph="none" pos="word" start_char="7980">it</TOKEN>
<TOKEN end_char="7986" id="token-85-5" morph="none" pos="word" start_char="7983">much</TOKEN>
<TOKEN end_char="7991" id="token-85-6" morph="none" pos="word" start_char="7988">more</TOKEN>
<TOKEN end_char="8000" id="token-85-7" morph="none" pos="word" start_char="7993">probable</TOKEN>
<TOKEN end_char="8005" id="token-85-8" morph="none" pos="word" start_char="8002">that</TOKEN>
<TOKEN end_char="8014" id="token-85-9" morph="none" pos="unknown" start_char="8007">Covid-19</TOKEN>
<TOKEN end_char="8018" id="token-85-10" morph="none" pos="word" start_char="8016">was</TOKEN>
<TOKEN end_char="8027" id="token-85-11" morph="none" pos="word" start_char="8020">released</TOKEN>
<TOKEN end_char="8032" id="token-85-12" morph="none" pos="word" start_char="8029">from</TOKEN>
<TOKEN end_char="8034" id="token-85-13" morph="none" pos="word" start_char="8034">a</TOKEN>
<TOKEN end_char="8038" id="token-85-14" morph="none" pos="word" start_char="8036">lab</TOKEN>
<TOKEN end_char="8050" id="token-85-15" morph="none" pos="word" start_char="8040">considering</TOKEN>
<TOKEN end_char="8055" id="token-85-16" morph="none" pos="word" start_char="8052">that</TOKEN>
<TOKEN end_char="8059" id="token-85-17" morph="none" pos="word" start_char="8057">the</TOKEN>
<TOKEN end_char="8065" id="token-85-18" morph="none" pos="word" start_char="8061">Wuhan</TOKEN>
<TOKEN end_char="8069" id="token-85-19" morph="none" pos="word" start_char="8067">lab</TOKEN>
<TOKEN end_char="8077" id="token-85-20" morph="none" pos="word" start_char="8071">already</TOKEN>
<TOKEN end_char="8085" id="token-85-21" morph="none" pos="word" start_char="8079">created</TOKEN>
<TOKEN end_char="8087" id="token-85-22" morph="none" pos="word" start_char="8087">a</TOKEN>
<TOKEN end_char="8096" id="token-85-23" morph="none" pos="unknown" start_char="8089">SARS-Cov</TOKEN>
<TOKEN end_char="8103" id="token-85-24" morph="none" pos="word" start_char="8098">hybrid</TOKEN>
<TOKEN end_char="8111" id="token-85-25" morph="none" pos="word" start_char="8105">capable</TOKEN>
<TOKEN end_char="8114" id="token-85-26" morph="none" pos="word" start_char="8113">of</TOKEN>
<TOKEN end_char="8124" id="token-85-27" morph="none" pos="word" start_char="8116">infecting</TOKEN>
<TOKEN end_char="8130" id="token-85-28" morph="none" pos="word" start_char="8126">human</TOKEN>
<TOKEN end_char="8136" id="token-85-29" morph="none" pos="word" start_char="8132">cells</TOKEN>
<TOKEN end_char="8137" id="token-85-30" morph="none" pos="punct" start_char="8137">.</TOKEN>
</SEG>
<SEG end_char="8285" id="segment-86" start_char="8139">
<ORIGINAL_TEXT>The mainstream theories about how it jumped to humans make little sense and there is very little evidence to actually show how it jumped to humans.</ORIGINAL_TEXT>
<TOKEN end_char="8141" id="token-86-0" morph="none" pos="word" start_char="8139">The</TOKEN>
<TOKEN end_char="8152" id="token-86-1" morph="none" pos="word" start_char="8143">mainstream</TOKEN>
<TOKEN end_char="8161" id="token-86-2" morph="none" pos="word" start_char="8154">theories</TOKEN>
<TOKEN end_char="8167" id="token-86-3" morph="none" pos="word" start_char="8163">about</TOKEN>
<TOKEN end_char="8171" id="token-86-4" morph="none" pos="word" start_char="8169">how</TOKEN>
<TOKEN end_char="8174" id="token-86-5" morph="none" pos="word" start_char="8173">it</TOKEN>
<TOKEN end_char="8181" id="token-86-6" morph="none" pos="word" start_char="8176">jumped</TOKEN>
<TOKEN end_char="8184" id="token-86-7" morph="none" pos="word" start_char="8183">to</TOKEN>
<TOKEN end_char="8191" id="token-86-8" morph="none" pos="word" start_char="8186">humans</TOKEN>
<TOKEN end_char="8196" id="token-86-9" morph="none" pos="word" start_char="8193">make</TOKEN>
<TOKEN end_char="8203" id="token-86-10" morph="none" pos="word" start_char="8198">little</TOKEN>
<TOKEN end_char="8209" id="token-86-11" morph="none" pos="word" start_char="8205">sense</TOKEN>
<TOKEN end_char="8213" id="token-86-12" morph="none" pos="word" start_char="8211">and</TOKEN>
<TOKEN end_char="8219" id="token-86-13" morph="none" pos="word" start_char="8215">there</TOKEN>
<TOKEN end_char="8222" id="token-86-14" morph="none" pos="word" start_char="8221">is</TOKEN>
<TOKEN end_char="8227" id="token-86-15" morph="none" pos="word" start_char="8224">very</TOKEN>
<TOKEN end_char="8234" id="token-86-16" morph="none" pos="word" start_char="8229">little</TOKEN>
<TOKEN end_char="8243" id="token-86-17" morph="none" pos="word" start_char="8236">evidence</TOKEN>
<TOKEN end_char="8246" id="token-86-18" morph="none" pos="word" start_char="8245">to</TOKEN>
<TOKEN end_char="8255" id="token-86-19" morph="none" pos="word" start_char="8248">actually</TOKEN>
<TOKEN end_char="8260" id="token-86-20" morph="none" pos="word" start_char="8257">show</TOKEN>
<TOKEN end_char="8264" id="token-86-21" morph="none" pos="word" start_char="8262">how</TOKEN>
<TOKEN end_char="8267" id="token-86-22" morph="none" pos="word" start_char="8266">it</TOKEN>
<TOKEN end_char="8274" id="token-86-23" morph="none" pos="word" start_char="8269">jumped</TOKEN>
<TOKEN end_char="8277" id="token-86-24" morph="none" pos="word" start_char="8276">to</TOKEN>
<TOKEN end_char="8284" id="token-86-25" morph="none" pos="word" start_char="8279">humans</TOKEN>
<TOKEN end_char="8285" id="token-86-26" morph="none" pos="punct" start_char="8285">.</TOKEN>
</SEG>
<SEG end_char="8520" id="segment-87" start_char="8287">
<ORIGINAL_TEXT>The origin point of the virus would suggest a very high probability that the virus was accidentally leaked from the Wuhan lab, which isn't hard to imagine considering how effectively Covid-19 spreads regardless of all safety measures.</ORIGINAL_TEXT>
<TOKEN end_char="8289" id="token-87-0" morph="none" pos="word" start_char="8287">The</TOKEN>
<TOKEN end_char="8296" id="token-87-1" morph="none" pos="word" start_char="8291">origin</TOKEN>
<TOKEN end_char="8302" id="token-87-2" morph="none" pos="word" start_char="8298">point</TOKEN>
<TOKEN end_char="8305" id="token-87-3" morph="none" pos="word" start_char="8304">of</TOKEN>
<TOKEN end_char="8309" id="token-87-4" morph="none" pos="word" start_char="8307">the</TOKEN>
<TOKEN end_char="8315" id="token-87-5" morph="none" pos="word" start_char="8311">virus</TOKEN>
<TOKEN end_char="8321" id="token-87-6" morph="none" pos="word" start_char="8317">would</TOKEN>
<TOKEN end_char="8329" id="token-87-7" morph="none" pos="word" start_char="8323">suggest</TOKEN>
<TOKEN end_char="8331" id="token-87-8" morph="none" pos="word" start_char="8331">a</TOKEN>
<TOKEN end_char="8336" id="token-87-9" morph="none" pos="word" start_char="8333">very</TOKEN>
<TOKEN end_char="8341" id="token-87-10" morph="none" pos="word" start_char="8338">high</TOKEN>
<TOKEN end_char="8353" id="token-87-11" morph="none" pos="word" start_char="8343">probability</TOKEN>
<TOKEN end_char="8358" id="token-87-12" morph="none" pos="word" start_char="8355">that</TOKEN>
<TOKEN end_char="8362" id="token-87-13" morph="none" pos="word" start_char="8360">the</TOKEN>
<TOKEN end_char="8368" id="token-87-14" morph="none" pos="word" start_char="8364">virus</TOKEN>
<TOKEN end_char="8372" id="token-87-15" morph="none" pos="word" start_char="8370">was</TOKEN>
<TOKEN end_char="8385" id="token-87-16" morph="none" pos="word" start_char="8374">accidentally</TOKEN>
<TOKEN end_char="8392" id="token-87-17" morph="none" pos="word" start_char="8387">leaked</TOKEN>
<TOKEN end_char="8397" id="token-87-18" morph="none" pos="word" start_char="8394">from</TOKEN>
<TOKEN end_char="8401" id="token-87-19" morph="none" pos="word" start_char="8399">the</TOKEN>
<TOKEN end_char="8407" id="token-87-20" morph="none" pos="word" start_char="8403">Wuhan</TOKEN>
<TOKEN end_char="8411" id="token-87-21" morph="none" pos="word" start_char="8409">lab</TOKEN>
<TOKEN end_char="8412" id="token-87-22" morph="none" pos="punct" start_char="8412">,</TOKEN>
<TOKEN end_char="8418" id="token-87-23" morph="none" pos="word" start_char="8414">which</TOKEN>
<TOKEN end_char="8424" id="token-87-24" morph="none" pos="word" start_char="8420">isn't</TOKEN>
<TOKEN end_char="8429" id="token-87-25" morph="none" pos="word" start_char="8426">hard</TOKEN>
<TOKEN end_char="8432" id="token-87-26" morph="none" pos="word" start_char="8431">to</TOKEN>
<TOKEN end_char="8440" id="token-87-27" morph="none" pos="word" start_char="8434">imagine</TOKEN>
<TOKEN end_char="8452" id="token-87-28" morph="none" pos="word" start_char="8442">considering</TOKEN>
<TOKEN end_char="8456" id="token-87-29" morph="none" pos="word" start_char="8454">how</TOKEN>
<TOKEN end_char="8468" id="token-87-30" morph="none" pos="word" start_char="8458">effectively</TOKEN>
<TOKEN end_char="8477" id="token-87-31" morph="none" pos="unknown" start_char="8470">Covid-19</TOKEN>
<TOKEN end_char="8485" id="token-87-32" morph="none" pos="word" start_char="8479">spreads</TOKEN>
<TOKEN end_char="8496" id="token-87-33" morph="none" pos="word" start_char="8487">regardless</TOKEN>
<TOKEN end_char="8499" id="token-87-34" morph="none" pos="word" start_char="8498">of</TOKEN>
<TOKEN end_char="8503" id="token-87-35" morph="none" pos="word" start_char="8501">all</TOKEN>
<TOKEN end_char="8510" id="token-87-36" morph="none" pos="word" start_char="8505">safety</TOKEN>
<TOKEN end_char="8519" id="token-87-37" morph="none" pos="word" start_char="8512">measures</TOKEN>
<TOKEN end_char="8520" id="token-87-38" morph="none" pos="punct" start_char="8520">.</TOKEN>
</SEG>
<SEG end_char="8744" id="segment-88" start_char="8523">
<ORIGINAL_TEXT>In 2005, a group including researchers from the Wuhan Institute of Virology published research into the origin of the SARS coronavirus, finding that China's horseshoe bats are natural reservoirs of SARS-like coronaviruses.</ORIGINAL_TEXT>
<TOKEN end_char="8524" id="token-88-0" morph="none" pos="word" start_char="8523">In</TOKEN>
<TOKEN end_char="8529" id="token-88-1" morph="none" pos="word" start_char="8526">2005</TOKEN>
<TOKEN end_char="8530" id="token-88-2" morph="none" pos="punct" start_char="8530">,</TOKEN>
<TOKEN end_char="8532" id="token-88-3" morph="none" pos="word" start_char="8532">a</TOKEN>
<TOKEN end_char="8538" id="token-88-4" morph="none" pos="word" start_char="8534">group</TOKEN>
<TOKEN end_char="8548" id="token-88-5" morph="none" pos="word" start_char="8540">including</TOKEN>
<TOKEN end_char="8560" id="token-88-6" morph="none" pos="word" start_char="8550">researchers</TOKEN>
<TOKEN end_char="8565" id="token-88-7" morph="none" pos="word" start_char="8562">from</TOKEN>
<TOKEN end_char="8569" id="token-88-8" morph="none" pos="word" start_char="8567">the</TOKEN>
<TOKEN end_char="8575" id="token-88-9" morph="none" pos="word" start_char="8571">Wuhan</TOKEN>
<TOKEN end_char="8585" id="token-88-10" morph="none" pos="word" start_char="8577">Institute</TOKEN>
<TOKEN end_char="8588" id="token-88-11" morph="none" pos="word" start_char="8587">of</TOKEN>
<TOKEN end_char="8597" id="token-88-12" morph="none" pos="word" start_char="8590">Virology</TOKEN>
<TOKEN end_char="8607" id="token-88-13" morph="none" pos="word" start_char="8599">published</TOKEN>
<TOKEN end_char="8616" id="token-88-14" morph="none" pos="word" start_char="8609">research</TOKEN>
<TOKEN end_char="8621" id="token-88-15" morph="none" pos="word" start_char="8618">into</TOKEN>
<TOKEN end_char="8625" id="token-88-16" morph="none" pos="word" start_char="8623">the</TOKEN>
<TOKEN end_char="8632" id="token-88-17" morph="none" pos="word" start_char="8627">origin</TOKEN>
<TOKEN end_char="8635" id="token-88-18" morph="none" pos="word" start_char="8634">of</TOKEN>
<TOKEN end_char="8639" id="token-88-19" morph="none" pos="word" start_char="8637">the</TOKEN>
<TOKEN end_char="8644" id="token-88-20" morph="none" pos="word" start_char="8641">SARS</TOKEN>
<TOKEN end_char="8656" id="token-88-21" morph="none" pos="word" start_char="8646">coronavirus</TOKEN>
<TOKEN end_char="8657" id="token-88-22" morph="none" pos="punct" start_char="8657">,</TOKEN>
<TOKEN end_char="8665" id="token-88-23" morph="none" pos="word" start_char="8659">finding</TOKEN>
<TOKEN end_char="8670" id="token-88-24" morph="none" pos="word" start_char="8667">that</TOKEN>
<TOKEN end_char="8678" id="token-88-25" morph="none" pos="word" start_char="8672">China's</TOKEN>
<TOKEN end_char="8688" id="token-88-26" morph="none" pos="word" start_char="8680">horseshoe</TOKEN>
<TOKEN end_char="8693" id="token-88-27" morph="none" pos="word" start_char="8690">bats</TOKEN>
<TOKEN end_char="8697" id="token-88-28" morph="none" pos="word" start_char="8695">are</TOKEN>
<TOKEN end_char="8705" id="token-88-29" morph="none" pos="word" start_char="8699">natural</TOKEN>
<TOKEN end_char="8716" id="token-88-30" morph="none" pos="word" start_char="8707">reservoirs</TOKEN>
<TOKEN end_char="8719" id="token-88-31" morph="none" pos="word" start_char="8718">of</TOKEN>
<TOKEN end_char="8729" id="token-88-32" morph="none" pos="unknown" start_char="8721">SARS-like</TOKEN>
<TOKEN end_char="8743" id="token-88-33" morph="none" pos="word" start_char="8731">coronaviruses</TOKEN>
<TOKEN end_char="8744" id="token-88-34" morph="none" pos="punct" start_char="8744">.</TOKEN>
</SEG>
<SEG end_char="8933" id="segment-89" start_char="8746">
<ORIGINAL_TEXT>[6] Continuing this work over a period of years, researchers from the Institute sampled thousands of horseshoe bats in locations across China, isolating over 300 bat coronavirus sequences.</ORIGINAL_TEXT>
<TOKEN end_char="8746" id="token-89-0" morph="none" pos="punct" start_char="8746">[</TOKEN>
<TOKEN end_char="8747" id="token-89-1" morph="none" pos="word" start_char="8747">6</TOKEN>
<TOKEN end_char="8748" id="token-89-2" morph="none" pos="punct" start_char="8748">]</TOKEN>
<TOKEN end_char="8759" id="token-89-3" morph="none" pos="word" start_char="8750">Continuing</TOKEN>
<TOKEN end_char="8764" id="token-89-4" morph="none" pos="word" start_char="8761">this</TOKEN>
<TOKEN end_char="8769" id="token-89-5" morph="none" pos="word" start_char="8766">work</TOKEN>
<TOKEN end_char="8774" id="token-89-6" morph="none" pos="word" start_char="8771">over</TOKEN>
<TOKEN end_char="8776" id="token-89-7" morph="none" pos="word" start_char="8776">a</TOKEN>
<TOKEN end_char="8783" id="token-89-8" morph="none" pos="word" start_char="8778">period</TOKEN>
<TOKEN end_char="8786" id="token-89-9" morph="none" pos="word" start_char="8785">of</TOKEN>
<TOKEN end_char="8792" id="token-89-10" morph="none" pos="word" start_char="8788">years</TOKEN>
<TOKEN end_char="8793" id="token-89-11" morph="none" pos="punct" start_char="8793">,</TOKEN>
<TOKEN end_char="8805" id="token-89-12" morph="none" pos="word" start_char="8795">researchers</TOKEN>
<TOKEN end_char="8810" id="token-89-13" morph="none" pos="word" start_char="8807">from</TOKEN>
<TOKEN end_char="8814" id="token-89-14" morph="none" pos="word" start_char="8812">the</TOKEN>
<TOKEN end_char="8824" id="token-89-15" morph="none" pos="word" start_char="8816">Institute</TOKEN>
<TOKEN end_char="8832" id="token-89-16" morph="none" pos="word" start_char="8826">sampled</TOKEN>
<TOKEN end_char="8842" id="token-89-17" morph="none" pos="word" start_char="8834">thousands</TOKEN>
<TOKEN end_char="8845" id="token-89-18" morph="none" pos="word" start_char="8844">of</TOKEN>
<TOKEN end_char="8855" id="token-89-19" morph="none" pos="word" start_char="8847">horseshoe</TOKEN>
<TOKEN end_char="8860" id="token-89-20" morph="none" pos="word" start_char="8857">bats</TOKEN>
<TOKEN end_char="8863" id="token-89-21" morph="none" pos="word" start_char="8862">in</TOKEN>
<TOKEN end_char="8873" id="token-89-22" morph="none" pos="word" start_char="8865">locations</TOKEN>
<TOKEN end_char="8880" id="token-89-23" morph="none" pos="word" start_char="8875">across</TOKEN>
<TOKEN end_char="8886" id="token-89-24" morph="none" pos="word" start_char="8882">China</TOKEN>
<TOKEN end_char="8887" id="token-89-25" morph="none" pos="punct" start_char="8887">,</TOKEN>
<TOKEN end_char="8897" id="token-89-26" morph="none" pos="word" start_char="8889">isolating</TOKEN>
<TOKEN end_char="8902" id="token-89-27" morph="none" pos="word" start_char="8899">over</TOKEN>
<TOKEN end_char="8906" id="token-89-28" morph="none" pos="word" start_char="8904">300</TOKEN>
<TOKEN end_char="8910" id="token-89-29" morph="none" pos="word" start_char="8908">bat</TOKEN>
<TOKEN end_char="8922" id="token-89-30" morph="none" pos="word" start_char="8912">coronavirus</TOKEN>
<TOKEN end_char="8932" id="token-89-31" morph="none" pos="word" start_char="8924">sequences</TOKEN>
<TOKEN end_char="8933" id="token-89-32" morph="none" pos="punct" start_char="8933">.</TOKEN>
</SEG>
<SEG end_char="9049" id="segment-90" start_char="8935">
<ORIGINAL_TEXT>[7] In 2015, the Institute published successful research on whether a bat coronavirus could be made to infect HeLa.</ORIGINAL_TEXT>
<TOKEN end_char="8935" id="token-90-0" morph="none" pos="punct" start_char="8935">[</TOKEN>
<TOKEN end_char="8936" id="token-90-1" morph="none" pos="word" start_char="8936">7</TOKEN>
<TOKEN end_char="8937" id="token-90-2" morph="none" pos="punct" start_char="8937">]</TOKEN>
<TOKEN end_char="8940" id="token-90-3" morph="none" pos="word" start_char="8939">In</TOKEN>
<TOKEN end_char="8945" id="token-90-4" morph="none" pos="word" start_char="8942">2015</TOKEN>
<TOKEN end_char="8946" id="token-90-5" morph="none" pos="punct" start_char="8946">,</TOKEN>
<TOKEN end_char="8950" id="token-90-6" morph="none" pos="word" start_char="8948">the</TOKEN>
<TOKEN end_char="8960" id="token-90-7" morph="none" pos="word" start_char="8952">Institute</TOKEN>
<TOKEN end_char="8970" id="token-90-8" morph="none" pos="word" start_char="8962">published</TOKEN>
<TOKEN end_char="8981" id="token-90-9" morph="none" pos="word" start_char="8972">successful</TOKEN>
<TOKEN end_char="8990" id="token-90-10" morph="none" pos="word" start_char="8983">research</TOKEN>
<TOKEN end_char="8993" id="token-90-11" morph="none" pos="word" start_char="8992">on</TOKEN>
<TOKEN end_char="9001" id="token-90-12" morph="none" pos="word" start_char="8995">whether</TOKEN>
<TOKEN end_char="9003" id="token-90-13" morph="none" pos="word" start_char="9003">a</TOKEN>
<TOKEN end_char="9007" id="token-90-14" morph="none" pos="word" start_char="9005">bat</TOKEN>
<TOKEN end_char="9019" id="token-90-15" morph="none" pos="word" start_char="9009">coronavirus</TOKEN>
<TOKEN end_char="9025" id="token-90-16" morph="none" pos="word" start_char="9021">could</TOKEN>
<TOKEN end_char="9028" id="token-90-17" morph="none" pos="word" start_char="9027">be</TOKEN>
<TOKEN end_char="9033" id="token-90-18" morph="none" pos="word" start_char="9030">made</TOKEN>
<TOKEN end_char="9036" id="token-90-19" morph="none" pos="word" start_char="9035">to</TOKEN>
<TOKEN end_char="9043" id="token-90-20" morph="none" pos="word" start_char="9038">infect</TOKEN>
<TOKEN end_char="9048" id="token-90-21" morph="none" pos="word" start_char="9045">HeLa</TOKEN>
<TOKEN end_char="9049" id="token-90-22" morph="none" pos="punct" start_char="9049">.</TOKEN>
</SEG>
<SEG end_char="9211" id="segment-91" start_char="9051">
<ORIGINAL_TEXT>A team from the Institute engineered a hybrid virus, combining a bat coronavirus with a SARS virus that had been adapted to grow in mice and mimic human disease.</ORIGINAL_TEXT>
<TOKEN end_char="9051" id="token-91-0" morph="none" pos="word" start_char="9051">A</TOKEN>
<TOKEN end_char="9056" id="token-91-1" morph="none" pos="word" start_char="9053">team</TOKEN>
<TOKEN end_char="9061" id="token-91-2" morph="none" pos="word" start_char="9058">from</TOKEN>
<TOKEN end_char="9065" id="token-91-3" morph="none" pos="word" start_char="9063">the</TOKEN>
<TOKEN end_char="9075" id="token-91-4" morph="none" pos="word" start_char="9067">Institute</TOKEN>
<TOKEN end_char="9086" id="token-91-5" morph="none" pos="word" start_char="9077">engineered</TOKEN>
<TOKEN end_char="9088" id="token-91-6" morph="none" pos="word" start_char="9088">a</TOKEN>
<TOKEN end_char="9095" id="token-91-7" morph="none" pos="word" start_char="9090">hybrid</TOKEN>
<TOKEN end_char="9101" id="token-91-8" morph="none" pos="word" start_char="9097">virus</TOKEN>
<TOKEN end_char="9102" id="token-91-9" morph="none" pos="punct" start_char="9102">,</TOKEN>
<TOKEN end_char="9112" id="token-91-10" morph="none" pos="word" start_char="9104">combining</TOKEN>
<TOKEN end_char="9114" id="token-91-11" morph="none" pos="word" start_char="9114">a</TOKEN>
<TOKEN end_char="9118" id="token-91-12" morph="none" pos="word" start_char="9116">bat</TOKEN>
<TOKEN end_char="9130" id="token-91-13" morph="none" pos="word" start_char="9120">coronavirus</TOKEN>
<TOKEN end_char="9135" id="token-91-14" morph="none" pos="word" start_char="9132">with</TOKEN>
<TOKEN end_char="9137" id="token-91-15" morph="none" pos="word" start_char="9137">a</TOKEN>
<TOKEN end_char="9142" id="token-91-16" morph="none" pos="word" start_char="9139">SARS</TOKEN>
<TOKEN end_char="9148" id="token-91-17" morph="none" pos="word" start_char="9144">virus</TOKEN>
<TOKEN end_char="9153" id="token-91-18" morph="none" pos="word" start_char="9150">that</TOKEN>
<TOKEN end_char="9157" id="token-91-19" morph="none" pos="word" start_char="9155">had</TOKEN>
<TOKEN end_char="9162" id="token-91-20" morph="none" pos="word" start_char="9159">been</TOKEN>
<TOKEN end_char="9170" id="token-91-21" morph="none" pos="word" start_char="9164">adapted</TOKEN>
<TOKEN end_char="9173" id="token-91-22" morph="none" pos="word" start_char="9172">to</TOKEN>
<TOKEN end_char="9178" id="token-91-23" morph="none" pos="word" start_char="9175">grow</TOKEN>
<TOKEN end_char="9181" id="token-91-24" morph="none" pos="word" start_char="9180">in</TOKEN>
<TOKEN end_char="9186" id="token-91-25" morph="none" pos="word" start_char="9183">mice</TOKEN>
<TOKEN end_char="9190" id="token-91-26" morph="none" pos="word" start_char="9188">and</TOKEN>
<TOKEN end_char="9196" id="token-91-27" morph="none" pos="word" start_char="9192">mimic</TOKEN>
<TOKEN end_char="9202" id="token-91-28" morph="none" pos="word" start_char="9198">human</TOKEN>
<TOKEN end_char="9210" id="token-91-29" morph="none" pos="word" start_char="9204">disease</TOKEN>
<TOKEN end_char="9211" id="token-91-30" morph="none" pos="punct" start_char="9211">.</TOKEN>
</SEG>
<SEG end_char="9260" id="segment-92" start_char="9213">
<ORIGINAL_TEXT>The hybrid virus was able to infect human cells.</ORIGINAL_TEXT>
<TOKEN end_char="9215" id="token-92-0" morph="none" pos="word" start_char="9213">The</TOKEN>
<TOKEN end_char="9222" id="token-92-1" morph="none" pos="word" start_char="9217">hybrid</TOKEN>
<TOKEN end_char="9228" id="token-92-2" morph="none" pos="word" start_char="9224">virus</TOKEN>
<TOKEN end_char="9232" id="token-92-3" morph="none" pos="word" start_char="9230">was</TOKEN>
<TOKEN end_char="9237" id="token-92-4" morph="none" pos="word" start_char="9234">able</TOKEN>
<TOKEN end_char="9240" id="token-92-5" morph="none" pos="word" start_char="9239">to</TOKEN>
<TOKEN end_char="9247" id="token-92-6" morph="none" pos="word" start_char="9242">infect</TOKEN>
<TOKEN end_char="9253" id="token-92-7" morph="none" pos="word" start_char="9249">human</TOKEN>
<TOKEN end_char="9259" id="token-92-8" morph="none" pos="word" start_char="9255">cells</TOKEN>
<TOKEN end_char="9260" id="token-92-9" morph="none" pos="punct" start_char="9260">.</TOKEN>
</SEG>
<SEG end_char="9318" id="segment-93" start_char="9262">
<ORIGINAL_TEXT>[8][9] Wuhan Institute of Virology - Coronavirus research</ORIGINAL_TEXT>
<TOKEN end_char="9262" id="token-93-0" morph="none" pos="punct" start_char="9262">[</TOKEN>
<TOKEN end_char="9266" id="token-93-1" morph="none" pos="unknown" start_char="9263">8][9</TOKEN>
<TOKEN end_char="9267" id="token-93-2" morph="none" pos="punct" start_char="9267">]</TOKEN>
<TOKEN end_char="9273" id="token-93-3" morph="none" pos="word" start_char="9269">Wuhan</TOKEN>
<TOKEN end_char="9283" id="token-93-4" morph="none" pos="word" start_char="9275">Institute</TOKEN>
<TOKEN end_char="9286" id="token-93-5" morph="none" pos="word" start_char="9285">of</TOKEN>
<TOKEN end_char="9295" id="token-93-6" morph="none" pos="word" start_char="9288">Virology</TOKEN>
<TOKEN end_char="9297" id="token-93-7" morph="none" pos="punct" start_char="9297">-</TOKEN>
<TOKEN end_char="9309" id="token-93-8" morph="none" pos="word" start_char="9299">Coronavirus</TOKEN>
<TOKEN end_char="9318" id="token-93-9" morph="none" pos="word" start_char="9311">research</TOKEN>
</SEG>
<SEG end_char="9381" id="segment-94" start_char="9321">
<ORIGINAL_TEXT>edit on 30/12/2020 by ChaoticOrder because: (no reason given)</ORIGINAL_TEXT>
<TOKEN end_char="9324" id="token-94-0" morph="none" pos="word" start_char="9321">edit</TOKEN>
<TOKEN end_char="9327" id="token-94-1" morph="none" pos="word" start_char="9326">on</TOKEN>
<TOKEN end_char="9338" id="token-94-2" morph="none" pos="unknown" start_char="9329">30/12/2020</TOKEN>
<TOKEN end_char="9341" id="token-94-3" morph="none" pos="word" start_char="9340">by</TOKEN>
<TOKEN end_char="9354" id="token-94-4" morph="none" pos="word" start_char="9343">ChaoticOrder</TOKEN>
<TOKEN end_char="9362" id="token-94-5" morph="none" pos="word" start_char="9356">because</TOKEN>
<TOKEN end_char="9363" id="token-94-6" morph="none" pos="punct" start_char="9363">:</TOKEN>
<TOKEN end_char="9365" id="token-94-7" morph="none" pos="punct" start_char="9365">(</TOKEN>
<TOKEN end_char="9367" id="token-94-8" morph="none" pos="word" start_char="9366">no</TOKEN>
<TOKEN end_char="9374" id="token-94-9" morph="none" pos="word" start_char="9369">reason</TOKEN>
<TOKEN end_char="9380" id="token-94-10" morph="none" pos="word" start_char="9376">given</TOKEN>
<TOKEN end_char="9381" id="token-94-11" morph="none" pos="punct" start_char="9381">)</TOKEN>
</SEG>
<SEG end_char="9593" id="segment-95" start_char="9386">
<ORIGINAL_TEXT>pretty sure i remember reading about this before the virus spread outside of china after watching a video about it from a youtuber that used to live in china and still had contacts he talked to about it with.</ORIGINAL_TEXT>
<TOKEN end_char="9391" id="token-95-0" morph="none" pos="word" start_char="9386">pretty</TOKEN>
<TOKEN end_char="9396" id="token-95-1" morph="none" pos="word" start_char="9393">sure</TOKEN>
<TOKEN end_char="9398" id="token-95-2" morph="none" pos="word" start_char="9398">i</TOKEN>
<TOKEN end_char="9407" id="token-95-3" morph="none" pos="word" start_char="9400">remember</TOKEN>
<TOKEN end_char="9415" id="token-95-4" morph="none" pos="word" start_char="9409">reading</TOKEN>
<TOKEN end_char="9421" id="token-95-5" morph="none" pos="word" start_char="9417">about</TOKEN>
<TOKEN end_char="9426" id="token-95-6" morph="none" pos="word" start_char="9423">this</TOKEN>
<TOKEN end_char="9433" id="token-95-7" morph="none" pos="word" start_char="9428">before</TOKEN>
<TOKEN end_char="9437" id="token-95-8" morph="none" pos="word" start_char="9435">the</TOKEN>
<TOKEN end_char="9443" id="token-95-9" morph="none" pos="word" start_char="9439">virus</TOKEN>
<TOKEN end_char="9450" id="token-95-10" morph="none" pos="word" start_char="9445">spread</TOKEN>
<TOKEN end_char="9458" id="token-95-11" morph="none" pos="word" start_char="9452">outside</TOKEN>
<TOKEN end_char="9461" id="token-95-12" morph="none" pos="word" start_char="9460">of</TOKEN>
<TOKEN end_char="9467" id="token-95-13" morph="none" pos="word" start_char="9463">china</TOKEN>
<TOKEN end_char="9473" id="token-95-14" morph="none" pos="word" start_char="9469">after</TOKEN>
<TOKEN end_char="9482" id="token-95-15" morph="none" pos="word" start_char="9475">watching</TOKEN>
<TOKEN end_char="9484" id="token-95-16" morph="none" pos="word" start_char="9484">a</TOKEN>
<TOKEN end_char="9490" id="token-95-17" morph="none" pos="word" start_char="9486">video</TOKEN>
<TOKEN end_char="9496" id="token-95-18" morph="none" pos="word" start_char="9492">about</TOKEN>
<TOKEN end_char="9499" id="token-95-19" morph="none" pos="word" start_char="9498">it</TOKEN>
<TOKEN end_char="9504" id="token-95-20" morph="none" pos="word" start_char="9501">from</TOKEN>
<TOKEN end_char="9506" id="token-95-21" morph="none" pos="word" start_char="9506">a</TOKEN>
<TOKEN end_char="9515" id="token-95-22" morph="none" pos="word" start_char="9508">youtuber</TOKEN>
<TOKEN end_char="9520" id="token-95-23" morph="none" pos="word" start_char="9517">that</TOKEN>
<TOKEN end_char="9525" id="token-95-24" morph="none" pos="word" start_char="9522">used</TOKEN>
<TOKEN end_char="9528" id="token-95-25" morph="none" pos="word" start_char="9527">to</TOKEN>
<TOKEN end_char="9533" id="token-95-26" morph="none" pos="word" start_char="9530">live</TOKEN>
<TOKEN end_char="9536" id="token-95-27" morph="none" pos="word" start_char="9535">in</TOKEN>
<TOKEN end_char="9542" id="token-95-28" morph="none" pos="word" start_char="9538">china</TOKEN>
<TOKEN end_char="9546" id="token-95-29" morph="none" pos="word" start_char="9544">and</TOKEN>
<TOKEN end_char="9552" id="token-95-30" morph="none" pos="word" start_char="9548">still</TOKEN>
<TOKEN end_char="9556" id="token-95-31" morph="none" pos="word" start_char="9554">had</TOKEN>
<TOKEN end_char="9565" id="token-95-32" morph="none" pos="word" start_char="9558">contacts</TOKEN>
<TOKEN end_char="9568" id="token-95-33" morph="none" pos="word" start_char="9567">he</TOKEN>
<TOKEN end_char="9575" id="token-95-34" morph="none" pos="word" start_char="9570">talked</TOKEN>
<TOKEN end_char="9578" id="token-95-35" morph="none" pos="word" start_char="9577">to</TOKEN>
<TOKEN end_char="9584" id="token-95-36" morph="none" pos="word" start_char="9580">about</TOKEN>
<TOKEN end_char="9587" id="token-95-37" morph="none" pos="word" start_char="9586">it</TOKEN>
<TOKEN end_char="9592" id="token-95-38" morph="none" pos="word" start_char="9589">with</TOKEN>
<TOKEN end_char="9593" id="token-95-39" morph="none" pos="punct" start_char="9593">.</TOKEN>
</SEG>
<SEG end_char="9760" id="segment-96" start_char="9595">
<ORIGINAL_TEXT>dont remember the details but it was pretty compelling information indicating a coverup and researchers disappearing or being reassigned before the virus became news.</ORIGINAL_TEXT>
<TOKEN end_char="9598" id="token-96-0" morph="none" pos="word" start_char="9595">dont</TOKEN>
<TOKEN end_char="9607" id="token-96-1" morph="none" pos="word" start_char="9600">remember</TOKEN>
<TOKEN end_char="9611" id="token-96-2" morph="none" pos="word" start_char="9609">the</TOKEN>
<TOKEN end_char="9619" id="token-96-3" morph="none" pos="word" start_char="9613">details</TOKEN>
<TOKEN end_char="9623" id="token-96-4" morph="none" pos="word" start_char="9621">but</TOKEN>
<TOKEN end_char="9626" id="token-96-5" morph="none" pos="word" start_char="9625">it</TOKEN>
<TOKEN end_char="9630" id="token-96-6" morph="none" pos="word" start_char="9628">was</TOKEN>
<TOKEN end_char="9637" id="token-96-7" morph="none" pos="word" start_char="9632">pretty</TOKEN>
<TOKEN end_char="9648" id="token-96-8" morph="none" pos="word" start_char="9639">compelling</TOKEN>
<TOKEN end_char="9660" id="token-96-9" morph="none" pos="word" start_char="9650">information</TOKEN>
<TOKEN end_char="9671" id="token-96-10" morph="none" pos="word" start_char="9662">indicating</TOKEN>
<TOKEN end_char="9673" id="token-96-11" morph="none" pos="word" start_char="9673">a</TOKEN>
<TOKEN end_char="9681" id="token-96-12" morph="none" pos="word" start_char="9675">coverup</TOKEN>
<TOKEN end_char="9685" id="token-96-13" morph="none" pos="word" start_char="9683">and</TOKEN>
<TOKEN end_char="9697" id="token-96-14" morph="none" pos="word" start_char="9687">researchers</TOKEN>
<TOKEN end_char="9710" id="token-96-15" morph="none" pos="word" start_char="9699">disappearing</TOKEN>
<TOKEN end_char="9713" id="token-96-16" morph="none" pos="word" start_char="9712">or</TOKEN>
<TOKEN end_char="9719" id="token-96-17" morph="none" pos="word" start_char="9715">being</TOKEN>
<TOKEN end_char="9730" id="token-96-18" morph="none" pos="word" start_char="9721">reassigned</TOKEN>
<TOKEN end_char="9737" id="token-96-19" morph="none" pos="word" start_char="9732">before</TOKEN>
<TOKEN end_char="9741" id="token-96-20" morph="none" pos="word" start_char="9739">the</TOKEN>
<TOKEN end_char="9747" id="token-96-21" morph="none" pos="word" start_char="9743">virus</TOKEN>
<TOKEN end_char="9754" id="token-96-22" morph="none" pos="word" start_char="9749">became</TOKEN>
<TOKEN end_char="9759" id="token-96-23" morph="none" pos="word" start_char="9756">news</TOKEN>
<TOKEN end_char="9760" id="token-96-24" morph="none" pos="punct" start_char="9760">.</TOKEN>
</SEG>
<SEG end_char="9787" id="segment-97" start_char="9764">
<ORIGINAL_TEXT>a reply to: ChaoticOrder</ORIGINAL_TEXT>
<TOKEN end_char="9764" id="token-97-0" morph="none" pos="word" start_char="9764">a</TOKEN>
<TOKEN end_char="9770" id="token-97-1" morph="none" pos="word" start_char="9766">reply</TOKEN>
<TOKEN end_char="9773" id="token-97-2" morph="none" pos="word" start_char="9772">to</TOKEN>
<TOKEN end_char="9774" id="token-97-3" morph="none" pos="punct" start_char="9774">:</TOKEN>
<TOKEN end_char="9787" id="token-97-4" morph="none" pos="word" start_char="9776">ChaoticOrder</TOKEN>
</SEG>
<SEG end_char="9863" id="segment-98" start_char="9790">
<ORIGINAL_TEXT>Yes : can see those possibilities, with significant levels of probability.</ORIGINAL_TEXT>
<TOKEN end_char="9792" id="token-98-0" morph="none" pos="word" start_char="9790">Yes</TOKEN>
<TOKEN end_char="9794" id="token-98-1" morph="none" pos="punct" start_char="9794">:</TOKEN>
<TOKEN end_char="9798" id="token-98-2" morph="none" pos="word" start_char="9796">can</TOKEN>
<TOKEN end_char="9802" id="token-98-3" morph="none" pos="word" start_char="9800">see</TOKEN>
<TOKEN end_char="9808" id="token-98-4" morph="none" pos="word" start_char="9804">those</TOKEN>
<TOKEN end_char="9822" id="token-98-5" morph="none" pos="word" start_char="9810">possibilities</TOKEN>
<TOKEN end_char="9823" id="token-98-6" morph="none" pos="punct" start_char="9823">,</TOKEN>
<TOKEN end_char="9828" id="token-98-7" morph="none" pos="word" start_char="9825">with</TOKEN>
<TOKEN end_char="9840" id="token-98-8" morph="none" pos="word" start_char="9830">significant</TOKEN>
<TOKEN end_char="9847" id="token-98-9" morph="none" pos="word" start_char="9842">levels</TOKEN>
<TOKEN end_char="9850" id="token-98-10" morph="none" pos="word" start_char="9849">of</TOKEN>
<TOKEN end_char="9862" id="token-98-11" morph="none" pos="word" start_char="9852">probability</TOKEN>
<TOKEN end_char="9863" id="token-98-12" morph="none" pos="punct" start_char="9863">.</TOKEN>
</SEG>
<SEG end_char="9898" id="segment-99" start_char="9866">
<ORIGINAL_TEXT>My problem is in trusting the AI.</ORIGINAL_TEXT>
<TOKEN end_char="9867" id="token-99-0" morph="none" pos="word" start_char="9866">My</TOKEN>
<TOKEN end_char="9875" id="token-99-1" morph="none" pos="word" start_char="9869">problem</TOKEN>
<TOKEN end_char="9878" id="token-99-2" morph="none" pos="word" start_char="9877">is</TOKEN>
<TOKEN end_char="9881" id="token-99-3" morph="none" pos="word" start_char="9880">in</TOKEN>
<TOKEN end_char="9890" id="token-99-4" morph="none" pos="word" start_char="9883">trusting</TOKEN>
<TOKEN end_char="9894" id="token-99-5" morph="none" pos="word" start_char="9892">the</TOKEN>
<TOKEN end_char="9897" id="token-99-6" morph="none" pos="word" start_char="9896">AI</TOKEN>
<TOKEN end_char="9898" id="token-99-7" morph="none" pos="punct" start_char="9898">.</TOKEN>
</SEG>
<SEG end_char="9938" id="segment-100" start_char="9900">
<ORIGINAL_TEXT>Or rather : the AI, that isn't neutral.</ORIGINAL_TEXT>
<TOKEN end_char="9901" id="token-100-0" morph="none" pos="word" start_char="9900">Or</TOKEN>
<TOKEN end_char="9908" id="token-100-1" morph="none" pos="word" start_char="9903">rather</TOKEN>
<TOKEN end_char="9910" id="token-100-2" morph="none" pos="punct" start_char="9910">:</TOKEN>
<TOKEN end_char="9914" id="token-100-3" morph="none" pos="word" start_char="9912">the</TOKEN>
<TOKEN end_char="9917" id="token-100-4" morph="none" pos="word" start_char="9916">AI</TOKEN>
<TOKEN end_char="9918" id="token-100-5" morph="none" pos="punct" start_char="9918">,</TOKEN>
<TOKEN end_char="9923" id="token-100-6" morph="none" pos="word" start_char="9920">that</TOKEN>
<TOKEN end_char="9929" id="token-100-7" morph="none" pos="word" start_char="9925">isn't</TOKEN>
<TOKEN end_char="9937" id="token-100-8" morph="none" pos="word" start_char="9931">neutral</TOKEN>
<TOKEN end_char="9938" id="token-100-9" morph="none" pos="punct" start_char="9938">.</TOKEN>
</SEG>
<SEG end_char="10021" id="segment-101" start_char="9940">
<ORIGINAL_TEXT>The AI : that is owned, financed, and programmed by someone, or some organization.</ORIGINAL_TEXT>
<TOKEN end_char="9942" id="token-101-0" morph="none" pos="word" start_char="9940">The</TOKEN>
<TOKEN end_char="9945" id="token-101-1" morph="none" pos="word" start_char="9944">AI</TOKEN>
<TOKEN end_char="9947" id="token-101-2" morph="none" pos="punct" start_char="9947">:</TOKEN>
<TOKEN end_char="9952" id="token-101-3" morph="none" pos="word" start_char="9949">that</TOKEN>
<TOKEN end_char="9955" id="token-101-4" morph="none" pos="word" start_char="9954">is</TOKEN>
<TOKEN end_char="9961" id="token-101-5" morph="none" pos="word" start_char="9957">owned</TOKEN>
<TOKEN end_char="9962" id="token-101-6" morph="none" pos="punct" start_char="9962">,</TOKEN>
<TOKEN end_char="9971" id="token-101-7" morph="none" pos="word" start_char="9964">financed</TOKEN>
<TOKEN end_char="9972" id="token-101-8" morph="none" pos="punct" start_char="9972">,</TOKEN>
<TOKEN end_char="9976" id="token-101-9" morph="none" pos="word" start_char="9974">and</TOKEN>
<TOKEN end_char="9987" id="token-101-10" morph="none" pos="word" start_char="9978">programmed</TOKEN>
<TOKEN end_char="9990" id="token-101-11" morph="none" pos="word" start_char="9989">by</TOKEN>
<TOKEN end_char="9998" id="token-101-12" morph="none" pos="word" start_char="9992">someone</TOKEN>
<TOKEN end_char="9999" id="token-101-13" morph="none" pos="punct" start_char="9999">,</TOKEN>
<TOKEN end_char="10002" id="token-101-14" morph="none" pos="word" start_char="10001">or</TOKEN>
<TOKEN end_char="10007" id="token-101-15" morph="none" pos="word" start_char="10004">some</TOKEN>
<TOKEN end_char="10020" id="token-101-16" morph="none" pos="word" start_char="10009">organization</TOKEN>
<TOKEN end_char="10021" id="token-101-17" morph="none" pos="punct" start_char="10021">.</TOKEN>
</SEG>
<SEG end_char="10051" id="segment-102" start_char="10024">
<ORIGINAL_TEXT>What might their agenda be ?</ORIGINAL_TEXT>
<TOKEN end_char="10027" id="token-102-0" morph="none" pos="word" start_char="10024">What</TOKEN>
<TOKEN end_char="10033" id="token-102-1" morph="none" pos="word" start_char="10029">might</TOKEN>
<TOKEN end_char="10039" id="token-102-2" morph="none" pos="word" start_char="10035">their</TOKEN>
<TOKEN end_char="10046" id="token-102-3" morph="none" pos="word" start_char="10041">agenda</TOKEN>
<TOKEN end_char="10049" id="token-102-4" morph="none" pos="word" start_char="10048">be</TOKEN>
<TOKEN end_char="10051" id="token-102-5" morph="none" pos="punct" start_char="10051">?</TOKEN>
</SEG>
<SEG end_char="10150" id="segment-103" start_char="10054">
<ORIGINAL_TEXT>Will it become more and more difficult, to spot the narrative, the further that AI tech evolves ?</ORIGINAL_TEXT>
<TOKEN end_char="10057" id="token-103-0" morph="none" pos="word" start_char="10054">Will</TOKEN>
<TOKEN end_char="10060" id="token-103-1" morph="none" pos="word" start_char="10059">it</TOKEN>
<TOKEN end_char="10067" id="token-103-2" morph="none" pos="word" start_char="10062">become</TOKEN>
<TOKEN end_char="10072" id="token-103-3" morph="none" pos="word" start_char="10069">more</TOKEN>
<TOKEN end_char="10076" id="token-103-4" morph="none" pos="word" start_char="10074">and</TOKEN>
<TOKEN end_char="10081" id="token-103-5" morph="none" pos="word" start_char="10078">more</TOKEN>
<TOKEN end_char="10091" id="token-103-6" morph="none" pos="word" start_char="10083">difficult</TOKEN>
<TOKEN end_char="10092" id="token-103-7" morph="none" pos="punct" start_char="10092">,</TOKEN>
<TOKEN end_char="10095" id="token-103-8" morph="none" pos="word" start_char="10094">to</TOKEN>
<TOKEN end_char="10100" id="token-103-9" morph="none" pos="word" start_char="10097">spot</TOKEN>
<TOKEN end_char="10104" id="token-103-10" morph="none" pos="word" start_char="10102">the</TOKEN>
<TOKEN end_char="10114" id="token-103-11" morph="none" pos="word" start_char="10106">narrative</TOKEN>
<TOKEN end_char="10115" id="token-103-12" morph="none" pos="punct" start_char="10115">,</TOKEN>
<TOKEN end_char="10119" id="token-103-13" morph="none" pos="word" start_char="10117">the</TOKEN>
<TOKEN end_char="10127" id="token-103-14" morph="none" pos="word" start_char="10121">further</TOKEN>
<TOKEN end_char="10132" id="token-103-15" morph="none" pos="word" start_char="10129">that</TOKEN>
<TOKEN end_char="10135" id="token-103-16" morph="none" pos="word" start_char="10134">AI</TOKEN>
<TOKEN end_char="10140" id="token-103-17" morph="none" pos="word" start_char="10137">tech</TOKEN>
<TOKEN end_char="10148" id="token-103-18" morph="none" pos="word" start_char="10142">evolves</TOKEN>
<TOKEN end_char="10150" id="token-103-19" morph="none" pos="punct" start_char="10150">?</TOKEN>
</SEG>
<SEG end_char="10201" id="segment-104" start_char="10153">
<ORIGINAL_TEXT>( Ooops : that almost sounds like : AI evolves...</ORIGINAL_TEXT>
<TOKEN end_char="10153" id="token-104-0" morph="none" pos="punct" start_char="10153">(</TOKEN>
<TOKEN end_char="10159" id="token-104-1" morph="none" pos="word" start_char="10155">Ooops</TOKEN>
<TOKEN end_char="10161" id="token-104-2" morph="none" pos="punct" start_char="10161">:</TOKEN>
<TOKEN end_char="10166" id="token-104-3" morph="none" pos="word" start_char="10163">that</TOKEN>
<TOKEN end_char="10173" id="token-104-4" morph="none" pos="word" start_char="10168">almost</TOKEN>
<TOKEN end_char="10180" id="token-104-5" morph="none" pos="word" start_char="10175">sounds</TOKEN>
<TOKEN end_char="10185" id="token-104-6" morph="none" pos="word" start_char="10182">like</TOKEN>
<TOKEN end_char="10187" id="token-104-7" morph="none" pos="punct" start_char="10187">:</TOKEN>
<TOKEN end_char="10190" id="token-104-8" morph="none" pos="word" start_char="10189">AI</TOKEN>
<TOKEN end_char="10198" id="token-104-9" morph="none" pos="word" start_char="10192">evolves</TOKEN>
<TOKEN end_char="10201" id="token-104-10" morph="none" pos="punct" start_char="10199">...</TOKEN>
</SEG>
<SEG end_char="10210" id="segment-105" start_char="10203">
<ORIGINAL_TEXT>LoL !! )</ORIGINAL_TEXT>
<TOKEN end_char="10205" id="token-105-0" morph="none" pos="word" start_char="10203">LoL</TOKEN>
<TOKEN end_char="10208" id="token-105-1" morph="none" pos="punct" start_char="10207">!!</TOKEN>
<TOKEN end_char="10210" id="token-105-2" morph="none" pos="punct" start_char="10210">)</TOKEN>
<TRANSLATED_TEXT>LoL!!!</TRANSLATED_TEXT><DETECTED_LANGUAGE>fr</DETECTED_LANGUAGE></SEG>
<SEG end_char="10313" id="segment-106" start_char="10214">
<ORIGINAL_TEXT>One thing I will add is that Fauci used tax payer dollars to pay for that gain of function research.</ORIGINAL_TEXT>
<TOKEN end_char="10216" id="token-106-0" morph="none" pos="word" start_char="10214">One</TOKEN>
<TOKEN end_char="10222" id="token-106-1" morph="none" pos="word" start_char="10218">thing</TOKEN>
<TOKEN end_char="10224" id="token-106-2" morph="none" pos="word" start_char="10224">I</TOKEN>
<TOKEN end_char="10229" id="token-106-3" morph="none" pos="word" start_char="10226">will</TOKEN>
<TOKEN end_char="10233" id="token-106-4" morph="none" pos="word" start_char="10231">add</TOKEN>
<TOKEN end_char="10236" id="token-106-5" morph="none" pos="word" start_char="10235">is</TOKEN>
<TOKEN end_char="10241" id="token-106-6" morph="none" pos="word" start_char="10238">that</TOKEN>
<TOKEN end_char="10247" id="token-106-7" morph="none" pos="word" start_char="10243">Fauci</TOKEN>
<TOKEN end_char="10252" id="token-106-8" morph="none" pos="word" start_char="10249">used</TOKEN>
<TOKEN end_char="10256" id="token-106-9" morph="none" pos="word" start_char="10254">tax</TOKEN>
<TOKEN end_char="10262" id="token-106-10" morph="none" pos="word" start_char="10258">payer</TOKEN>
<TOKEN end_char="10270" id="token-106-11" morph="none" pos="word" start_char="10264">dollars</TOKEN>
<TOKEN end_char="10273" id="token-106-12" morph="none" pos="word" start_char="10272">to</TOKEN>
<TOKEN end_char="10277" id="token-106-13" morph="none" pos="word" start_char="10275">pay</TOKEN>
<TOKEN end_char="10281" id="token-106-14" morph="none" pos="word" start_char="10279">for</TOKEN>
<TOKEN end_char="10286" id="token-106-15" morph="none" pos="word" start_char="10283">that</TOKEN>
<TOKEN end_char="10291" id="token-106-16" morph="none" pos="word" start_char="10288">gain</TOKEN>
<TOKEN end_char="10294" id="token-106-17" morph="none" pos="word" start_char="10293">of</TOKEN>
<TOKEN end_char="10303" id="token-106-18" morph="none" pos="word" start_char="10296">function</TOKEN>
<TOKEN end_char="10312" id="token-106-19" morph="none" pos="word" start_char="10305">research</TOKEN>
<TOKEN end_char="10313" id="token-106-20" morph="none" pos="punct" start_char="10313">.</TOKEN>
</SEG>
<SEG end_char="10398" id="segment-107" start_char="10316">
<ORIGINAL_TEXT>This has all been known by those who followed the bread crumbs since the beginning.</ORIGINAL_TEXT>
<TOKEN end_char="10319" id="token-107-0" morph="none" pos="word" start_char="10316">This</TOKEN>
<TOKEN end_char="10323" id="token-107-1" morph="none" pos="word" start_char="10321">has</TOKEN>
<TOKEN end_char="10327" id="token-107-2" morph="none" pos="word" start_char="10325">all</TOKEN>
<TOKEN end_char="10332" id="token-107-3" morph="none" pos="word" start_char="10329">been</TOKEN>
<TOKEN end_char="10338" id="token-107-4" morph="none" pos="word" start_char="10334">known</TOKEN>
<TOKEN end_char="10341" id="token-107-5" morph="none" pos="word" start_char="10340">by</TOKEN>
<TOKEN end_char="10347" id="token-107-6" morph="none" pos="word" start_char="10343">those</TOKEN>
<TOKEN end_char="10351" id="token-107-7" morph="none" pos="word" start_char="10349">who</TOKEN>
<TOKEN end_char="10360" id="token-107-8" morph="none" pos="word" start_char="10353">followed</TOKEN>
<TOKEN end_char="10364" id="token-107-9" morph="none" pos="word" start_char="10362">the</TOKEN>
<TOKEN end_char="10370" id="token-107-10" morph="none" pos="word" start_char="10366">bread</TOKEN>
<TOKEN end_char="10377" id="token-107-11" morph="none" pos="word" start_char="10372">crumbs</TOKEN>
<TOKEN end_char="10383" id="token-107-12" morph="none" pos="word" start_char="10379">since</TOKEN>
<TOKEN end_char="10387" id="token-107-13" morph="none" pos="word" start_char="10385">the</TOKEN>
<TOKEN end_char="10397" id="token-107-14" morph="none" pos="word" start_char="10389">beginning</TOKEN>
<TOKEN end_char="10398" id="token-107-15" morph="none" pos="punct" start_char="10398">.</TOKEN>
</SEG>
<SEG end_char="10531" id="segment-108" start_char="10401">
<ORIGINAL_TEXT>The release in my opinion was not accidental, but rushed out prematurely when Trump's election forced them to bump up their timing.</ORIGINAL_TEXT>
<TOKEN end_char="10403" id="token-108-0" morph="none" pos="word" start_char="10401">The</TOKEN>
<TOKEN end_char="10411" id="token-108-1" morph="none" pos="word" start_char="10405">release</TOKEN>
<TOKEN end_char="10414" id="token-108-2" morph="none" pos="word" start_char="10413">in</TOKEN>
<TOKEN end_char="10417" id="token-108-3" morph="none" pos="word" start_char="10416">my</TOKEN>
<TOKEN end_char="10425" id="token-108-4" morph="none" pos="word" start_char="10419">opinion</TOKEN>
<TOKEN end_char="10429" id="token-108-5" morph="none" pos="word" start_char="10427">was</TOKEN>
<TOKEN end_char="10433" id="token-108-6" morph="none" pos="word" start_char="10431">not</TOKEN>
<TOKEN end_char="10444" id="token-108-7" morph="none" pos="word" start_char="10435">accidental</TOKEN>
<TOKEN end_char="10445" id="token-108-8" morph="none" pos="punct" start_char="10445">,</TOKEN>
<TOKEN end_char="10449" id="token-108-9" morph="none" pos="word" start_char="10447">but</TOKEN>
<TOKEN end_char="10456" id="token-108-10" morph="none" pos="word" start_char="10451">rushed</TOKEN>
<TOKEN end_char="10460" id="token-108-11" morph="none" pos="word" start_char="10458">out</TOKEN>
<TOKEN end_char="10472" id="token-108-12" morph="none" pos="word" start_char="10462">prematurely</TOKEN>
<TOKEN end_char="10477" id="token-108-13" morph="none" pos="word" start_char="10474">when</TOKEN>
<TOKEN end_char="10485" id="token-108-14" morph="none" pos="word" start_char="10479">Trump's</TOKEN>
<TOKEN end_char="10494" id="token-108-15" morph="none" pos="word" start_char="10487">election</TOKEN>
<TOKEN end_char="10501" id="token-108-16" morph="none" pos="word" start_char="10496">forced</TOKEN>
<TOKEN end_char="10506" id="token-108-17" morph="none" pos="word" start_char="10503">them</TOKEN>
<TOKEN end_char="10509" id="token-108-18" morph="none" pos="word" start_char="10508">to</TOKEN>
<TOKEN end_char="10514" id="token-108-19" morph="none" pos="word" start_char="10511">bump</TOKEN>
<TOKEN end_char="10517" id="token-108-20" morph="none" pos="word" start_char="10516">up</TOKEN>
<TOKEN end_char="10523" id="token-108-21" morph="none" pos="word" start_char="10519">their</TOKEN>
<TOKEN end_char="10530" id="token-108-22" morph="none" pos="word" start_char="10525">timing</TOKEN>
<TOKEN end_char="10531" id="token-108-23" morph="none" pos="punct" start_char="10531">.</TOKEN>
</SEG>
<SEG end_char="10632" id="segment-109" start_char="10534">
<ORIGINAL_TEXT>My best bet is the plan is now to continue the hype while continuing work on the real deal eos bug.</ORIGINAL_TEXT>
<TOKEN end_char="10535" id="token-109-0" morph="none" pos="word" start_char="10534">My</TOKEN>
<TOKEN end_char="10540" id="token-109-1" morph="none" pos="word" start_char="10537">best</TOKEN>
<TOKEN end_char="10544" id="token-109-2" morph="none" pos="word" start_char="10542">bet</TOKEN>
<TOKEN end_char="10547" id="token-109-3" morph="none" pos="word" start_char="10546">is</TOKEN>
<TOKEN end_char="10551" id="token-109-4" morph="none" pos="word" start_char="10549">the</TOKEN>
<TOKEN end_char="10556" id="token-109-5" morph="none" pos="word" start_char="10553">plan</TOKEN>
<TOKEN end_char="10559" id="token-109-6" morph="none" pos="word" start_char="10558">is</TOKEN>
<TOKEN end_char="10563" id="token-109-7" morph="none" pos="word" start_char="10561">now</TOKEN>
<TOKEN end_char="10566" id="token-109-8" morph="none" pos="word" start_char="10565">to</TOKEN>
<TOKEN end_char="10575" id="token-109-9" morph="none" pos="word" start_char="10568">continue</TOKEN>
<TOKEN end_char="10579" id="token-109-10" morph="none" pos="word" start_char="10577">the</TOKEN>
<TOKEN end_char="10584" id="token-109-11" morph="none" pos="word" start_char="10581">hype</TOKEN>
<TOKEN end_char="10590" id="token-109-12" morph="none" pos="word" start_char="10586">while</TOKEN>
<TOKEN end_char="10601" id="token-109-13" morph="none" pos="word" start_char="10592">continuing</TOKEN>
<TOKEN end_char="10606" id="token-109-14" morph="none" pos="word" start_char="10603">work</TOKEN>
<TOKEN end_char="10609" id="token-109-15" morph="none" pos="word" start_char="10608">on</TOKEN>
<TOKEN end_char="10613" id="token-109-16" morph="none" pos="word" start_char="10611">the</TOKEN>
<TOKEN end_char="10618" id="token-109-17" morph="none" pos="word" start_char="10615">real</TOKEN>
<TOKEN end_char="10623" id="token-109-18" morph="none" pos="word" start_char="10620">deal</TOKEN>
<TOKEN end_char="10627" id="token-109-19" morph="none" pos="word" start_char="10625">eos</TOKEN>
<TOKEN end_char="10631" id="token-109-20" morph="none" pos="word" start_char="10629">bug</TOKEN>
<TOKEN end_char="10632" id="token-109-21" morph="none" pos="punct" start_char="10632">.</TOKEN>
</SEG>
<SEG end_char="10928" id="segment-110" start_char="10634">
<ORIGINAL_TEXT>Once that one gets unleashed people will be begging to be chipped and let into sterile FEMA work camps, and those who can't will still beg to have their children taken from them and put in one, so their children can at least have a chance at life even if it is in a FEMA slave camp or work farm.</ORIGINAL_TEXT>
<TOKEN end_char="10637" id="token-110-0" morph="none" pos="word" start_char="10634">Once</TOKEN>
<TOKEN end_char="10642" id="token-110-1" morph="none" pos="word" start_char="10639">that</TOKEN>
<TOKEN end_char="10646" id="token-110-2" morph="none" pos="word" start_char="10644">one</TOKEN>
<TOKEN end_char="10651" id="token-110-3" morph="none" pos="word" start_char="10648">gets</TOKEN>
<TOKEN end_char="10661" id="token-110-4" morph="none" pos="word" start_char="10653">unleashed</TOKEN>
<TOKEN end_char="10668" id="token-110-5" morph="none" pos="word" start_char="10663">people</TOKEN>
<TOKEN end_char="10673" id="token-110-6" morph="none" pos="word" start_char="10670">will</TOKEN>
<TOKEN end_char="10676" id="token-110-7" morph="none" pos="word" start_char="10675">be</TOKEN>
<TOKEN end_char="10684" id="token-110-8" morph="none" pos="word" start_char="10678">begging</TOKEN>
<TOKEN end_char="10687" id="token-110-9" morph="none" pos="word" start_char="10686">to</TOKEN>
<TOKEN end_char="10690" id="token-110-10" morph="none" pos="word" start_char="10689">be</TOKEN>
<TOKEN end_char="10698" id="token-110-11" morph="none" pos="word" start_char="10692">chipped</TOKEN>
<TOKEN end_char="10702" id="token-110-12" morph="none" pos="word" start_char="10700">and</TOKEN>
<TOKEN end_char="10706" id="token-110-13" morph="none" pos="word" start_char="10704">let</TOKEN>
<TOKEN end_char="10711" id="token-110-14" morph="none" pos="word" start_char="10708">into</TOKEN>
<TOKEN end_char="10719" id="token-110-15" morph="none" pos="word" start_char="10713">sterile</TOKEN>
<TOKEN end_char="10724" id="token-110-16" morph="none" pos="word" start_char="10721">FEMA</TOKEN>
<TOKEN end_char="10729" id="token-110-17" morph="none" pos="word" start_char="10726">work</TOKEN>
<TOKEN end_char="10735" id="token-110-18" morph="none" pos="word" start_char="10731">camps</TOKEN>
<TOKEN end_char="10736" id="token-110-19" morph="none" pos="punct" start_char="10736">,</TOKEN>
<TOKEN end_char="10740" id="token-110-20" morph="none" pos="word" start_char="10738">and</TOKEN>
<TOKEN end_char="10746" id="token-110-21" morph="none" pos="word" start_char="10742">those</TOKEN>
<TOKEN end_char="10750" id="token-110-22" morph="none" pos="word" start_char="10748">who</TOKEN>
<TOKEN end_char="10756" id="token-110-23" morph="none" pos="word" start_char="10752">can't</TOKEN>
<TOKEN end_char="10761" id="token-110-24" morph="none" pos="word" start_char="10758">will</TOKEN>
<TOKEN end_char="10767" id="token-110-25" morph="none" pos="word" start_char="10763">still</TOKEN>
<TOKEN end_char="10771" id="token-110-26" morph="none" pos="word" start_char="10769">beg</TOKEN>
<TOKEN end_char="10774" id="token-110-27" morph="none" pos="word" start_char="10773">to</TOKEN>
<TOKEN end_char="10779" id="token-110-28" morph="none" pos="word" start_char="10776">have</TOKEN>
<TOKEN end_char="10785" id="token-110-29" morph="none" pos="word" start_char="10781">their</TOKEN>
<TOKEN end_char="10794" id="token-110-30" morph="none" pos="word" start_char="10787">children</TOKEN>
<TOKEN end_char="10800" id="token-110-31" morph="none" pos="word" start_char="10796">taken</TOKEN>
<TOKEN end_char="10805" id="token-110-32" morph="none" pos="word" start_char="10802">from</TOKEN>
<TOKEN end_char="10810" id="token-110-33" morph="none" pos="word" start_char="10807">them</TOKEN>
<TOKEN end_char="10814" id="token-110-34" morph="none" pos="word" start_char="10812">and</TOKEN>
<TOKEN end_char="10818" id="token-110-35" morph="none" pos="word" start_char="10816">put</TOKEN>
<TOKEN end_char="10821" id="token-110-36" morph="none" pos="word" start_char="10820">in</TOKEN>
<TOKEN end_char="10825" id="token-110-37" morph="none" pos="word" start_char="10823">one</TOKEN>
<TOKEN end_char="10826" id="token-110-38" morph="none" pos="punct" start_char="10826">,</TOKEN>
<TOKEN end_char="10829" id="token-110-39" morph="none" pos="word" start_char="10828">so</TOKEN>
<TOKEN end_char="10835" id="token-110-40" morph="none" pos="word" start_char="10831">their</TOKEN>
<TOKEN end_char="10844" id="token-110-41" morph="none" pos="word" start_char="10837">children</TOKEN>
<TOKEN end_char="10848" id="token-110-42" morph="none" pos="word" start_char="10846">can</TOKEN>
<TOKEN end_char="10851" id="token-110-43" morph="none" pos="word" start_char="10850">at</TOKEN>
<TOKEN end_char="10857" id="token-110-44" morph="none" pos="word" start_char="10853">least</TOKEN>
<TOKEN end_char="10862" id="token-110-45" morph="none" pos="word" start_char="10859">have</TOKEN>
<TOKEN end_char="10864" id="token-110-46" morph="none" pos="word" start_char="10864">a</TOKEN>
<TOKEN end_char="10871" id="token-110-47" morph="none" pos="word" start_char="10866">chance</TOKEN>
<TOKEN end_char="10874" id="token-110-48" morph="none" pos="word" start_char="10873">at</TOKEN>
<TOKEN end_char="10879" id="token-110-49" morph="none" pos="word" start_char="10876">life</TOKEN>
<TOKEN end_char="10884" id="token-110-50" morph="none" pos="word" start_char="10881">even</TOKEN>
<TOKEN end_char="10887" id="token-110-51" morph="none" pos="word" start_char="10886">if</TOKEN>
<TOKEN end_char="10890" id="token-110-52" morph="none" pos="word" start_char="10889">it</TOKEN>
<TOKEN end_char="10893" id="token-110-53" morph="none" pos="word" start_char="10892">is</TOKEN>
<TOKEN end_char="10896" id="token-110-54" morph="none" pos="word" start_char="10895">in</TOKEN>
<TOKEN end_char="10898" id="token-110-55" morph="none" pos="word" start_char="10898">a</TOKEN>
<TOKEN end_char="10903" id="token-110-56" morph="none" pos="word" start_char="10900">FEMA</TOKEN>
<TOKEN end_char="10909" id="token-110-57" morph="none" pos="word" start_char="10905">slave</TOKEN>
<TOKEN end_char="10914" id="token-110-58" morph="none" pos="word" start_char="10911">camp</TOKEN>
<TOKEN end_char="10917" id="token-110-59" morph="none" pos="word" start_char="10916">or</TOKEN>
<TOKEN end_char="10922" id="token-110-60" morph="none" pos="word" start_char="10919">work</TOKEN>
<TOKEN end_char="10927" id="token-110-61" morph="none" pos="word" start_char="10924">farm</TOKEN>
<TOKEN end_char="10928" id="token-110-62" morph="none" pos="punct" start_char="10928">.</TOKEN>
</SEG>
<SEG end_char="11009" id="segment-111" start_char="10931">
<ORIGINAL_TEXT>Think of covid as the foreplay that will tell them how far they can go with us.</ORIGINAL_TEXT>
<TOKEN end_char="10935" id="token-111-0" morph="none" pos="word" start_char="10931">Think</TOKEN>
<TOKEN end_char="10938" id="token-111-1" morph="none" pos="word" start_char="10937">of</TOKEN>
<TOKEN end_char="10944" id="token-111-2" morph="none" pos="word" start_char="10940">covid</TOKEN>
<TOKEN end_char="10947" id="token-111-3" morph="none" pos="word" start_char="10946">as</TOKEN>
<TOKEN end_char="10951" id="token-111-4" morph="none" pos="word" start_char="10949">the</TOKEN>
<TOKEN end_char="10960" id="token-111-5" morph="none" pos="word" start_char="10953">foreplay</TOKEN>
<TOKEN end_char="10965" id="token-111-6" morph="none" pos="word" start_char="10962">that</TOKEN>
<TOKEN end_char="10970" id="token-111-7" morph="none" pos="word" start_char="10967">will</TOKEN>
<TOKEN end_char="10975" id="token-111-8" morph="none" pos="word" start_char="10972">tell</TOKEN>
<TOKEN end_char="10980" id="token-111-9" morph="none" pos="word" start_char="10977">them</TOKEN>
<TOKEN end_char="10984" id="token-111-10" morph="none" pos="word" start_char="10982">how</TOKEN>
<TOKEN end_char="10988" id="token-111-11" morph="none" pos="word" start_char="10986">far</TOKEN>
<TOKEN end_char="10993" id="token-111-12" morph="none" pos="word" start_char="10990">they</TOKEN>
<TOKEN end_char="10997" id="token-111-13" morph="none" pos="word" start_char="10995">can</TOKEN>
<TOKEN end_char="11000" id="token-111-14" morph="none" pos="word" start_char="10999">go</TOKEN>
<TOKEN end_char="11005" id="token-111-15" morph="none" pos="word" start_char="11002">with</TOKEN>
<TOKEN end_char="11008" id="token-111-16" morph="none" pos="word" start_char="11007">us</TOKEN>
<TOKEN end_char="11009" id="token-111-17" morph="none" pos="punct" start_char="11009">.</TOKEN>
</SEG>
<SEG end_char="11188" id="segment-112" start_char="11011">
<ORIGINAL_TEXT>Once mandated vaccines, lock downs, and mask mandates, and ubi have become fully normalized for the seasonal flue come covid, the other shoe should be just getting ready to drop.</ORIGINAL_TEXT>
<TOKEN end_char="11014" id="token-112-0" morph="none" pos="word" start_char="11011">Once</TOKEN>
<TOKEN end_char="11023" id="token-112-1" morph="none" pos="word" start_char="11016">mandated</TOKEN>
<TOKEN end_char="11032" id="token-112-2" morph="none" pos="word" start_char="11025">vaccines</TOKEN>
<TOKEN end_char="11033" id="token-112-3" morph="none" pos="punct" start_char="11033">,</TOKEN>
<TOKEN end_char="11038" id="token-112-4" morph="none" pos="word" start_char="11035">lock</TOKEN>
<TOKEN end_char="11044" id="token-112-5" morph="none" pos="word" start_char="11040">downs</TOKEN>
<TOKEN end_char="11045" id="token-112-6" morph="none" pos="punct" start_char="11045">,</TOKEN>
<TOKEN end_char="11049" id="token-112-7" morph="none" pos="word" start_char="11047">and</TOKEN>
<TOKEN end_char="11054" id="token-112-8" morph="none" pos="word" start_char="11051">mask</TOKEN>
<TOKEN end_char="11063" id="token-112-9" morph="none" pos="word" start_char="11056">mandates</TOKEN>
<TOKEN end_char="11064" id="token-112-10" morph="none" pos="punct" start_char="11064">,</TOKEN>
<TOKEN end_char="11068" id="token-112-11" morph="none" pos="word" start_char="11066">and</TOKEN>
<TOKEN end_char="11072" id="token-112-12" morph="none" pos="word" start_char="11070">ubi</TOKEN>
<TOKEN end_char="11077" id="token-112-13" morph="none" pos="word" start_char="11074">have</TOKEN>
<TOKEN end_char="11084" id="token-112-14" morph="none" pos="word" start_char="11079">become</TOKEN>
<TOKEN end_char="11090" id="token-112-15" morph="none" pos="word" start_char="11086">fully</TOKEN>
<TOKEN end_char="11101" id="token-112-16" morph="none" pos="word" start_char="11092">normalized</TOKEN>
<TOKEN end_char="11105" id="token-112-17" morph="none" pos="word" start_char="11103">for</TOKEN>
<TOKEN end_char="11109" id="token-112-18" morph="none" pos="word" start_char="11107">the</TOKEN>
<TOKEN end_char="11118" id="token-112-19" morph="none" pos="word" start_char="11111">seasonal</TOKEN>
<TOKEN end_char="11123" id="token-112-20" morph="none" pos="word" start_char="11120">flue</TOKEN>
<TOKEN end_char="11128" id="token-112-21" morph="none" pos="word" start_char="11125">come</TOKEN>
<TOKEN end_char="11134" id="token-112-22" morph="none" pos="word" start_char="11130">covid</TOKEN>
<TOKEN end_char="11135" id="token-112-23" morph="none" pos="punct" start_char="11135">,</TOKEN>
<TOKEN end_char="11139" id="token-112-24" morph="none" pos="word" start_char="11137">the</TOKEN>
<TOKEN end_char="11145" id="token-112-25" morph="none" pos="word" start_char="11141">other</TOKEN>
<TOKEN end_char="11150" id="token-112-26" morph="none" pos="word" start_char="11147">shoe</TOKEN>
<TOKEN end_char="11157" id="token-112-27" morph="none" pos="word" start_char="11152">should</TOKEN>
<TOKEN end_char="11160" id="token-112-28" morph="none" pos="word" start_char="11159">be</TOKEN>
<TOKEN end_char="11165" id="token-112-29" morph="none" pos="word" start_char="11162">just</TOKEN>
<TOKEN end_char="11173" id="token-112-30" morph="none" pos="word" start_char="11167">getting</TOKEN>
<TOKEN end_char="11179" id="token-112-31" morph="none" pos="word" start_char="11175">ready</TOKEN>
<TOKEN end_char="11182" id="token-112-32" morph="none" pos="word" start_char="11181">to</TOKEN>
<TOKEN end_char="11187" id="token-112-33" morph="none" pos="word" start_char="11184">drop</TOKEN>
<TOKEN end_char="11188" id="token-112-34" morph="none" pos="punct" start_char="11188">.</TOKEN>
</SEG>
<SEG end_char="11286" id="segment-113" start_char="11193">
<ORIGINAL_TEXT>originally posted by: CriticalStinker a reply to: slatesteam Your point is purely speculative.</ORIGINAL_TEXT>
<TOKEN end_char="11202" id="token-113-0" morph="none" pos="word" start_char="11193">originally</TOKEN>
<TOKEN end_char="11209" id="token-113-1" morph="none" pos="word" start_char="11204">posted</TOKEN>
<TOKEN end_char="11212" id="token-113-2" morph="none" pos="word" start_char="11211">by</TOKEN>
<TOKEN end_char="11213" id="token-113-3" morph="none" pos="punct" start_char="11213">:</TOKEN>
<TOKEN end_char="11229" id="token-113-4" morph="none" pos="word" start_char="11215">CriticalStinker</TOKEN>
<TOKEN end_char="11231" id="token-113-5" morph="none" pos="word" start_char="11231">a</TOKEN>
<TOKEN end_char="11237" id="token-113-6" morph="none" pos="word" start_char="11233">reply</TOKEN>
<TOKEN end_char="11240" id="token-113-7" morph="none" pos="word" start_char="11239">to</TOKEN>
<TOKEN end_char="11241" id="token-113-8" morph="none" pos="punct" start_char="11241">:</TOKEN>
<TOKEN end_char="11252" id="token-113-9" morph="none" pos="word" start_char="11243">slatesteam</TOKEN>
<TOKEN end_char="11257" id="token-113-10" morph="none" pos="word" start_char="11254">Your</TOKEN>
<TOKEN end_char="11263" id="token-113-11" morph="none" pos="word" start_char="11259">point</TOKEN>
<TOKEN end_char="11266" id="token-113-12" morph="none" pos="word" start_char="11265">is</TOKEN>
<TOKEN end_char="11273" id="token-113-13" morph="none" pos="word" start_char="11268">purely</TOKEN>
<TOKEN end_char="11285" id="token-113-14" morph="none" pos="word" start_char="11275">speculative</TOKEN>
<TOKEN end_char="11286" id="token-113-15" morph="none" pos="punct" start_char="11286">.</TOKEN>
</SEG>
<SEG end_char="11345" id="segment-114" start_char="11288">
<ORIGINAL_TEXT>If that was the case, I'd expect more rhetoric from China.</ORIGINAL_TEXT>
<TOKEN end_char="11289" id="token-114-0" morph="none" pos="word" start_char="11288">If</TOKEN>
<TOKEN end_char="11294" id="token-114-1" morph="none" pos="word" start_char="11291">that</TOKEN>
<TOKEN end_char="11298" id="token-114-2" morph="none" pos="word" start_char="11296">was</TOKEN>
<TOKEN end_char="11302" id="token-114-3" morph="none" pos="word" start_char="11300">the</TOKEN>
<TOKEN end_char="11307" id="token-114-4" morph="none" pos="word" start_char="11304">case</TOKEN>
<TOKEN end_char="11308" id="token-114-5" morph="none" pos="punct" start_char="11308">,</TOKEN>
<TOKEN end_char="11312" id="token-114-6" morph="none" pos="word" start_char="11310">I'd</TOKEN>
<TOKEN end_char="11319" id="token-114-7" morph="none" pos="word" start_char="11314">expect</TOKEN>
<TOKEN end_char="11324" id="token-114-8" morph="none" pos="word" start_char="11321">more</TOKEN>
<TOKEN end_char="11333" id="token-114-9" morph="none" pos="word" start_char="11326">rhetoric</TOKEN>
<TOKEN end_char="11338" id="token-114-10" morph="none" pos="word" start_char="11335">from</TOKEN>
<TOKEN end_char="11344" id="token-114-11" morph="none" pos="word" start_char="11340">China</TOKEN>
<TOKEN end_char="11345" id="token-114-12" morph="none" pos="punct" start_char="11345">.</TOKEN>
</SEG>
<SEG end_char="11393" id="segment-115" start_char="11348">
<ORIGINAL_TEXT>Define more.... thats all the CCP is good for</ORIGINAL_TEXT>
<TOKEN end_char="11353" id="token-115-0" morph="none" pos="word" start_char="11348">Define</TOKEN>
<TOKEN end_char="11358" id="token-115-1" morph="none" pos="word" start_char="11355">more</TOKEN>
<TOKEN end_char="11362" id="token-115-2" morph="none" pos="punct" start_char="11359">....</TOKEN>
<TOKEN end_char="11369" id="token-115-3" morph="none" pos="word" start_char="11364">thats</TOKEN>
<TOKEN end_char="11373" id="token-115-4" morph="none" pos="word" start_char="11371">all</TOKEN>
<TOKEN end_char="11377" id="token-115-5" morph="none" pos="word" start_char="11375">the</TOKEN>
<TOKEN end_char="11381" id="token-115-6" morph="none" pos="word" start_char="11379">CCP</TOKEN>
<TOKEN end_char="11384" id="token-115-7" morph="none" pos="word" start_char="11383">is</TOKEN>
<TOKEN end_char="11389" id="token-115-8" morph="none" pos="word" start_char="11386">good</TOKEN>
<TOKEN end_char="11393" id="token-115-9" morph="none" pos="word" start_char="11391">for</TOKEN>
</SEG>
<SEG end_char="11407" id="segment-116" start_char="11396">
<ORIGINAL_TEXT>And so what.</ORIGINAL_TEXT>
<TOKEN end_char="11398" id="token-116-0" morph="none" pos="word" start_char="11396">And</TOKEN>
<TOKEN end_char="11401" id="token-116-1" morph="none" pos="word" start_char="11400">so</TOKEN>
<TOKEN end_char="11406" id="token-116-2" morph="none" pos="word" start_char="11403">what</TOKEN>
<TOKEN end_char="11407" id="token-116-3" morph="none" pos="punct" start_char="11407">.</TOKEN>
</SEG>
<SEG end_char="11540" id="segment-117" start_char="11409">
<ORIGINAL_TEXT>It cant be refuted and its just food for thought.... all this is confirmable btw, with the exception this is EXACTLY what happened</ORIGINAL_TEXT>
<TOKEN end_char="11410" id="token-117-0" morph="none" pos="word" start_char="11409">It</TOKEN>
<TOKEN end_char="11416" id="token-117-1" morph="none" pos="word" start_char="11412">cant</TOKEN>
<TOKEN end_char="11419" id="token-117-2" morph="none" pos="word" start_char="11418">be</TOKEN>
<TOKEN end_char="11427" id="token-117-3" morph="none" pos="word" start_char="11421">refuted</TOKEN>
<TOKEN end_char="11431" id="token-117-4" morph="none" pos="word" start_char="11429">and</TOKEN>
<TOKEN end_char="11436" id="token-117-5" morph="none" pos="word" start_char="11433">its</TOKEN>
<TOKEN end_char="11441" id="token-117-6" morph="none" pos="word" start_char="11438">just</TOKEN>
<TOKEN end_char="11446" id="token-117-7" morph="none" pos="word" start_char="11443">food</TOKEN>
<TOKEN end_char="11450" id="token-117-8" morph="none" pos="word" start_char="11448">for</TOKEN>
<TOKEN end_char="11458" id="token-117-9" morph="none" pos="word" start_char="11452">thought</TOKEN>
<TOKEN end_char="11462" id="token-117-10" morph="none" pos="punct" start_char="11459">....</TOKEN>
<TOKEN end_char="11466" id="token-117-11" morph="none" pos="word" start_char="11464">all</TOKEN>
<TOKEN end_char="11471" id="token-117-12" morph="none" pos="word" start_char="11468">this</TOKEN>
<TOKEN end_char="11474" id="token-117-13" morph="none" pos="word" start_char="11473">is</TOKEN>
<TOKEN end_char="11486" id="token-117-14" morph="none" pos="word" start_char="11476">confirmable</TOKEN>
<TOKEN end_char="11490" id="token-117-15" morph="none" pos="word" start_char="11488">btw</TOKEN>
<TOKEN end_char="11491" id="token-117-16" morph="none" pos="punct" start_char="11491">,</TOKEN>
<TOKEN end_char="11496" id="token-117-17" morph="none" pos="word" start_char="11493">with</TOKEN>
<TOKEN end_char="11500" id="token-117-18" morph="none" pos="word" start_char="11498">the</TOKEN>
<TOKEN end_char="11510" id="token-117-19" morph="none" pos="word" start_char="11502">exception</TOKEN>
<TOKEN end_char="11515" id="token-117-20" morph="none" pos="word" start_char="11512">this</TOKEN>
<TOKEN end_char="11518" id="token-117-21" morph="none" pos="word" start_char="11517">is</TOKEN>
<TOKEN end_char="11526" id="token-117-22" morph="none" pos="word" start_char="11520">EXACTLY</TOKEN>
<TOKEN end_char="11531" id="token-117-23" morph="none" pos="word" start_char="11528">what</TOKEN>
<TOKEN end_char="11540" id="token-117-24" morph="none" pos="word" start_char="11533">happened</TOKEN>
</SEG>
<SEG end_char="11561" id="segment-118" start_char="11545">
<ORIGINAL_TEXT>a reply to: dug88</ORIGINAL_TEXT>
<TOKEN end_char="11545" id="token-118-0" morph="none" pos="word" start_char="11545">a</TOKEN>
<TOKEN end_char="11551" id="token-118-1" morph="none" pos="word" start_char="11547">reply</TOKEN>
<TOKEN end_char="11554" id="token-118-2" morph="none" pos="word" start_char="11553">to</TOKEN>
<TOKEN end_char="11555" id="token-118-3" morph="none" pos="punct" start_char="11555">:</TOKEN>
<TOKEN end_char="11561" id="token-118-4" morph="none" pos="word" start_char="11557">dug88</TOKEN>
</SEG>
<SEG end_char="11693" id="segment-119" start_char="11564">
<ORIGINAL_TEXT>The problem with systems like this is that if you put bias data into them you merely get out a conclusion that supports this bias.</ORIGINAL_TEXT>
<TOKEN end_char="11566" id="token-119-0" morph="none" pos="word" start_char="11564">The</TOKEN>
<TOKEN end_char="11574" id="token-119-1" morph="none" pos="word" start_char="11568">problem</TOKEN>
<TOKEN end_char="11579" id="token-119-2" morph="none" pos="word" start_char="11576">with</TOKEN>
<TOKEN end_char="11587" id="token-119-3" morph="none" pos="word" start_char="11581">systems</TOKEN>
<TOKEN end_char="11592" id="token-119-4" morph="none" pos="word" start_char="11589">like</TOKEN>
<TOKEN end_char="11597" id="token-119-5" morph="none" pos="word" start_char="11594">this</TOKEN>
<TOKEN end_char="11600" id="token-119-6" morph="none" pos="word" start_char="11599">is</TOKEN>
<TOKEN end_char="11605" id="token-119-7" morph="none" pos="word" start_char="11602">that</TOKEN>
<TOKEN end_char="11608" id="token-119-8" morph="none" pos="word" start_char="11607">if</TOKEN>
<TOKEN end_char="11612" id="token-119-9" morph="none" pos="word" start_char="11610">you</TOKEN>
<TOKEN end_char="11616" id="token-119-10" morph="none" pos="word" start_char="11614">put</TOKEN>
<TOKEN end_char="11621" id="token-119-11" morph="none" pos="word" start_char="11618">bias</TOKEN>
<TOKEN end_char="11626" id="token-119-12" morph="none" pos="word" start_char="11623">data</TOKEN>
<TOKEN end_char="11631" id="token-119-13" morph="none" pos="word" start_char="11628">into</TOKEN>
<TOKEN end_char="11636" id="token-119-14" morph="none" pos="word" start_char="11633">them</TOKEN>
<TOKEN end_char="11640" id="token-119-15" morph="none" pos="word" start_char="11638">you</TOKEN>
<TOKEN end_char="11647" id="token-119-16" morph="none" pos="word" start_char="11642">merely</TOKEN>
<TOKEN end_char="11651" id="token-119-17" morph="none" pos="word" start_char="11649">get</TOKEN>
<TOKEN end_char="11655" id="token-119-18" morph="none" pos="word" start_char="11653">out</TOKEN>
<TOKEN end_char="11657" id="token-119-19" morph="none" pos="word" start_char="11657">a</TOKEN>
<TOKEN end_char="11668" id="token-119-20" morph="none" pos="word" start_char="11659">conclusion</TOKEN>
<TOKEN end_char="11673" id="token-119-21" morph="none" pos="word" start_char="11670">that</TOKEN>
<TOKEN end_char="11682" id="token-119-22" morph="none" pos="word" start_char="11675">supports</TOKEN>
<TOKEN end_char="11687" id="token-119-23" morph="none" pos="word" start_char="11684">this</TOKEN>
<TOKEN end_char="11692" id="token-119-24" morph="none" pos="word" start_char="11689">bias</TOKEN>
<TOKEN end_char="11693" id="token-119-25" morph="none" pos="punct" start_char="11693">.</TOKEN>
</SEG>
<SEG end_char="11848" id="segment-120" start_char="11696">
<ORIGINAL_TEXT>Like those sentencing algorithms that jailed young black men for longer than normal based on court documents where human judges did the exact same thing.</ORIGINAL_TEXT>
<TOKEN end_char="11699" id="token-120-0" morph="none" pos="word" start_char="11696">Like</TOKEN>
<TOKEN end_char="11705" id="token-120-1" morph="none" pos="word" start_char="11701">those</TOKEN>
<TOKEN end_char="11716" id="token-120-2" morph="none" pos="word" start_char="11707">sentencing</TOKEN>
<TOKEN end_char="11727" id="token-120-3" morph="none" pos="word" start_char="11718">algorithms</TOKEN>
<TOKEN end_char="11732" id="token-120-4" morph="none" pos="word" start_char="11729">that</TOKEN>
<TOKEN end_char="11739" id="token-120-5" morph="none" pos="word" start_char="11734">jailed</TOKEN>
<TOKEN end_char="11745" id="token-120-6" morph="none" pos="word" start_char="11741">young</TOKEN>
<TOKEN end_char="11751" id="token-120-7" morph="none" pos="word" start_char="11747">black</TOKEN>
<TOKEN end_char="11755" id="token-120-8" morph="none" pos="word" start_char="11753">men</TOKEN>
<TOKEN end_char="11759" id="token-120-9" morph="none" pos="word" start_char="11757">for</TOKEN>
<TOKEN end_char="11766" id="token-120-10" morph="none" pos="word" start_char="11761">longer</TOKEN>
<TOKEN end_char="11771" id="token-120-11" morph="none" pos="word" start_char="11768">than</TOKEN>
<TOKEN end_char="11778" id="token-120-12" morph="none" pos="word" start_char="11773">normal</TOKEN>
<TOKEN end_char="11784" id="token-120-13" morph="none" pos="word" start_char="11780">based</TOKEN>
<TOKEN end_char="11787" id="token-120-14" morph="none" pos="word" start_char="11786">on</TOKEN>
<TOKEN end_char="11793" id="token-120-15" morph="none" pos="word" start_char="11789">court</TOKEN>
<TOKEN end_char="11803" id="token-120-16" morph="none" pos="word" start_char="11795">documents</TOKEN>
<TOKEN end_char="11809" id="token-120-17" morph="none" pos="word" start_char="11805">where</TOKEN>
<TOKEN end_char="11815" id="token-120-18" morph="none" pos="word" start_char="11811">human</TOKEN>
<TOKEN end_char="11822" id="token-120-19" morph="none" pos="word" start_char="11817">judges</TOKEN>
<TOKEN end_char="11826" id="token-120-20" morph="none" pos="word" start_char="11824">did</TOKEN>
<TOKEN end_char="11830" id="token-120-21" morph="none" pos="word" start_char="11828">the</TOKEN>
<TOKEN end_char="11836" id="token-120-22" morph="none" pos="word" start_char="11832">exact</TOKEN>
<TOKEN end_char="11841" id="token-120-23" morph="none" pos="word" start_char="11838">same</TOKEN>
<TOKEN end_char="11847" id="token-120-24" morph="none" pos="word" start_char="11843">thing</TOKEN>
<TOKEN end_char="11848" id="token-120-25" morph="none" pos="punct" start_char="11848">.</TOKEN>
</SEG>
<SEG end_char="12003" id="segment-121" start_char="11851">
<ORIGINAL_TEXT>Until they submit their source data and algorithms for peer review I'm going to look at this as being a bias ai confirming the bias if it's bias creator.</ORIGINAL_TEXT>
<TOKEN end_char="11855" id="token-121-0" morph="none" pos="word" start_char="11851">Until</TOKEN>
<TOKEN end_char="11860" id="token-121-1" morph="none" pos="word" start_char="11857">they</TOKEN>
<TOKEN end_char="11867" id="token-121-2" morph="none" pos="word" start_char="11862">submit</TOKEN>
<TOKEN end_char="11873" id="token-121-3" morph="none" pos="word" start_char="11869">their</TOKEN>
<TOKEN end_char="11880" id="token-121-4" morph="none" pos="word" start_char="11875">source</TOKEN>
<TOKEN end_char="11885" id="token-121-5" morph="none" pos="word" start_char="11882">data</TOKEN>
<TOKEN end_char="11889" id="token-121-6" morph="none" pos="word" start_char="11887">and</TOKEN>
<TOKEN end_char="11900" id="token-121-7" morph="none" pos="word" start_char="11891">algorithms</TOKEN>
<TOKEN end_char="11904" id="token-121-8" morph="none" pos="word" start_char="11902">for</TOKEN>
<TOKEN end_char="11909" id="token-121-9" morph="none" pos="word" start_char="11906">peer</TOKEN>
<TOKEN end_char="11916" id="token-121-10" morph="none" pos="word" start_char="11911">review</TOKEN>
<TOKEN end_char="11920" id="token-121-11" morph="none" pos="word" start_char="11918">I'm</TOKEN>
<TOKEN end_char="11926" id="token-121-12" morph="none" pos="word" start_char="11922">going</TOKEN>
<TOKEN end_char="11929" id="token-121-13" morph="none" pos="word" start_char="11928">to</TOKEN>
<TOKEN end_char="11934" id="token-121-14" morph="none" pos="word" start_char="11931">look</TOKEN>
<TOKEN end_char="11937" id="token-121-15" morph="none" pos="word" start_char="11936">at</TOKEN>
<TOKEN end_char="11942" id="token-121-16" morph="none" pos="word" start_char="11939">this</TOKEN>
<TOKEN end_char="11945" id="token-121-17" morph="none" pos="word" start_char="11944">as</TOKEN>
<TOKEN end_char="11951" id="token-121-18" morph="none" pos="word" start_char="11947">being</TOKEN>
<TOKEN end_char="11953" id="token-121-19" morph="none" pos="word" start_char="11953">a</TOKEN>
<TOKEN end_char="11958" id="token-121-20" morph="none" pos="word" start_char="11955">bias</TOKEN>
<TOKEN end_char="11961" id="token-121-21" morph="none" pos="word" start_char="11960">ai</TOKEN>
<TOKEN end_char="11972" id="token-121-22" morph="none" pos="word" start_char="11963">confirming</TOKEN>
<TOKEN end_char="11976" id="token-121-23" morph="none" pos="word" start_char="11974">the</TOKEN>
<TOKEN end_char="11981" id="token-121-24" morph="none" pos="word" start_char="11978">bias</TOKEN>
<TOKEN end_char="11984" id="token-121-25" morph="none" pos="word" start_char="11983">if</TOKEN>
<TOKEN end_char="11989" id="token-121-26" morph="none" pos="word" start_char="11986">it's</TOKEN>
<TOKEN end_char="11994" id="token-121-27" morph="none" pos="word" start_char="11991">bias</TOKEN>
<TOKEN end_char="12002" id="token-121-28" morph="none" pos="word" start_char="11996">creator</TOKEN>
<TOKEN end_char="12003" id="token-121-29" morph="none" pos="punct" start_char="12003">.</TOKEN>
</SEG>
<SEG end_char="12095" id="segment-122" start_char="12007">
<ORIGINAL_TEXT>edit on 12/31/20 by Gothmog because: Not following with off-topic and ignorant statements</ORIGINAL_TEXT>
<TOKEN end_char="12010" id="token-122-0" morph="none" pos="word" start_char="12007">edit</TOKEN>
<TOKEN end_char="12013" id="token-122-1" morph="none" pos="word" start_char="12012">on</TOKEN>
<TOKEN end_char="12022" id="token-122-2" morph="none" pos="unknown" start_char="12015">12/31/20</TOKEN>
<TOKEN end_char="12025" id="token-122-3" morph="none" pos="word" start_char="12024">by</TOKEN>
<TOKEN end_char="12033" id="token-122-4" morph="none" pos="word" start_char="12027">Gothmog</TOKEN>
<TOKEN end_char="12041" id="token-122-5" morph="none" pos="word" start_char="12035">because</TOKEN>
<TOKEN end_char="12042" id="token-122-6" morph="none" pos="punct" start_char="12042">:</TOKEN>
<TOKEN end_char="12046" id="token-122-7" morph="none" pos="word" start_char="12044">Not</TOKEN>
<TOKEN end_char="12056" id="token-122-8" morph="none" pos="word" start_char="12048">following</TOKEN>
<TOKEN end_char="12061" id="token-122-9" morph="none" pos="word" start_char="12058">with</TOKEN>
<TOKEN end_char="12071" id="token-122-10" morph="none" pos="unknown" start_char="12063">off-topic</TOKEN>
<TOKEN end_char="12075" id="token-122-11" morph="none" pos="word" start_char="12073">and</TOKEN>
<TOKEN end_char="12084" id="token-122-12" morph="none" pos="word" start_char="12077">ignorant</TOKEN>
<TOKEN end_char="12095" id="token-122-13" morph="none" pos="word" start_char="12086">statements</TOKEN>
</SEG>
<SEG end_char="12179" id="segment-123" start_char="12099">
<ORIGINAL_TEXT>The case for lab origins is much stronger than zoonotic from all I have reviewed.</ORIGINAL_TEXT>
<TOKEN end_char="12101" id="token-123-0" morph="none" pos="word" start_char="12099">The</TOKEN>
<TOKEN end_char="12106" id="token-123-1" morph="none" pos="word" start_char="12103">case</TOKEN>
<TOKEN end_char="12110" id="token-123-2" morph="none" pos="word" start_char="12108">for</TOKEN>
<TOKEN end_char="12114" id="token-123-3" morph="none" pos="word" start_char="12112">lab</TOKEN>
<TOKEN end_char="12122" id="token-123-4" morph="none" pos="word" start_char="12116">origins</TOKEN>
<TOKEN end_char="12125" id="token-123-5" morph="none" pos="word" start_char="12124">is</TOKEN>
<TOKEN end_char="12130" id="token-123-6" morph="none" pos="word" start_char="12127">much</TOKEN>
<TOKEN end_char="12139" id="token-123-7" morph="none" pos="word" start_char="12132">stronger</TOKEN>
<TOKEN end_char="12144" id="token-123-8" morph="none" pos="word" start_char="12141">than</TOKEN>
<TOKEN end_char="12153" id="token-123-9" morph="none" pos="word" start_char="12146">zoonotic</TOKEN>
<TOKEN end_char="12158" id="token-123-10" morph="none" pos="word" start_char="12155">from</TOKEN>
<TOKEN end_char="12162" id="token-123-11" morph="none" pos="word" start_char="12160">all</TOKEN>
<TOKEN end_char="12164" id="token-123-12" morph="none" pos="word" start_char="12164">I</TOKEN>
<TOKEN end_char="12169" id="token-123-13" morph="none" pos="word" start_char="12166">have</TOKEN>
<TOKEN end_char="12178" id="token-123-14" morph="none" pos="word" start_char="12171">reviewed</TOKEN>
<TOKEN end_char="12179" id="token-123-15" morph="none" pos="punct" start_char="12179">.</TOKEN>
</SEG>
<SEG end_char="12296" id="segment-124" start_char="12182">
<ORIGINAL_TEXT>As being an accidental or intentional release, Event 201 does provide a compelling case for an intentional release.</ORIGINAL_TEXT>
<TOKEN end_char="12183" id="token-124-0" morph="none" pos="word" start_char="12182">As</TOKEN>
<TOKEN end_char="12189" id="token-124-1" morph="none" pos="word" start_char="12185">being</TOKEN>
<TOKEN end_char="12192" id="token-124-2" morph="none" pos="word" start_char="12191">an</TOKEN>
<TOKEN end_char="12203" id="token-124-3" morph="none" pos="word" start_char="12194">accidental</TOKEN>
<TOKEN end_char="12206" id="token-124-4" morph="none" pos="word" start_char="12205">or</TOKEN>
<TOKEN end_char="12218" id="token-124-5" morph="none" pos="word" start_char="12208">intentional</TOKEN>
<TOKEN end_char="12226" id="token-124-6" morph="none" pos="word" start_char="12220">release</TOKEN>
<TOKEN end_char="12227" id="token-124-7" morph="none" pos="punct" start_char="12227">,</TOKEN>
<TOKEN end_char="12233" id="token-124-8" morph="none" pos="word" start_char="12229">Event</TOKEN>
<TOKEN end_char="12237" id="token-124-9" morph="none" pos="word" start_char="12235">201</TOKEN>
<TOKEN end_char="12242" id="token-124-10" morph="none" pos="word" start_char="12239">does</TOKEN>
<TOKEN end_char="12250" id="token-124-11" morph="none" pos="word" start_char="12244">provide</TOKEN>
<TOKEN end_char="12252" id="token-124-12" morph="none" pos="word" start_char="12252">a</TOKEN>
<TOKEN end_char="12263" id="token-124-13" morph="none" pos="word" start_char="12254">compelling</TOKEN>
<TOKEN end_char="12268" id="token-124-14" morph="none" pos="word" start_char="12265">case</TOKEN>
<TOKEN end_char="12272" id="token-124-15" morph="none" pos="word" start_char="12270">for</TOKEN>
<TOKEN end_char="12275" id="token-124-16" morph="none" pos="word" start_char="12274">an</TOKEN>
<TOKEN end_char="12287" id="token-124-17" morph="none" pos="word" start_char="12277">intentional</TOKEN>
<TOKEN end_char="12295" id="token-124-18" morph="none" pos="word" start_char="12289">release</TOKEN>
<TOKEN end_char="12296" id="token-124-19" morph="none" pos="punct" start_char="12296">.</TOKEN>
</SEG>
<SEG end_char="12408" id="segment-125" start_char="12298">
<ORIGINAL_TEXT>Those involved in the pandemic planning stages are still in positions of power and influence in covid response.</ORIGINAL_TEXT>
<TOKEN end_char="12302" id="token-125-0" morph="none" pos="word" start_char="12298">Those</TOKEN>
<TOKEN end_char="12311" id="token-125-1" morph="none" pos="word" start_char="12304">involved</TOKEN>
<TOKEN end_char="12314" id="token-125-2" morph="none" pos="word" start_char="12313">in</TOKEN>
<TOKEN end_char="12318" id="token-125-3" morph="none" pos="word" start_char="12316">the</TOKEN>
<TOKEN end_char="12327" id="token-125-4" morph="none" pos="word" start_char="12320">pandemic</TOKEN>
<TOKEN end_char="12336" id="token-125-5" morph="none" pos="word" start_char="12329">planning</TOKEN>
<TOKEN end_char="12343" id="token-125-6" morph="none" pos="word" start_char="12338">stages</TOKEN>
<TOKEN end_char="12347" id="token-125-7" morph="none" pos="word" start_char="12345">are</TOKEN>
<TOKEN end_char="12353" id="token-125-8" morph="none" pos="word" start_char="12349">still</TOKEN>
<TOKEN end_char="12356" id="token-125-9" morph="none" pos="word" start_char="12355">in</TOKEN>
<TOKEN end_char="12366" id="token-125-10" morph="none" pos="word" start_char="12358">positions</TOKEN>
<TOKEN end_char="12369" id="token-125-11" morph="none" pos="word" start_char="12368">of</TOKEN>
<TOKEN end_char="12375" id="token-125-12" morph="none" pos="word" start_char="12371">power</TOKEN>
<TOKEN end_char="12379" id="token-125-13" morph="none" pos="word" start_char="12377">and</TOKEN>
<TOKEN end_char="12389" id="token-125-14" morph="none" pos="word" start_char="12381">influence</TOKEN>
<TOKEN end_char="12392" id="token-125-15" morph="none" pos="word" start_char="12391">in</TOKEN>
<TOKEN end_char="12398" id="token-125-16" morph="none" pos="word" start_char="12394">covid</TOKEN>
<TOKEN end_char="12407" id="token-125-17" morph="none" pos="word" start_char="12400">response</TOKEN>
<TOKEN end_char="12408" id="token-125-18" morph="none" pos="punct" start_char="12408">.</TOKEN>
</SEG>
<SEG end_char="12508" id="segment-126" start_char="12410">
<ORIGINAL_TEXT>Suppressing treatments while promoting lock downs, masks, social distancing and vaccination policy.</ORIGINAL_TEXT>
<TOKEN end_char="12420" id="token-126-0" morph="none" pos="word" start_char="12410">Suppressing</TOKEN>
<TOKEN end_char="12431" id="token-126-1" morph="none" pos="word" start_char="12422">treatments</TOKEN>
<TOKEN end_char="12437" id="token-126-2" morph="none" pos="word" start_char="12433">while</TOKEN>
<TOKEN end_char="12447" id="token-126-3" morph="none" pos="word" start_char="12439">promoting</TOKEN>
<TOKEN end_char="12452" id="token-126-4" morph="none" pos="word" start_char="12449">lock</TOKEN>
<TOKEN end_char="12458" id="token-126-5" morph="none" pos="word" start_char="12454">downs</TOKEN>
<TOKEN end_char="12459" id="token-126-6" morph="none" pos="punct" start_char="12459">,</TOKEN>
<TOKEN end_char="12465" id="token-126-7" morph="none" pos="word" start_char="12461">masks</TOKEN>
<TOKEN end_char="12466" id="token-126-8" morph="none" pos="punct" start_char="12466">,</TOKEN>
<TOKEN end_char="12473" id="token-126-9" morph="none" pos="word" start_char="12468">social</TOKEN>
<TOKEN end_char="12484" id="token-126-10" morph="none" pos="word" start_char="12475">distancing</TOKEN>
<TOKEN end_char="12488" id="token-126-11" morph="none" pos="word" start_char="12486">and</TOKEN>
<TOKEN end_char="12500" id="token-126-12" morph="none" pos="word" start_char="12490">vaccination</TOKEN>
<TOKEN end_char="12507" id="token-126-13" morph="none" pos="word" start_char="12502">policy</TOKEN>
<TOKEN end_char="12508" id="token-126-14" morph="none" pos="punct" start_char="12508">.</TOKEN>
</SEG>
<SEG end_char="12558" id="segment-127" start_char="12511">
<ORIGINAL_TEXT>It does look like a biological attack from here.</ORIGINAL_TEXT>
<TOKEN end_char="12512" id="token-127-0" morph="none" pos="word" start_char="12511">It</TOKEN>
<TOKEN end_char="12517" id="token-127-1" morph="none" pos="word" start_char="12514">does</TOKEN>
<TOKEN end_char="12522" id="token-127-2" morph="none" pos="word" start_char="12519">look</TOKEN>
<TOKEN end_char="12527" id="token-127-3" morph="none" pos="word" start_char="12524">like</TOKEN>
<TOKEN end_char="12529" id="token-127-4" morph="none" pos="word" start_char="12529">a</TOKEN>
<TOKEN end_char="12540" id="token-127-5" morph="none" pos="word" start_char="12531">biological</TOKEN>
<TOKEN end_char="12547" id="token-127-6" morph="none" pos="word" start_char="12542">attack</TOKEN>
<TOKEN end_char="12552" id="token-127-7" morph="none" pos="word" start_char="12549">from</TOKEN>
<TOKEN end_char="12557" id="token-127-8" morph="none" pos="word" start_char="12554">here</TOKEN>
<TOKEN end_char="12558" id="token-127-9" morph="none" pos="punct" start_char="12558">.</TOKEN>
</SEG>
<SEG end_char="12643" id="segment-128" start_char="12560">
<ORIGINAL_TEXT>Some are motivated by money as the government coffers got pillaged during the panic.</ORIGINAL_TEXT>
<TOKEN end_char="12563" id="token-128-0" morph="none" pos="word" start_char="12560">Some</TOKEN>
<TOKEN end_char="12567" id="token-128-1" morph="none" pos="word" start_char="12565">are</TOKEN>
<TOKEN end_char="12577" id="token-128-2" morph="none" pos="word" start_char="12569">motivated</TOKEN>
<TOKEN end_char="12580" id="token-128-3" morph="none" pos="word" start_char="12579">by</TOKEN>
<TOKEN end_char="12586" id="token-128-4" morph="none" pos="word" start_char="12582">money</TOKEN>
<TOKEN end_char="12589" id="token-128-5" morph="none" pos="word" start_char="12588">as</TOKEN>
<TOKEN end_char="12593" id="token-128-6" morph="none" pos="word" start_char="12591">the</TOKEN>
<TOKEN end_char="12604" id="token-128-7" morph="none" pos="word" start_char="12595">government</TOKEN>
<TOKEN end_char="12612" id="token-128-8" morph="none" pos="word" start_char="12606">coffers</TOKEN>
<TOKEN end_char="12616" id="token-128-9" morph="none" pos="word" start_char="12614">got</TOKEN>
<TOKEN end_char="12625" id="token-128-10" morph="none" pos="word" start_char="12618">pillaged</TOKEN>
<TOKEN end_char="12632" id="token-128-11" morph="none" pos="word" start_char="12627">during</TOKEN>
<TOKEN end_char="12636" id="token-128-12" morph="none" pos="word" start_char="12634">the</TOKEN>
<TOKEN end_char="12642" id="token-128-13" morph="none" pos="word" start_char="12638">panic</TOKEN>
<TOKEN end_char="12643" id="token-128-14" morph="none" pos="punct" start_char="12643">.</TOKEN>
</SEG>
<SEG end_char="12751" id="segment-129" start_char="12645">
<ORIGINAL_TEXT>Most are trying to do the right thing, just a bit lost with all the censorship and misinformation going on.</ORIGINAL_TEXT>
<TOKEN end_char="12648" id="token-129-0" morph="none" pos="word" start_char="12645">Most</TOKEN>
<TOKEN end_char="12652" id="token-129-1" morph="none" pos="word" start_char="12650">are</TOKEN>
<TOKEN end_char="12659" id="token-129-2" morph="none" pos="word" start_char="12654">trying</TOKEN>
<TOKEN end_char="12662" id="token-129-3" morph="none" pos="word" start_char="12661">to</TOKEN>
<TOKEN end_char="12665" id="token-129-4" morph="none" pos="word" start_char="12664">do</TOKEN>
<TOKEN end_char="12669" id="token-129-5" morph="none" pos="word" start_char="12667">the</TOKEN>
<TOKEN end_char="12675" id="token-129-6" morph="none" pos="word" start_char="12671">right</TOKEN>
<TOKEN end_char="12681" id="token-129-7" morph="none" pos="word" start_char="12677">thing</TOKEN>
<TOKEN end_char="12682" id="token-129-8" morph="none" pos="punct" start_char="12682">,</TOKEN>
<TOKEN end_char="12687" id="token-129-9" morph="none" pos="word" start_char="12684">just</TOKEN>
<TOKEN end_char="12689" id="token-129-10" morph="none" pos="word" start_char="12689">a</TOKEN>
<TOKEN end_char="12693" id="token-129-11" morph="none" pos="word" start_char="12691">bit</TOKEN>
<TOKEN end_char="12698" id="token-129-12" morph="none" pos="word" start_char="12695">lost</TOKEN>
<TOKEN end_char="12703" id="token-129-13" morph="none" pos="word" start_char="12700">with</TOKEN>
<TOKEN end_char="12707" id="token-129-14" morph="none" pos="word" start_char="12705">all</TOKEN>
<TOKEN end_char="12711" id="token-129-15" morph="none" pos="word" start_char="12709">the</TOKEN>
<TOKEN end_char="12722" id="token-129-16" morph="none" pos="word" start_char="12713">censorship</TOKEN>
<TOKEN end_char="12726" id="token-129-17" morph="none" pos="word" start_char="12724">and</TOKEN>
<TOKEN end_char="12741" id="token-129-18" morph="none" pos="word" start_char="12728">misinformation</TOKEN>
<TOKEN end_char="12747" id="token-129-19" morph="none" pos="word" start_char="12743">going</TOKEN>
<TOKEN end_char="12750" id="token-129-20" morph="none" pos="word" start_char="12749">on</TOKEN>
<TOKEN end_char="12751" id="token-129-21" morph="none" pos="punct" start_char="12751">.</TOKEN>
</SEG>
<SEG end_char="12795" id="segment-130" start_char="12753">
<ORIGINAL_TEXT>A few are trying to kill of the population.</ORIGINAL_TEXT>
<TOKEN end_char="12753" id="token-130-0" morph="none" pos="word" start_char="12753">A</TOKEN>
<TOKEN end_char="12757" id="token-130-1" morph="none" pos="word" start_char="12755">few</TOKEN>
<TOKEN end_char="12761" id="token-130-2" morph="none" pos="word" start_char="12759">are</TOKEN>
<TOKEN end_char="12768" id="token-130-3" morph="none" pos="word" start_char="12763">trying</TOKEN>
<TOKEN end_char="12771" id="token-130-4" morph="none" pos="word" start_char="12770">to</TOKEN>
<TOKEN end_char="12776" id="token-130-5" morph="none" pos="word" start_char="12773">kill</TOKEN>
<TOKEN end_char="12779" id="token-130-6" morph="none" pos="word" start_char="12778">of</TOKEN>
<TOKEN end_char="12783" id="token-130-7" morph="none" pos="word" start_char="12781">the</TOKEN>
<TOKEN end_char="12794" id="token-130-8" morph="none" pos="word" start_char="12785">population</TOKEN>
<TOKEN end_char="12795" id="token-130-9" morph="none" pos="punct" start_char="12795">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
<LCTL_TEXT lang="eng">
<DOC grammar="none" id="L0C04ATEF" lang="eng" raw_text_char_length="9304" raw_text_md5="b16e7afd7b81e5f922a759473be1124e" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="48" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Antibody Tests For Coronavirus Can Miss The Mark</ORIGINAL_TEXT>
<TOKEN end_char="8" id="token-0-0" morph="none" pos="word" start_char="1">Antibody</TOKEN>
<TOKEN end_char="14" id="token-0-1" morph="none" pos="word" start_char="10">Tests</TOKEN>
<TOKEN end_char="18" id="token-0-2" morph="none" pos="word" start_char="16">For</TOKEN>
<TOKEN end_char="30" id="token-0-3" morph="none" pos="word" start_char="20">Coronavirus</TOKEN>
<TOKEN end_char="34" id="token-0-4" morph="none" pos="word" start_char="32">Can</TOKEN>
<TOKEN end_char="39" id="token-0-5" morph="none" pos="word" start_char="36">Miss</TOKEN>
<TOKEN end_char="43" id="token-0-6" morph="none" pos="word" start_char="41">The</TOKEN>
<TOKEN end_char="48" id="token-0-7" morph="none" pos="word" start_char="45">Mark</TOKEN>
</SEG>
<SEG end_char="204" id="segment-1" start_char="52">
<ORIGINAL_TEXT>Dozens of blood tests are rapidly coming on the market to identify people who have been exposed to the coronavirus by checking for antibodies against it.</ORIGINAL_TEXT>
<TOKEN end_char="57" id="token-1-0" morph="none" pos="word" start_char="52">Dozens</TOKEN>
<TOKEN end_char="60" id="token-1-1" morph="none" pos="word" start_char="59">of</TOKEN>
<TOKEN end_char="66" id="token-1-2" morph="none" pos="word" start_char="62">blood</TOKEN>
<TOKEN end_char="72" id="token-1-3" morph="none" pos="word" start_char="68">tests</TOKEN>
<TOKEN end_char="76" id="token-1-4" morph="none" pos="word" start_char="74">are</TOKEN>
<TOKEN end_char="84" id="token-1-5" morph="none" pos="word" start_char="78">rapidly</TOKEN>
<TOKEN end_char="91" id="token-1-6" morph="none" pos="word" start_char="86">coming</TOKEN>
<TOKEN end_char="94" id="token-1-7" morph="none" pos="word" start_char="93">on</TOKEN>
<TOKEN end_char="98" id="token-1-8" morph="none" pos="word" start_char="96">the</TOKEN>
<TOKEN end_char="105" id="token-1-9" morph="none" pos="word" start_char="100">market</TOKEN>
<TOKEN end_char="108" id="token-1-10" morph="none" pos="word" start_char="107">to</TOKEN>
<TOKEN end_char="117" id="token-1-11" morph="none" pos="word" start_char="110">identify</TOKEN>
<TOKEN end_char="124" id="token-1-12" morph="none" pos="word" start_char="119">people</TOKEN>
<TOKEN end_char="128" id="token-1-13" morph="none" pos="word" start_char="126">who</TOKEN>
<TOKEN end_char="133" id="token-1-14" morph="none" pos="word" start_char="130">have</TOKEN>
<TOKEN end_char="138" id="token-1-15" morph="none" pos="word" start_char="135">been</TOKEN>
<TOKEN end_char="146" id="token-1-16" morph="none" pos="word" start_char="140">exposed</TOKEN>
<TOKEN end_char="149" id="token-1-17" morph="none" pos="word" start_char="148">to</TOKEN>
<TOKEN end_char="153" id="token-1-18" morph="none" pos="word" start_char="151">the</TOKEN>
<TOKEN end_char="165" id="token-1-19" morph="none" pos="word" start_char="155">coronavirus</TOKEN>
<TOKEN end_char="168" id="token-1-20" morph="none" pos="word" start_char="167">by</TOKEN>
<TOKEN end_char="177" id="token-1-21" morph="none" pos="word" start_char="170">checking</TOKEN>
<TOKEN end_char="181" id="token-1-22" morph="none" pos="word" start_char="179">for</TOKEN>
<TOKEN end_char="192" id="token-1-23" morph="none" pos="word" start_char="183">antibodies</TOKEN>
<TOKEN end_char="200" id="token-1-24" morph="none" pos="word" start_char="194">against</TOKEN>
<TOKEN end_char="203" id="token-1-25" morph="none" pos="word" start_char="202">it</TOKEN>
<TOKEN end_char="204" id="token-1-26" morph="none" pos="punct" start_char="204">.</TOKEN>
</SEG>
<SEG end_char="407" id="segment-2" start_char="207">
<ORIGINAL_TEXT>The Food and Drug Administration doesn't set standards for these kinds of tests, but even those that meet the government's informal standard may produce many false answers and provide false assurances.</ORIGINAL_TEXT>
<TOKEN end_char="209" id="token-2-0" morph="none" pos="word" start_char="207">The</TOKEN>
<TOKEN end_char="214" id="token-2-1" morph="none" pos="word" start_char="211">Food</TOKEN>
<TOKEN end_char="218" id="token-2-2" morph="none" pos="word" start_char="216">and</TOKEN>
<TOKEN end_char="223" id="token-2-3" morph="none" pos="word" start_char="220">Drug</TOKEN>
<TOKEN end_char="238" id="token-2-4" morph="none" pos="word" start_char="225">Administration</TOKEN>
<TOKEN end_char="246" id="token-2-5" morph="none" pos="word" start_char="240">doesn't</TOKEN>
<TOKEN end_char="250" id="token-2-6" morph="none" pos="word" start_char="248">set</TOKEN>
<TOKEN end_char="260" id="token-2-7" morph="none" pos="word" start_char="252">standards</TOKEN>
<TOKEN end_char="264" id="token-2-8" morph="none" pos="word" start_char="262">for</TOKEN>
<TOKEN end_char="270" id="token-2-9" morph="none" pos="word" start_char="266">these</TOKEN>
<TOKEN end_char="276" id="token-2-10" morph="none" pos="word" start_char="272">kinds</TOKEN>
<TOKEN end_char="279" id="token-2-11" morph="none" pos="word" start_char="278">of</TOKEN>
<TOKEN end_char="285" id="token-2-12" morph="none" pos="word" start_char="281">tests</TOKEN>
<TOKEN end_char="286" id="token-2-13" morph="none" pos="punct" start_char="286">,</TOKEN>
<TOKEN end_char="290" id="token-2-14" morph="none" pos="word" start_char="288">but</TOKEN>
<TOKEN end_char="295" id="token-2-15" morph="none" pos="word" start_char="292">even</TOKEN>
<TOKEN end_char="301" id="token-2-16" morph="none" pos="word" start_char="297">those</TOKEN>
<TOKEN end_char="306" id="token-2-17" morph="none" pos="word" start_char="303">that</TOKEN>
<TOKEN end_char="311" id="token-2-18" morph="none" pos="word" start_char="308">meet</TOKEN>
<TOKEN end_char="315" id="token-2-19" morph="none" pos="word" start_char="313">the</TOKEN>
<TOKEN end_char="328" id="token-2-20" morph="none" pos="word" start_char="317">government's</TOKEN>
<TOKEN end_char="337" id="token-2-21" morph="none" pos="word" start_char="330">informal</TOKEN>
<TOKEN end_char="346" id="token-2-22" morph="none" pos="word" start_char="339">standard</TOKEN>
<TOKEN end_char="350" id="token-2-23" morph="none" pos="word" start_char="348">may</TOKEN>
<TOKEN end_char="358" id="token-2-24" morph="none" pos="word" start_char="352">produce</TOKEN>
<TOKEN end_char="363" id="token-2-25" morph="none" pos="word" start_char="360">many</TOKEN>
<TOKEN end_char="369" id="token-2-26" morph="none" pos="word" start_char="365">false</TOKEN>
<TOKEN end_char="377" id="token-2-27" morph="none" pos="word" start_char="371">answers</TOKEN>
<TOKEN end_char="381" id="token-2-28" morph="none" pos="word" start_char="379">and</TOKEN>
<TOKEN end_char="389" id="token-2-29" morph="none" pos="word" start_char="383">provide</TOKEN>
<TOKEN end_char="395" id="token-2-30" morph="none" pos="word" start_char="391">false</TOKEN>
<TOKEN end_char="406" id="token-2-31" morph="none" pos="word" start_char="397">assurances</TOKEN>
<TOKEN end_char="407" id="token-2-32" morph="none" pos="punct" start_char="407">.</TOKEN>
</SEG>
<SEG end_char="563" id="segment-3" start_char="409">
<ORIGINAL_TEXT>The imperfect results could be a big disappointment to people who are looking toward these tests to help them return to something resembling a normal life.</ORIGINAL_TEXT>
<TOKEN end_char="411" id="token-3-0" morph="none" pos="word" start_char="409">The</TOKEN>
<TOKEN end_char="421" id="token-3-1" morph="none" pos="word" start_char="413">imperfect</TOKEN>
<TOKEN end_char="429" id="token-3-2" morph="none" pos="word" start_char="423">results</TOKEN>
<TOKEN end_char="435" id="token-3-3" morph="none" pos="word" start_char="431">could</TOKEN>
<TOKEN end_char="438" id="token-3-4" morph="none" pos="word" start_char="437">be</TOKEN>
<TOKEN end_char="440" id="token-3-5" morph="none" pos="word" start_char="440">a</TOKEN>
<TOKEN end_char="444" id="token-3-6" morph="none" pos="word" start_char="442">big</TOKEN>
<TOKEN end_char="459" id="token-3-7" morph="none" pos="word" start_char="446">disappointment</TOKEN>
<TOKEN end_char="462" id="token-3-8" morph="none" pos="word" start_char="461">to</TOKEN>
<TOKEN end_char="469" id="token-3-9" morph="none" pos="word" start_char="464">people</TOKEN>
<TOKEN end_char="473" id="token-3-10" morph="none" pos="word" start_char="471">who</TOKEN>
<TOKEN end_char="477" id="token-3-11" morph="none" pos="word" start_char="475">are</TOKEN>
<TOKEN end_char="485" id="token-3-12" morph="none" pos="word" start_char="479">looking</TOKEN>
<TOKEN end_char="492" id="token-3-13" morph="none" pos="word" start_char="487">toward</TOKEN>
<TOKEN end_char="498" id="token-3-14" morph="none" pos="word" start_char="494">these</TOKEN>
<TOKEN end_char="504" id="token-3-15" morph="none" pos="word" start_char="500">tests</TOKEN>
<TOKEN end_char="507" id="token-3-16" morph="none" pos="word" start_char="506">to</TOKEN>
<TOKEN end_char="512" id="token-3-17" morph="none" pos="word" start_char="509">help</TOKEN>
<TOKEN end_char="517" id="token-3-18" morph="none" pos="word" start_char="514">them</TOKEN>
<TOKEN end_char="524" id="token-3-19" morph="none" pos="word" start_char="519">return</TOKEN>
<TOKEN end_char="527" id="token-3-20" morph="none" pos="word" start_char="526">to</TOKEN>
<TOKEN end_char="537" id="token-3-21" morph="none" pos="word" start_char="529">something</TOKEN>
<TOKEN end_char="548" id="token-3-22" morph="none" pos="word" start_char="539">resembling</TOKEN>
<TOKEN end_char="550" id="token-3-23" morph="none" pos="word" start_char="550">a</TOKEN>
<TOKEN end_char="557" id="token-3-24" morph="none" pos="word" start_char="552">normal</TOKEN>
<TOKEN end_char="562" id="token-3-25" morph="none" pos="word" start_char="559">life</TOKEN>
<TOKEN end_char="563" id="token-3-26" morph="none" pos="punct" start_char="563">.</TOKEN>
</SEG>
<SEG end_char="682" id="segment-4" start_char="566">
<ORIGINAL_TEXT>First of all, it's not clear whether someone who has antibodies to the coronavirus in their blood is actually immune.</ORIGINAL_TEXT>
<TOKEN end_char="570" id="token-4-0" morph="none" pos="word" start_char="566">First</TOKEN>
<TOKEN end_char="573" id="token-4-1" morph="none" pos="word" start_char="572">of</TOKEN>
<TOKEN end_char="577" id="token-4-2" morph="none" pos="word" start_char="575">all</TOKEN>
<TOKEN end_char="578" id="token-4-3" morph="none" pos="punct" start_char="578">,</TOKEN>
<TOKEN end_char="583" id="token-4-4" morph="none" pos="word" start_char="580">it's</TOKEN>
<TOKEN end_char="587" id="token-4-5" morph="none" pos="word" start_char="585">not</TOKEN>
<TOKEN end_char="593" id="token-4-6" morph="none" pos="word" start_char="589">clear</TOKEN>
<TOKEN end_char="601" id="token-4-7" morph="none" pos="word" start_char="595">whether</TOKEN>
<TOKEN end_char="609" id="token-4-8" morph="none" pos="word" start_char="603">someone</TOKEN>
<TOKEN end_char="613" id="token-4-9" morph="none" pos="word" start_char="611">who</TOKEN>
<TOKEN end_char="617" id="token-4-10" morph="none" pos="word" start_char="615">has</TOKEN>
<TOKEN end_char="628" id="token-4-11" morph="none" pos="word" start_char="619">antibodies</TOKEN>
<TOKEN end_char="631" id="token-4-12" morph="none" pos="word" start_char="630">to</TOKEN>
<TOKEN end_char="635" id="token-4-13" morph="none" pos="word" start_char="633">the</TOKEN>
<TOKEN end_char="647" id="token-4-14" morph="none" pos="word" start_char="637">coronavirus</TOKEN>
<TOKEN end_char="650" id="token-4-15" morph="none" pos="word" start_char="649">in</TOKEN>
<TOKEN end_char="656" id="token-4-16" morph="none" pos="word" start_char="652">their</TOKEN>
<TOKEN end_char="662" id="token-4-17" morph="none" pos="word" start_char="658">blood</TOKEN>
<TOKEN end_char="665" id="token-4-18" morph="none" pos="word" start_char="664">is</TOKEN>
<TOKEN end_char="674" id="token-4-19" morph="none" pos="word" start_char="667">actually</TOKEN>
<TOKEN end_char="681" id="token-4-20" morph="none" pos="word" start_char="676">immune</TOKEN>
<TOKEN end_char="682" id="token-4-21" morph="none" pos="punct" start_char="682">.</TOKEN>
</SEG>
<SEG end_char="752" id="segment-5" start_char="684">
<ORIGINAL_TEXT>Your body produces these antibodies within about a week of infection.</ORIGINAL_TEXT>
<TOKEN end_char="687" id="token-5-0" morph="none" pos="word" start_char="684">Your</TOKEN>
<TOKEN end_char="692" id="token-5-1" morph="none" pos="word" start_char="689">body</TOKEN>
<TOKEN end_char="701" id="token-5-2" morph="none" pos="word" start_char="694">produces</TOKEN>
<TOKEN end_char="707" id="token-5-3" morph="none" pos="word" start_char="703">these</TOKEN>
<TOKEN end_char="718" id="token-5-4" morph="none" pos="word" start_char="709">antibodies</TOKEN>
<TOKEN end_char="725" id="token-5-5" morph="none" pos="word" start_char="720">within</TOKEN>
<TOKEN end_char="731" id="token-5-6" morph="none" pos="word" start_char="727">about</TOKEN>
<TOKEN end_char="733" id="token-5-7" morph="none" pos="word" start_char="733">a</TOKEN>
<TOKEN end_char="738" id="token-5-8" morph="none" pos="word" start_char="735">week</TOKEN>
<TOKEN end_char="741" id="token-5-9" morph="none" pos="word" start_char="740">of</TOKEN>
<TOKEN end_char="751" id="token-5-10" morph="none" pos="word" start_char="743">infection</TOKEN>
<TOKEN end_char="752" id="token-5-11" morph="none" pos="punct" start_char="752">.</TOKEN>
</SEG>
<SEG end_char="881" id="segment-6" start_char="755">
<ORIGINAL_TEXT>In many other diseases, people do have a period of immunity after they have been exposed to a microbe and recover from illness.</ORIGINAL_TEXT>
<TOKEN end_char="756" id="token-6-0" morph="none" pos="word" start_char="755">In</TOKEN>
<TOKEN end_char="761" id="token-6-1" morph="none" pos="word" start_char="758">many</TOKEN>
<TOKEN end_char="767" id="token-6-2" morph="none" pos="word" start_char="763">other</TOKEN>
<TOKEN end_char="776" id="token-6-3" morph="none" pos="word" start_char="769">diseases</TOKEN>
<TOKEN end_char="777" id="token-6-4" morph="none" pos="punct" start_char="777">,</TOKEN>
<TOKEN end_char="784" id="token-6-5" morph="none" pos="word" start_char="779">people</TOKEN>
<TOKEN end_char="787" id="token-6-6" morph="none" pos="word" start_char="786">do</TOKEN>
<TOKEN end_char="792" id="token-6-7" morph="none" pos="word" start_char="789">have</TOKEN>
<TOKEN end_char="794" id="token-6-8" morph="none" pos="word" start_char="794">a</TOKEN>
<TOKEN end_char="801" id="token-6-9" morph="none" pos="word" start_char="796">period</TOKEN>
<TOKEN end_char="804" id="token-6-10" morph="none" pos="word" start_char="803">of</TOKEN>
<TOKEN end_char="813" id="token-6-11" morph="none" pos="word" start_char="806">immunity</TOKEN>
<TOKEN end_char="819" id="token-6-12" morph="none" pos="word" start_char="815">after</TOKEN>
<TOKEN end_char="824" id="token-6-13" morph="none" pos="word" start_char="821">they</TOKEN>
<TOKEN end_char="829" id="token-6-14" morph="none" pos="word" start_char="826">have</TOKEN>
<TOKEN end_char="834" id="token-6-15" morph="none" pos="word" start_char="831">been</TOKEN>
<TOKEN end_char="842" id="token-6-16" morph="none" pos="word" start_char="836">exposed</TOKEN>
<TOKEN end_char="845" id="token-6-17" morph="none" pos="word" start_char="844">to</TOKEN>
<TOKEN end_char="847" id="token-6-18" morph="none" pos="word" start_char="847">a</TOKEN>
<TOKEN end_char="855" id="token-6-19" morph="none" pos="word" start_char="849">microbe</TOKEN>
<TOKEN end_char="859" id="token-6-20" morph="none" pos="word" start_char="857">and</TOKEN>
<TOKEN end_char="867" id="token-6-21" morph="none" pos="word" start_char="861">recover</TOKEN>
<TOKEN end_char="872" id="token-6-22" morph="none" pos="word" start_char="869">from</TOKEN>
<TOKEN end_char="880" id="token-6-23" morph="none" pos="word" start_char="874">illness</TOKEN>
<TOKEN end_char="881" id="token-6-24" morph="none" pos="punct" start_char="881">.</TOKEN>
</SEG>
<SEG end_char="942" id="segment-7" start_char="883">
<ORIGINAL_TEXT>But that has not been demonstrated yet with the coronavirus.</ORIGINAL_TEXT>
<TOKEN end_char="885" id="token-7-0" morph="none" pos="word" start_char="883">But</TOKEN>
<TOKEN end_char="890" id="token-7-1" morph="none" pos="word" start_char="887">that</TOKEN>
<TOKEN end_char="894" id="token-7-2" morph="none" pos="word" start_char="892">has</TOKEN>
<TOKEN end_char="898" id="token-7-3" morph="none" pos="word" start_char="896">not</TOKEN>
<TOKEN end_char="903" id="token-7-4" morph="none" pos="word" start_char="900">been</TOKEN>
<TOKEN end_char="916" id="token-7-5" morph="none" pos="word" start_char="905">demonstrated</TOKEN>
<TOKEN end_char="920" id="token-7-6" morph="none" pos="word" start_char="918">yet</TOKEN>
<TOKEN end_char="925" id="token-7-7" morph="none" pos="word" start_char="922">with</TOKEN>
<TOKEN end_char="929" id="token-7-8" morph="none" pos="word" start_char="927">the</TOKEN>
<TOKEN end_char="941" id="token-7-9" morph="none" pos="word" start_char="931">coronavirus</TOKEN>
<TOKEN end_char="942" id="token-7-10" morph="none" pos="punct" start_char="942">.</TOKEN>
</SEG>
<SEG end_char="1034" id="segment-8" start_char="945">
<ORIGINAL_TEXT>Another problem is that test results are wrong much more frequently than you might expect.</ORIGINAL_TEXT>
<TOKEN end_char="951" id="token-8-0" morph="none" pos="word" start_char="945">Another</TOKEN>
<TOKEN end_char="959" id="token-8-1" morph="none" pos="word" start_char="953">problem</TOKEN>
<TOKEN end_char="962" id="token-8-2" morph="none" pos="word" start_char="961">is</TOKEN>
<TOKEN end_char="967" id="token-8-3" morph="none" pos="word" start_char="964">that</TOKEN>
<TOKEN end_char="972" id="token-8-4" morph="none" pos="word" start_char="969">test</TOKEN>
<TOKEN end_char="980" id="token-8-5" morph="none" pos="word" start_char="974">results</TOKEN>
<TOKEN end_char="984" id="token-8-6" morph="none" pos="word" start_char="982">are</TOKEN>
<TOKEN end_char="990" id="token-8-7" morph="none" pos="word" start_char="986">wrong</TOKEN>
<TOKEN end_char="995" id="token-8-8" morph="none" pos="word" start_char="992">much</TOKEN>
<TOKEN end_char="1000" id="token-8-9" morph="none" pos="word" start_char="997">more</TOKEN>
<TOKEN end_char="1011" id="token-8-10" morph="none" pos="word" start_char="1002">frequently</TOKEN>
<TOKEN end_char="1016" id="token-8-11" morph="none" pos="word" start_char="1013">than</TOKEN>
<TOKEN end_char="1020" id="token-8-12" morph="none" pos="word" start_char="1018">you</TOKEN>
<TOKEN end_char="1026" id="token-8-13" morph="none" pos="word" start_char="1022">might</TOKEN>
<TOKEN end_char="1033" id="token-8-14" morph="none" pos="word" start_char="1028">expect</TOKEN>
<TOKEN end_char="1034" id="token-8-15" morph="none" pos="punct" start_char="1034">.</TOKEN>
</SEG>
<SEG end_char="1160" id="segment-9" start_char="1036">
<ORIGINAL_TEXT>While tests may truthfully say they are more than 90% accurate, in practical use they can often perform far below that level.</ORIGINAL_TEXT>
<TOKEN end_char="1040" id="token-9-0" morph="none" pos="word" start_char="1036">While</TOKEN>
<TOKEN end_char="1046" id="token-9-1" morph="none" pos="word" start_char="1042">tests</TOKEN>
<TOKEN end_char="1050" id="token-9-2" morph="none" pos="word" start_char="1048">may</TOKEN>
<TOKEN end_char="1061" id="token-9-3" morph="none" pos="word" start_char="1052">truthfully</TOKEN>
<TOKEN end_char="1065" id="token-9-4" morph="none" pos="word" start_char="1063">say</TOKEN>
<TOKEN end_char="1070" id="token-9-5" morph="none" pos="word" start_char="1067">they</TOKEN>
<TOKEN end_char="1074" id="token-9-6" morph="none" pos="word" start_char="1072">are</TOKEN>
<TOKEN end_char="1079" id="token-9-7" morph="none" pos="word" start_char="1076">more</TOKEN>
<TOKEN end_char="1084" id="token-9-8" morph="none" pos="word" start_char="1081">than</TOKEN>
<TOKEN end_char="1087" id="token-9-9" morph="none" pos="word" start_char="1086">90</TOKEN>
<TOKEN end_char="1088" id="token-9-10" morph="none" pos="punct" start_char="1088">%</TOKEN>
<TOKEN end_char="1097" id="token-9-11" morph="none" pos="word" start_char="1090">accurate</TOKEN>
<TOKEN end_char="1098" id="token-9-12" morph="none" pos="punct" start_char="1098">,</TOKEN>
<TOKEN end_char="1101" id="token-9-13" morph="none" pos="word" start_char="1100">in</TOKEN>
<TOKEN end_char="1111" id="token-9-14" morph="none" pos="word" start_char="1103">practical</TOKEN>
<TOKEN end_char="1115" id="token-9-15" morph="none" pos="word" start_char="1113">use</TOKEN>
<TOKEN end_char="1120" id="token-9-16" morph="none" pos="word" start_char="1117">they</TOKEN>
<TOKEN end_char="1124" id="token-9-17" morph="none" pos="word" start_char="1122">can</TOKEN>
<TOKEN end_char="1130" id="token-9-18" morph="none" pos="word" start_char="1126">often</TOKEN>
<TOKEN end_char="1138" id="token-9-19" morph="none" pos="word" start_char="1132">perform</TOKEN>
<TOKEN end_char="1142" id="token-9-20" morph="none" pos="word" start_char="1140">far</TOKEN>
<TOKEN end_char="1148" id="token-9-21" morph="none" pos="word" start_char="1144">below</TOKEN>
<TOKEN end_char="1153" id="token-9-22" morph="none" pos="word" start_char="1150">that</TOKEN>
<TOKEN end_char="1159" id="token-9-23" morph="none" pos="word" start_char="1155">level</TOKEN>
<TOKEN end_char="1160" id="token-9-24" morph="none" pos="punct" start_char="1160">.</TOKEN>
</SEG>
<SEG end_char="1250" id="segment-10" start_char="1163">
<ORIGINAL_TEXT>Deborah VanderGaast of Tipton, Iowa, would love to know the results of an antibody test.</ORIGINAL_TEXT>
<TOKEN end_char="1169" id="token-10-0" morph="none" pos="word" start_char="1163">Deborah</TOKEN>
<TOKEN end_char="1181" id="token-10-1" morph="none" pos="word" start_char="1171">VanderGaast</TOKEN>
<TOKEN end_char="1184" id="token-10-2" morph="none" pos="word" start_char="1183">of</TOKEN>
<TOKEN end_char="1191" id="token-10-3" morph="none" pos="word" start_char="1186">Tipton</TOKEN>
<TOKEN end_char="1192" id="token-10-4" morph="none" pos="punct" start_char="1192">,</TOKEN>
<TOKEN end_char="1197" id="token-10-5" morph="none" pos="word" start_char="1194">Iowa</TOKEN>
<TOKEN end_char="1198" id="token-10-6" morph="none" pos="punct" start_char="1198">,</TOKEN>
<TOKEN end_char="1204" id="token-10-7" morph="none" pos="word" start_char="1200">would</TOKEN>
<TOKEN end_char="1209" id="token-10-8" morph="none" pos="word" start_char="1206">love</TOKEN>
<TOKEN end_char="1212" id="token-10-9" morph="none" pos="word" start_char="1211">to</TOKEN>
<TOKEN end_char="1217" id="token-10-10" morph="none" pos="word" start_char="1214">know</TOKEN>
<TOKEN end_char="1221" id="token-10-11" morph="none" pos="word" start_char="1219">the</TOKEN>
<TOKEN end_char="1229" id="token-10-12" morph="none" pos="word" start_char="1223">results</TOKEN>
<TOKEN end_char="1232" id="token-10-13" morph="none" pos="word" start_char="1231">of</TOKEN>
<TOKEN end_char="1235" id="token-10-14" morph="none" pos="word" start_char="1234">an</TOKEN>
<TOKEN end_char="1244" id="token-10-15" morph="none" pos="word" start_char="1237">antibody</TOKEN>
<TOKEN end_char="1249" id="token-10-16" morph="none" pos="word" start_char="1246">test</TOKEN>
<TOKEN end_char="1250" id="token-10-17" morph="none" pos="punct" start_char="1250">.</TOKEN>
</SEG>
<SEG end_char="1340" id="segment-11" start_char="1253">
<ORIGINAL_TEXT>"What if we're already immune and we just don't know it and we don't have to be scared?"</ORIGINAL_TEXT>
<TOKEN end_char="1253" id="token-11-0" morph="none" pos="punct" start_char="1253">"</TOKEN>
<TOKEN end_char="1257" id="token-11-1" morph="none" pos="word" start_char="1254">What</TOKEN>
<TOKEN end_char="1260" id="token-11-2" morph="none" pos="word" start_char="1259">if</TOKEN>
<TOKEN end_char="1266" id="token-11-3" morph="none" pos="word" start_char="1262">we're</TOKEN>
<TOKEN end_char="1274" id="token-11-4" morph="none" pos="word" start_char="1268">already</TOKEN>
<TOKEN end_char="1281" id="token-11-5" morph="none" pos="word" start_char="1276">immune</TOKEN>
<TOKEN end_char="1285" id="token-11-6" morph="none" pos="word" start_char="1283">and</TOKEN>
<TOKEN end_char="1288" id="token-11-7" morph="none" pos="word" start_char="1287">we</TOKEN>
<TOKEN end_char="1293" id="token-11-8" morph="none" pos="word" start_char="1290">just</TOKEN>
<TOKEN end_char="1299" id="token-11-9" morph="none" pos="word" start_char="1295">don't</TOKEN>
<TOKEN end_char="1304" id="token-11-10" morph="none" pos="word" start_char="1301">know</TOKEN>
<TOKEN end_char="1307" id="token-11-11" morph="none" pos="word" start_char="1306">it</TOKEN>
<TOKEN end_char="1311" id="token-11-12" morph="none" pos="word" start_char="1309">and</TOKEN>
<TOKEN end_char="1314" id="token-11-13" morph="none" pos="word" start_char="1313">we</TOKEN>
<TOKEN end_char="1320" id="token-11-14" morph="none" pos="word" start_char="1316">don't</TOKEN>
<TOKEN end_char="1325" id="token-11-15" morph="none" pos="word" start_char="1322">have</TOKEN>
<TOKEN end_char="1328" id="token-11-16" morph="none" pos="word" start_char="1327">to</TOKEN>
<TOKEN end_char="1331" id="token-11-17" morph="none" pos="word" start_char="1330">be</TOKEN>
<TOKEN end_char="1338" id="token-11-18" morph="none" pos="word" start_char="1333">scared</TOKEN>
<TOKEN end_char="1340" id="token-11-19" morph="none" pos="punct" start_char="1339">?"</TOKEN>
</SEG>
<SEG end_char="1350" id="segment-12" start_char="1342">
<ORIGINAL_TEXT>she asks.</ORIGINAL_TEXT>
<TOKEN end_char="1344" id="token-12-0" morph="none" pos="word" start_char="1342">she</TOKEN>
<TOKEN end_char="1349" id="token-12-1" morph="none" pos="word" start_char="1346">asks</TOKEN>
<TOKEN end_char="1350" id="token-12-2" morph="none" pos="punct" start_char="1350">.</TOKEN>
</SEG>
<SEG end_char="1446" id="segment-13" start_char="1352">
<ORIGINAL_TEXT>VanderGaast runs a day care center for children with developmental and behavioral disabilities.</ORIGINAL_TEXT>
<TOKEN end_char="1362" id="token-13-0" morph="none" pos="word" start_char="1352">VanderGaast</TOKEN>
<TOKEN end_char="1367" id="token-13-1" morph="none" pos="word" start_char="1364">runs</TOKEN>
<TOKEN end_char="1369" id="token-13-2" morph="none" pos="word" start_char="1369">a</TOKEN>
<TOKEN end_char="1373" id="token-13-3" morph="none" pos="word" start_char="1371">day</TOKEN>
<TOKEN end_char="1378" id="token-13-4" morph="none" pos="word" start_char="1375">care</TOKEN>
<TOKEN end_char="1385" id="token-13-5" morph="none" pos="word" start_char="1380">center</TOKEN>
<TOKEN end_char="1389" id="token-13-6" morph="none" pos="word" start_char="1387">for</TOKEN>
<TOKEN end_char="1398" id="token-13-7" morph="none" pos="word" start_char="1391">children</TOKEN>
<TOKEN end_char="1403" id="token-13-8" morph="none" pos="word" start_char="1400">with</TOKEN>
<TOKEN end_char="1417" id="token-13-9" morph="none" pos="word" start_char="1405">developmental</TOKEN>
<TOKEN end_char="1421" id="token-13-10" morph="none" pos="word" start_char="1419">and</TOKEN>
<TOKEN end_char="1432" id="token-13-11" morph="none" pos="word" start_char="1423">behavioral</TOKEN>
<TOKEN end_char="1445" id="token-13-12" morph="none" pos="word" start_char="1434">disabilities</TOKEN>
<TOKEN end_char="1446" id="token-13-13" morph="none" pos="punct" start_char="1446">.</TOKEN>
</SEG>
<SEG end_char="1489" id="segment-14" start_char="1448">
<ORIGINAL_TEXT>They're a lot like little kids everywhere.</ORIGINAL_TEXT>
<TOKEN end_char="1454" id="token-14-0" morph="none" pos="word" start_char="1448">They're</TOKEN>
<TOKEN end_char="1456" id="token-14-1" morph="none" pos="word" start_char="1456">a</TOKEN>
<TOKEN end_char="1460" id="token-14-2" morph="none" pos="word" start_char="1458">lot</TOKEN>
<TOKEN end_char="1465" id="token-14-3" morph="none" pos="word" start_char="1462">like</TOKEN>
<TOKEN end_char="1472" id="token-14-4" morph="none" pos="word" start_char="1467">little</TOKEN>
<TOKEN end_char="1477" id="token-14-5" morph="none" pos="word" start_char="1474">kids</TOKEN>
<TOKEN end_char="1488" id="token-14-6" morph="none" pos="word" start_char="1479">everywhere</TOKEN>
<TOKEN end_char="1489" id="token-14-7" morph="none" pos="punct" start_char="1489">.</TOKEN>
</SEG>
<SEG end_char="1611" id="segment-15" start_char="1492">
<ORIGINAL_TEXT>"We laugh about the sanitizing everything, because, it's just reinfected just two seconds later," she says with a laugh.</ORIGINAL_TEXT>
<TOKEN end_char="1492" id="token-15-0" morph="none" pos="punct" start_char="1492">"</TOKEN>
<TOKEN end_char="1494" id="token-15-1" morph="none" pos="word" start_char="1493">We</TOKEN>
<TOKEN end_char="1500" id="token-15-2" morph="none" pos="word" start_char="1496">laugh</TOKEN>
<TOKEN end_char="1506" id="token-15-3" morph="none" pos="word" start_char="1502">about</TOKEN>
<TOKEN end_char="1510" id="token-15-4" morph="none" pos="word" start_char="1508">the</TOKEN>
<TOKEN end_char="1521" id="token-15-5" morph="none" pos="word" start_char="1512">sanitizing</TOKEN>
<TOKEN end_char="1532" id="token-15-6" morph="none" pos="word" start_char="1523">everything</TOKEN>
<TOKEN end_char="1533" id="token-15-7" morph="none" pos="punct" start_char="1533">,</TOKEN>
<TOKEN end_char="1541" id="token-15-8" morph="none" pos="word" start_char="1535">because</TOKEN>
<TOKEN end_char="1542" id="token-15-9" morph="none" pos="punct" start_char="1542">,</TOKEN>
<TOKEN end_char="1547" id="token-15-10" morph="none" pos="word" start_char="1544">it's</TOKEN>
<TOKEN end_char="1552" id="token-15-11" morph="none" pos="word" start_char="1549">just</TOKEN>
<TOKEN end_char="1563" id="token-15-12" morph="none" pos="word" start_char="1554">reinfected</TOKEN>
<TOKEN end_char="1568" id="token-15-13" morph="none" pos="word" start_char="1565">just</TOKEN>
<TOKEN end_char="1572" id="token-15-14" morph="none" pos="word" start_char="1570">two</TOKEN>
<TOKEN end_char="1580" id="token-15-15" morph="none" pos="word" start_char="1574">seconds</TOKEN>
<TOKEN end_char="1586" id="token-15-16" morph="none" pos="word" start_char="1582">later</TOKEN>
<TOKEN end_char="1588" id="token-15-17" morph="none" pos="punct" start_char="1587">,"</TOKEN>
<TOKEN end_char="1592" id="token-15-18" morph="none" pos="word" start_char="1590">she</TOKEN>
<TOKEN end_char="1597" id="token-15-19" morph="none" pos="word" start_char="1594">says</TOKEN>
<TOKEN end_char="1602" id="token-15-20" morph="none" pos="word" start_char="1599">with</TOKEN>
<TOKEN end_char="1604" id="token-15-21" morph="none" pos="word" start_char="1604">a</TOKEN>
<TOKEN end_char="1610" id="token-15-22" morph="none" pos="word" start_char="1606">laugh</TOKEN>
<TOKEN end_char="1611" id="token-15-23" morph="none" pos="punct" start_char="1611">.</TOKEN>
</SEG>
<SEG end_char="1756" id="segment-16" start_char="1614">
<ORIGINAL_TEXT>VanderGaast is eagerly awaiting the rollout of a blood test that would be able to tell if she and her staff have antibodies to the coronavirus.</ORIGINAL_TEXT>
<TOKEN end_char="1624" id="token-16-0" morph="none" pos="word" start_char="1614">VanderGaast</TOKEN>
<TOKEN end_char="1627" id="token-16-1" morph="none" pos="word" start_char="1626">is</TOKEN>
<TOKEN end_char="1635" id="token-16-2" morph="none" pos="word" start_char="1629">eagerly</TOKEN>
<TOKEN end_char="1644" id="token-16-3" morph="none" pos="word" start_char="1637">awaiting</TOKEN>
<TOKEN end_char="1648" id="token-16-4" morph="none" pos="word" start_char="1646">the</TOKEN>
<TOKEN end_char="1656" id="token-16-5" morph="none" pos="word" start_char="1650">rollout</TOKEN>
<TOKEN end_char="1659" id="token-16-6" morph="none" pos="word" start_char="1658">of</TOKEN>
<TOKEN end_char="1661" id="token-16-7" morph="none" pos="word" start_char="1661">a</TOKEN>
<TOKEN end_char="1667" id="token-16-8" morph="none" pos="word" start_char="1663">blood</TOKEN>
<TOKEN end_char="1672" id="token-16-9" morph="none" pos="word" start_char="1669">test</TOKEN>
<TOKEN end_char="1677" id="token-16-10" morph="none" pos="word" start_char="1674">that</TOKEN>
<TOKEN end_char="1683" id="token-16-11" morph="none" pos="word" start_char="1679">would</TOKEN>
<TOKEN end_char="1686" id="token-16-12" morph="none" pos="word" start_char="1685">be</TOKEN>
<TOKEN end_char="1691" id="token-16-13" morph="none" pos="word" start_char="1688">able</TOKEN>
<TOKEN end_char="1694" id="token-16-14" morph="none" pos="word" start_char="1693">to</TOKEN>
<TOKEN end_char="1699" id="token-16-15" morph="none" pos="word" start_char="1696">tell</TOKEN>
<TOKEN end_char="1702" id="token-16-16" morph="none" pos="word" start_char="1701">if</TOKEN>
<TOKEN end_char="1706" id="token-16-17" morph="none" pos="word" start_char="1704">she</TOKEN>
<TOKEN end_char="1710" id="token-16-18" morph="none" pos="word" start_char="1708">and</TOKEN>
<TOKEN end_char="1714" id="token-16-19" morph="none" pos="word" start_char="1712">her</TOKEN>
<TOKEN end_char="1720" id="token-16-20" morph="none" pos="word" start_char="1716">staff</TOKEN>
<TOKEN end_char="1725" id="token-16-21" morph="none" pos="word" start_char="1722">have</TOKEN>
<TOKEN end_char="1736" id="token-16-22" morph="none" pos="word" start_char="1727">antibodies</TOKEN>
<TOKEN end_char="1739" id="token-16-23" morph="none" pos="word" start_char="1738">to</TOKEN>
<TOKEN end_char="1743" id="token-16-24" morph="none" pos="word" start_char="1741">the</TOKEN>
<TOKEN end_char="1755" id="token-16-25" morph="none" pos="word" start_char="1745">coronavirus</TOKEN>
<TOKEN end_char="1756" id="token-16-26" morph="none" pos="punct" start_char="1756">.</TOKEN>
</SEG>
<SEG end_char="1868" id="segment-17" start_char="1758">
<ORIGINAL_TEXT>They are now hypervigilant, and stay home for 14 days if they were even potentially exposed to the coronavirus.</ORIGINAL_TEXT>
<TOKEN end_char="1761" id="token-17-0" morph="none" pos="word" start_char="1758">They</TOKEN>
<TOKEN end_char="1765" id="token-17-1" morph="none" pos="word" start_char="1763">are</TOKEN>
<TOKEN end_char="1769" id="token-17-2" morph="none" pos="word" start_char="1767">now</TOKEN>
<TOKEN end_char="1783" id="token-17-3" morph="none" pos="word" start_char="1771">hypervigilant</TOKEN>
<TOKEN end_char="1784" id="token-17-4" morph="none" pos="punct" start_char="1784">,</TOKEN>
<TOKEN end_char="1788" id="token-17-5" morph="none" pos="word" start_char="1786">and</TOKEN>
<TOKEN end_char="1793" id="token-17-6" morph="none" pos="word" start_char="1790">stay</TOKEN>
<TOKEN end_char="1798" id="token-17-7" morph="none" pos="word" start_char="1795">home</TOKEN>
<TOKEN end_char="1802" id="token-17-8" morph="none" pos="word" start_char="1800">for</TOKEN>
<TOKEN end_char="1805" id="token-17-9" morph="none" pos="word" start_char="1804">14</TOKEN>
<TOKEN end_char="1810" id="token-17-10" morph="none" pos="word" start_char="1807">days</TOKEN>
<TOKEN end_char="1813" id="token-17-11" morph="none" pos="word" start_char="1812">if</TOKEN>
<TOKEN end_char="1818" id="token-17-12" morph="none" pos="word" start_char="1815">they</TOKEN>
<TOKEN end_char="1823" id="token-17-13" morph="none" pos="word" start_char="1820">were</TOKEN>
<TOKEN end_char="1828" id="token-17-14" morph="none" pos="word" start_char="1825">even</TOKEN>
<TOKEN end_char="1840" id="token-17-15" morph="none" pos="word" start_char="1830">potentially</TOKEN>
<TOKEN end_char="1848" id="token-17-16" morph="none" pos="word" start_char="1842">exposed</TOKEN>
<TOKEN end_char="1851" id="token-17-17" morph="none" pos="word" start_char="1850">to</TOKEN>
<TOKEN end_char="1855" id="token-17-18" morph="none" pos="word" start_char="1853">the</TOKEN>
<TOKEN end_char="1867" id="token-17-19" morph="none" pos="word" start_char="1857">coronavirus</TOKEN>
<TOKEN end_char="1868" id="token-17-20" morph="none" pos="punct" start_char="1868">.</TOKEN>
</SEG>
<SEG end_char="1939" id="segment-18" start_char="1870">
<ORIGINAL_TEXT>They wouldn't have to do that if they knew they were immune, she says.</ORIGINAL_TEXT>
<TOKEN end_char="1873" id="token-18-0" morph="none" pos="word" start_char="1870">They</TOKEN>
<TOKEN end_char="1882" id="token-18-1" morph="none" pos="word" start_char="1875">wouldn't</TOKEN>
<TOKEN end_char="1887" id="token-18-2" morph="none" pos="word" start_char="1884">have</TOKEN>
<TOKEN end_char="1890" id="token-18-3" morph="none" pos="word" start_char="1889">to</TOKEN>
<TOKEN end_char="1893" id="token-18-4" morph="none" pos="word" start_char="1892">do</TOKEN>
<TOKEN end_char="1898" id="token-18-5" morph="none" pos="word" start_char="1895">that</TOKEN>
<TOKEN end_char="1901" id="token-18-6" morph="none" pos="word" start_char="1900">if</TOKEN>
<TOKEN end_char="1906" id="token-18-7" morph="none" pos="word" start_char="1903">they</TOKEN>
<TOKEN end_char="1911" id="token-18-8" morph="none" pos="word" start_char="1908">knew</TOKEN>
<TOKEN end_char="1916" id="token-18-9" morph="none" pos="word" start_char="1913">they</TOKEN>
<TOKEN end_char="1921" id="token-18-10" morph="none" pos="word" start_char="1918">were</TOKEN>
<TOKEN end_char="1928" id="token-18-11" morph="none" pos="word" start_char="1923">immune</TOKEN>
<TOKEN end_char="1929" id="token-18-12" morph="none" pos="punct" start_char="1929">,</TOKEN>
<TOKEN end_char="1933" id="token-18-13" morph="none" pos="word" start_char="1931">she</TOKEN>
<TOKEN end_char="1938" id="token-18-14" morph="none" pos="word" start_char="1935">says</TOKEN>
<TOKEN end_char="1939" id="token-18-15" morph="none" pos="punct" start_char="1939">.</TOKEN>
</SEG>
<SEG end_char="2001" id="segment-19" start_char="1942">
<ORIGINAL_TEXT>VanderGaast says the test isn't available in her county now.</ORIGINAL_TEXT>
<TOKEN end_char="1952" id="token-19-0" morph="none" pos="word" start_char="1942">VanderGaast</TOKEN>
<TOKEN end_char="1957" id="token-19-1" morph="none" pos="word" start_char="1954">says</TOKEN>
<TOKEN end_char="1961" id="token-19-2" morph="none" pos="word" start_char="1959">the</TOKEN>
<TOKEN end_char="1966" id="token-19-3" morph="none" pos="word" start_char="1963">test</TOKEN>
<TOKEN end_char="1972" id="token-19-4" morph="none" pos="word" start_char="1968">isn't</TOKEN>
<TOKEN end_char="1982" id="token-19-5" morph="none" pos="word" start_char="1974">available</TOKEN>
<TOKEN end_char="1985" id="token-19-6" morph="none" pos="word" start_char="1984">in</TOKEN>
<TOKEN end_char="1989" id="token-19-7" morph="none" pos="word" start_char="1987">her</TOKEN>
<TOKEN end_char="1996" id="token-19-8" morph="none" pos="word" start_char="1991">county</TOKEN>
<TOKEN end_char="2000" id="token-19-9" morph="none" pos="word" start_char="1998">now</TOKEN>
<TOKEN end_char="2001" id="token-19-10" morph="none" pos="punct" start_char="2001">.</TOKEN>
</SEG>
<SEG end_char="2044" id="segment-20" start_char="2003">
<ORIGINAL_TEXT>But it is starting to take off nationally.</ORIGINAL_TEXT>
<TOKEN end_char="2005" id="token-20-0" morph="none" pos="word" start_char="2003">But</TOKEN>
<TOKEN end_char="2008" id="token-20-1" morph="none" pos="word" start_char="2007">it</TOKEN>
<TOKEN end_char="2011" id="token-20-2" morph="none" pos="word" start_char="2010">is</TOKEN>
<TOKEN end_char="2020" id="token-20-3" morph="none" pos="word" start_char="2013">starting</TOKEN>
<TOKEN end_char="2023" id="token-20-4" morph="none" pos="word" start_char="2022">to</TOKEN>
<TOKEN end_char="2028" id="token-20-5" morph="none" pos="word" start_char="2025">take</TOKEN>
<TOKEN end_char="2032" id="token-20-6" morph="none" pos="word" start_char="2030">off</TOKEN>
<TOKEN end_char="2043" id="token-20-7" morph="none" pos="word" start_char="2034">nationally</TOKEN>
<TOKEN end_char="2044" id="token-20-8" morph="none" pos="punct" start_char="2044">.</TOKEN>
</SEG>
<SEG end_char="2113" id="segment-21" start_char="2047">
<ORIGINAL_TEXT>Dr. Jeremy Gabrysch runs a mobile medical service in Austin, Texas.</ORIGINAL_TEXT>
<TOKEN end_char="2048" id="token-21-0" morph="none" pos="word" start_char="2047">Dr</TOKEN>
<TOKEN end_char="2049" id="token-21-1" morph="none" pos="punct" start_char="2049">.</TOKEN>
<TOKEN end_char="2056" id="token-21-2" morph="none" pos="word" start_char="2051">Jeremy</TOKEN>
<TOKEN end_char="2065" id="token-21-3" morph="none" pos="word" start_char="2058">Gabrysch</TOKEN>
<TOKEN end_char="2070" id="token-21-4" morph="none" pos="word" start_char="2067">runs</TOKEN>
<TOKEN end_char="2072" id="token-21-5" morph="none" pos="word" start_char="2072">a</TOKEN>
<TOKEN end_char="2079" id="token-21-6" morph="none" pos="word" start_char="2074">mobile</TOKEN>
<TOKEN end_char="2087" id="token-21-7" morph="none" pos="word" start_char="2081">medical</TOKEN>
<TOKEN end_char="2095" id="token-21-8" morph="none" pos="word" start_char="2089">service</TOKEN>
<TOKEN end_char="2098" id="token-21-9" morph="none" pos="word" start_char="2097">in</TOKEN>
<TOKEN end_char="2105" id="token-21-10" morph="none" pos="word" start_char="2100">Austin</TOKEN>
<TOKEN end_char="2106" id="token-21-11" morph="none" pos="punct" start_char="2106">,</TOKEN>
<TOKEN end_char="2112" id="token-21-12" morph="none" pos="word" start_char="2108">Texas</TOKEN>
<TOKEN end_char="2113" id="token-21-13" morph="none" pos="punct" start_char="2113">.</TOKEN>
</SEG>
<SEG end_char="2252" id="segment-22" start_char="2115">
<ORIGINAL_TEXT>He got a supply of antibody tests made by a major Chinese manufacturer and says he has tested several hundred people in the last few days.</ORIGINAL_TEXT>
<TOKEN end_char="2116" id="token-22-0" morph="none" pos="word" start_char="2115">He</TOKEN>
<TOKEN end_char="2120" id="token-22-1" morph="none" pos="word" start_char="2118">got</TOKEN>
<TOKEN end_char="2122" id="token-22-2" morph="none" pos="word" start_char="2122">a</TOKEN>
<TOKEN end_char="2129" id="token-22-3" morph="none" pos="word" start_char="2124">supply</TOKEN>
<TOKEN end_char="2132" id="token-22-4" morph="none" pos="word" start_char="2131">of</TOKEN>
<TOKEN end_char="2141" id="token-22-5" morph="none" pos="word" start_char="2134">antibody</TOKEN>
<TOKEN end_char="2147" id="token-22-6" morph="none" pos="word" start_char="2143">tests</TOKEN>
<TOKEN end_char="2152" id="token-22-7" morph="none" pos="word" start_char="2149">made</TOKEN>
<TOKEN end_char="2155" id="token-22-8" morph="none" pos="word" start_char="2154">by</TOKEN>
<TOKEN end_char="2157" id="token-22-9" morph="none" pos="word" start_char="2157">a</TOKEN>
<TOKEN end_char="2163" id="token-22-10" morph="none" pos="word" start_char="2159">major</TOKEN>
<TOKEN end_char="2171" id="token-22-11" morph="none" pos="word" start_char="2165">Chinese</TOKEN>
<TOKEN end_char="2184" id="token-22-12" morph="none" pos="word" start_char="2173">manufacturer</TOKEN>
<TOKEN end_char="2188" id="token-22-13" morph="none" pos="word" start_char="2186">and</TOKEN>
<TOKEN end_char="2193" id="token-22-14" morph="none" pos="word" start_char="2190">says</TOKEN>
<TOKEN end_char="2196" id="token-22-15" morph="none" pos="word" start_char="2195">he</TOKEN>
<TOKEN end_char="2200" id="token-22-16" morph="none" pos="word" start_char="2198">has</TOKEN>
<TOKEN end_char="2207" id="token-22-17" morph="none" pos="word" start_char="2202">tested</TOKEN>
<TOKEN end_char="2215" id="token-22-18" morph="none" pos="word" start_char="2209">several</TOKEN>
<TOKEN end_char="2223" id="token-22-19" morph="none" pos="word" start_char="2217">hundred</TOKEN>
<TOKEN end_char="2230" id="token-22-20" morph="none" pos="word" start_char="2225">people</TOKEN>
<TOKEN end_char="2233" id="token-22-21" morph="none" pos="word" start_char="2232">in</TOKEN>
<TOKEN end_char="2237" id="token-22-22" morph="none" pos="word" start_char="2235">the</TOKEN>
<TOKEN end_char="2242" id="token-22-23" morph="none" pos="word" start_char="2239">last</TOKEN>
<TOKEN end_char="2246" id="token-22-24" morph="none" pos="word" start_char="2244">few</TOKEN>
<TOKEN end_char="2251" id="token-22-25" morph="none" pos="word" start_char="2248">days</TOKEN>
<TOKEN end_char="2252" id="token-22-26" morph="none" pos="punct" start_char="2252">.</TOKEN>
</SEG>
<SEG end_char="2451" id="segment-23" start_char="2255">
<ORIGINAL_TEXT>"We offer the test for people who may have suspected they might have had coronavirus back in February or March when testing with the nasal swab [and PCR diagnostic test] was very limited," he says.</ORIGINAL_TEXT>
<TOKEN end_char="2255" id="token-23-0" morph="none" pos="punct" start_char="2255">"</TOKEN>
<TOKEN end_char="2257" id="token-23-1" morph="none" pos="word" start_char="2256">We</TOKEN>
<TOKEN end_char="2263" id="token-23-2" morph="none" pos="word" start_char="2259">offer</TOKEN>
<TOKEN end_char="2267" id="token-23-3" morph="none" pos="word" start_char="2265">the</TOKEN>
<TOKEN end_char="2272" id="token-23-4" morph="none" pos="word" start_char="2269">test</TOKEN>
<TOKEN end_char="2276" id="token-23-5" morph="none" pos="word" start_char="2274">for</TOKEN>
<TOKEN end_char="2283" id="token-23-6" morph="none" pos="word" start_char="2278">people</TOKEN>
<TOKEN end_char="2287" id="token-23-7" morph="none" pos="word" start_char="2285">who</TOKEN>
<TOKEN end_char="2291" id="token-23-8" morph="none" pos="word" start_char="2289">may</TOKEN>
<TOKEN end_char="2296" id="token-23-9" morph="none" pos="word" start_char="2293">have</TOKEN>
<TOKEN end_char="2306" id="token-23-10" morph="none" pos="word" start_char="2298">suspected</TOKEN>
<TOKEN end_char="2311" id="token-23-11" morph="none" pos="word" start_char="2308">they</TOKEN>
<TOKEN end_char="2317" id="token-23-12" morph="none" pos="word" start_char="2313">might</TOKEN>
<TOKEN end_char="2322" id="token-23-13" morph="none" pos="word" start_char="2319">have</TOKEN>
<TOKEN end_char="2326" id="token-23-14" morph="none" pos="word" start_char="2324">had</TOKEN>
<TOKEN end_char="2338" id="token-23-15" morph="none" pos="word" start_char="2328">coronavirus</TOKEN>
<TOKEN end_char="2343" id="token-23-16" morph="none" pos="word" start_char="2340">back</TOKEN>
<TOKEN end_char="2346" id="token-23-17" morph="none" pos="word" start_char="2345">in</TOKEN>
<TOKEN end_char="2355" id="token-23-18" morph="none" pos="word" start_char="2348">February</TOKEN>
<TOKEN end_char="2358" id="token-23-19" morph="none" pos="word" start_char="2357">or</TOKEN>
<TOKEN end_char="2364" id="token-23-20" morph="none" pos="word" start_char="2360">March</TOKEN>
<TOKEN end_char="2369" id="token-23-21" morph="none" pos="word" start_char="2366">when</TOKEN>
<TOKEN end_char="2377" id="token-23-22" morph="none" pos="word" start_char="2371">testing</TOKEN>
<TOKEN end_char="2382" id="token-23-23" morph="none" pos="word" start_char="2379">with</TOKEN>
<TOKEN end_char="2386" id="token-23-24" morph="none" pos="word" start_char="2384">the</TOKEN>
<TOKEN end_char="2392" id="token-23-25" morph="none" pos="word" start_char="2388">nasal</TOKEN>
<TOKEN end_char="2397" id="token-23-26" morph="none" pos="word" start_char="2394">swab</TOKEN>
<TOKEN end_char="2399" id="token-23-27" morph="none" pos="punct" start_char="2399">[</TOKEN>
<TOKEN end_char="2402" id="token-23-28" morph="none" pos="word" start_char="2400">and</TOKEN>
<TOKEN end_char="2406" id="token-23-29" morph="none" pos="word" start_char="2404">PCR</TOKEN>
<TOKEN end_char="2417" id="token-23-30" morph="none" pos="word" start_char="2408">diagnostic</TOKEN>
<TOKEN end_char="2422" id="token-23-31" morph="none" pos="word" start_char="2419">test</TOKEN>
<TOKEN end_char="2423" id="token-23-32" morph="none" pos="punct" start_char="2423">]</TOKEN>
<TOKEN end_char="2427" id="token-23-33" morph="none" pos="word" start_char="2425">was</TOKEN>
<TOKEN end_char="2432" id="token-23-34" morph="none" pos="word" start_char="2429">very</TOKEN>
<TOKEN end_char="2440" id="token-23-35" morph="none" pos="word" start_char="2434">limited</TOKEN>
<TOKEN end_char="2442" id="token-23-36" morph="none" pos="punct" start_char="2441">,"</TOKEN>
<TOKEN end_char="2445" id="token-23-37" morph="none" pos="word" start_char="2444">he</TOKEN>
<TOKEN end_char="2450" id="token-23-38" morph="none" pos="word" start_char="2447">says</TOKEN>
<TOKEN end_char="2451" id="token-23-39" morph="none" pos="punct" start_char="2451">.</TOKEN>
</SEG>
<SEG end_char="2475" id="segment-24" start_char="2453">
<ORIGINAL_TEXT>The charge: $49 a test.</ORIGINAL_TEXT>
<TOKEN end_char="2455" id="token-24-0" morph="none" pos="word" start_char="2453">The</TOKEN>
<TOKEN end_char="2462" id="token-24-1" morph="none" pos="word" start_char="2457">charge</TOKEN>
<TOKEN end_char="2463" id="token-24-2" morph="none" pos="punct" start_char="2463">:</TOKEN>
<TOKEN end_char="2467" id="token-24-3" morph="none" pos="unknown" start_char="2465">$49</TOKEN>
<TOKEN end_char="2469" id="token-24-4" morph="none" pos="word" start_char="2469">a</TOKEN>
<TOKEN end_char="2474" id="token-24-5" morph="none" pos="word" start_char="2471">test</TOKEN>
<TOKEN end_char="2475" id="token-24-6" morph="none" pos="punct" start_char="2475">.</TOKEN>
</SEG>
<SEG end_char="2568" id="segment-25" start_char="2478">
<ORIGINAL_TEXT>Gabrysch says he only tests people when he has other evidence they might have been exposed.</ORIGINAL_TEXT>
<TOKEN end_char="2485" id="token-25-0" morph="none" pos="word" start_char="2478">Gabrysch</TOKEN>
<TOKEN end_char="2490" id="token-25-1" morph="none" pos="word" start_char="2487">says</TOKEN>
<TOKEN end_char="2493" id="token-25-2" morph="none" pos="word" start_char="2492">he</TOKEN>
<TOKEN end_char="2498" id="token-25-3" morph="none" pos="word" start_char="2495">only</TOKEN>
<TOKEN end_char="2504" id="token-25-4" morph="none" pos="word" start_char="2500">tests</TOKEN>
<TOKEN end_char="2511" id="token-25-5" morph="none" pos="word" start_char="2506">people</TOKEN>
<TOKEN end_char="2516" id="token-25-6" morph="none" pos="word" start_char="2513">when</TOKEN>
<TOKEN end_char="2519" id="token-25-7" morph="none" pos="word" start_char="2518">he</TOKEN>
<TOKEN end_char="2523" id="token-25-8" morph="none" pos="word" start_char="2521">has</TOKEN>
<TOKEN end_char="2529" id="token-25-9" morph="none" pos="word" start_char="2525">other</TOKEN>
<TOKEN end_char="2538" id="token-25-10" morph="none" pos="word" start_char="2531">evidence</TOKEN>
<TOKEN end_char="2543" id="token-25-11" morph="none" pos="word" start_char="2540">they</TOKEN>
<TOKEN end_char="2549" id="token-25-12" morph="none" pos="word" start_char="2545">might</TOKEN>
<TOKEN end_char="2554" id="token-25-13" morph="none" pos="word" start_char="2551">have</TOKEN>
<TOKEN end_char="2559" id="token-25-14" morph="none" pos="word" start_char="2556">been</TOKEN>
<TOKEN end_char="2567" id="token-25-15" morph="none" pos="word" start_char="2561">exposed</TOKEN>
<TOKEN end_char="2568" id="token-25-16" morph="none" pos="punct" start_char="2568">.</TOKEN>
</SEG>
<SEG end_char="2772" id="segment-26" start_char="2570">
<ORIGINAL_TEXT>"If they had an illness that sounds like it could have been coronavirus and they had a positive antibody test, then it's very likely that this is a true positive, that they indeed had COVID-19," he says.</ORIGINAL_TEXT>
<TOKEN end_char="2570" id="token-26-0" morph="none" pos="punct" start_char="2570">"</TOKEN>
<TOKEN end_char="2572" id="token-26-1" morph="none" pos="word" start_char="2571">If</TOKEN>
<TOKEN end_char="2577" id="token-26-2" morph="none" pos="word" start_char="2574">they</TOKEN>
<TOKEN end_char="2581" id="token-26-3" morph="none" pos="word" start_char="2579">had</TOKEN>
<TOKEN end_char="2584" id="token-26-4" morph="none" pos="word" start_char="2583">an</TOKEN>
<TOKEN end_char="2592" id="token-26-5" morph="none" pos="word" start_char="2586">illness</TOKEN>
<TOKEN end_char="2597" id="token-26-6" morph="none" pos="word" start_char="2594">that</TOKEN>
<TOKEN end_char="2604" id="token-26-7" morph="none" pos="word" start_char="2599">sounds</TOKEN>
<TOKEN end_char="2609" id="token-26-8" morph="none" pos="word" start_char="2606">like</TOKEN>
<TOKEN end_char="2612" id="token-26-9" morph="none" pos="word" start_char="2611">it</TOKEN>
<TOKEN end_char="2618" id="token-26-10" morph="none" pos="word" start_char="2614">could</TOKEN>
<TOKEN end_char="2623" id="token-26-11" morph="none" pos="word" start_char="2620">have</TOKEN>
<TOKEN end_char="2628" id="token-26-12" morph="none" pos="word" start_char="2625">been</TOKEN>
<TOKEN end_char="2640" id="token-26-13" morph="none" pos="word" start_char="2630">coronavirus</TOKEN>
<TOKEN end_char="2644" id="token-26-14" morph="none" pos="word" start_char="2642">and</TOKEN>
<TOKEN end_char="2649" id="token-26-15" morph="none" pos="word" start_char="2646">they</TOKEN>
<TOKEN end_char="2653" id="token-26-16" morph="none" pos="word" start_char="2651">had</TOKEN>
<TOKEN end_char="2655" id="token-26-17" morph="none" pos="word" start_char="2655">a</TOKEN>
<TOKEN end_char="2664" id="token-26-18" morph="none" pos="word" start_char="2657">positive</TOKEN>
<TOKEN end_char="2673" id="token-26-19" morph="none" pos="word" start_char="2666">antibody</TOKEN>
<TOKEN end_char="2678" id="token-26-20" morph="none" pos="word" start_char="2675">test</TOKEN>
<TOKEN end_char="2679" id="token-26-21" morph="none" pos="punct" start_char="2679">,</TOKEN>
<TOKEN end_char="2684" id="token-26-22" morph="none" pos="word" start_char="2681">then</TOKEN>
<TOKEN end_char="2689" id="token-26-23" morph="none" pos="word" start_char="2686">it's</TOKEN>
<TOKEN end_char="2694" id="token-26-24" morph="none" pos="word" start_char="2691">very</TOKEN>
<TOKEN end_char="2701" id="token-26-25" morph="none" pos="word" start_char="2696">likely</TOKEN>
<TOKEN end_char="2706" id="token-26-26" morph="none" pos="word" start_char="2703">that</TOKEN>
<TOKEN end_char="2711" id="token-26-27" morph="none" pos="word" start_char="2708">this</TOKEN>
<TOKEN end_char="2714" id="token-26-28" morph="none" pos="word" start_char="2713">is</TOKEN>
<TOKEN end_char="2716" id="token-26-29" morph="none" pos="word" start_char="2716">a</TOKEN>
<TOKEN end_char="2721" id="token-26-30" morph="none" pos="word" start_char="2718">true</TOKEN>
<TOKEN end_char="2730" id="token-26-31" morph="none" pos="word" start_char="2723">positive</TOKEN>
<TOKEN end_char="2731" id="token-26-32" morph="none" pos="punct" start_char="2731">,</TOKEN>
<TOKEN end_char="2736" id="token-26-33" morph="none" pos="word" start_char="2733">that</TOKEN>
<TOKEN end_char="2741" id="token-26-34" morph="none" pos="word" start_char="2738">they</TOKEN>
<TOKEN end_char="2748" id="token-26-35" morph="none" pos="word" start_char="2743">indeed</TOKEN>
<TOKEN end_char="2752" id="token-26-36" morph="none" pos="word" start_char="2750">had</TOKEN>
<TOKEN end_char="2761" id="token-26-37" morph="none" pos="unknown" start_char="2754">COVID-19</TOKEN>
<TOKEN end_char="2763" id="token-26-38" morph="none" pos="punct" start_char="2762">,"</TOKEN>
<TOKEN end_char="2766" id="token-26-39" morph="none" pos="word" start_char="2765">he</TOKEN>
<TOKEN end_char="2771" id="token-26-40" morph="none" pos="word" start_char="2768">says</TOKEN>
<TOKEN end_char="2772" id="token-26-41" morph="none" pos="punct" start_char="2772">.</TOKEN>
</SEG>
<SEG end_char="2978" id="segment-27" start_char="2775">
<ORIGINAL_TEXT>The test he's using, produced by Guangzhou Wondfo Biotech in China, boasts a specificity of 99%, which means it only falsely says a blood sample contains antibodies against the coronavirus 1% of the time.</ORIGINAL_TEXT>
<TOKEN end_char="2777" id="token-27-0" morph="none" pos="word" start_char="2775">The</TOKEN>
<TOKEN end_char="2782" id="token-27-1" morph="none" pos="word" start_char="2779">test</TOKEN>
<TOKEN end_char="2787" id="token-27-2" morph="none" pos="word" start_char="2784">he's</TOKEN>
<TOKEN end_char="2793" id="token-27-3" morph="none" pos="word" start_char="2789">using</TOKEN>
<TOKEN end_char="2794" id="token-27-4" morph="none" pos="punct" start_char="2794">,</TOKEN>
<TOKEN end_char="2803" id="token-27-5" morph="none" pos="word" start_char="2796">produced</TOKEN>
<TOKEN end_char="2806" id="token-27-6" morph="none" pos="word" start_char="2805">by</TOKEN>
<TOKEN end_char="2816" id="token-27-7" morph="none" pos="word" start_char="2808">Guangzhou</TOKEN>
<TOKEN end_char="2823" id="token-27-8" morph="none" pos="word" start_char="2818">Wondfo</TOKEN>
<TOKEN end_char="2831" id="token-27-9" morph="none" pos="word" start_char="2825">Biotech</TOKEN>
<TOKEN end_char="2834" id="token-27-10" morph="none" pos="word" start_char="2833">in</TOKEN>
<TOKEN end_char="2840" id="token-27-11" morph="none" pos="word" start_char="2836">China</TOKEN>
<TOKEN end_char="2841" id="token-27-12" morph="none" pos="punct" start_char="2841">,</TOKEN>
<TOKEN end_char="2848" id="token-27-13" morph="none" pos="word" start_char="2843">boasts</TOKEN>
<TOKEN end_char="2850" id="token-27-14" morph="none" pos="word" start_char="2850">a</TOKEN>
<TOKEN end_char="2862" id="token-27-15" morph="none" pos="word" start_char="2852">specificity</TOKEN>
<TOKEN end_char="2865" id="token-27-16" morph="none" pos="word" start_char="2864">of</TOKEN>
<TOKEN end_char="2868" id="token-27-17" morph="none" pos="word" start_char="2867">99</TOKEN>
<TOKEN end_char="2870" id="token-27-18" morph="none" pos="punct" start_char="2869">%,</TOKEN>
<TOKEN end_char="2876" id="token-27-19" morph="none" pos="word" start_char="2872">which</TOKEN>
<TOKEN end_char="2882" id="token-27-20" morph="none" pos="word" start_char="2878">means</TOKEN>
<TOKEN end_char="2885" id="token-27-21" morph="none" pos="word" start_char="2884">it</TOKEN>
<TOKEN end_char="2890" id="token-27-22" morph="none" pos="word" start_char="2887">only</TOKEN>
<TOKEN end_char="2898" id="token-27-23" morph="none" pos="word" start_char="2892">falsely</TOKEN>
<TOKEN end_char="2903" id="token-27-24" morph="none" pos="word" start_char="2900">says</TOKEN>
<TOKEN end_char="2905" id="token-27-25" morph="none" pos="word" start_char="2905">a</TOKEN>
<TOKEN end_char="2911" id="token-27-26" morph="none" pos="word" start_char="2907">blood</TOKEN>
<TOKEN end_char="2918" id="token-27-27" morph="none" pos="word" start_char="2913">sample</TOKEN>
<TOKEN end_char="2927" id="token-27-28" morph="none" pos="word" start_char="2920">contains</TOKEN>
<TOKEN end_char="2938" id="token-27-29" morph="none" pos="word" start_char="2929">antibodies</TOKEN>
<TOKEN end_char="2946" id="token-27-30" morph="none" pos="word" start_char="2940">against</TOKEN>
<TOKEN end_char="2950" id="token-27-31" morph="none" pos="word" start_char="2948">the</TOKEN>
<TOKEN end_char="2962" id="token-27-32" morph="none" pos="word" start_char="2952">coronavirus</TOKEN>
<TOKEN end_char="2964" id="token-27-33" morph="none" pos="word" start_char="2964">1</TOKEN>
<TOKEN end_char="2965" id="token-27-34" morph="none" pos="punct" start_char="2965">%</TOKEN>
<TOKEN end_char="2968" id="token-27-35" morph="none" pos="word" start_char="2967">of</TOKEN>
<TOKEN end_char="2972" id="token-27-36" morph="none" pos="word" start_char="2970">the</TOKEN>
<TOKEN end_char="2977" id="token-27-37" morph="none" pos="word" start_char="2974">time</TOKEN>
<TOKEN end_char="2978" id="token-27-38" morph="none" pos="punct" start_char="2978">.</TOKEN>
</SEG>
<SEG end_char="3109" id="segment-28" start_char="2980">
<ORIGINAL_TEXT>But despite that impressive statistic, a test like that is not 99% correct, and in fact in some circumstances could be much worse.</ORIGINAL_TEXT>
<TOKEN end_char="2982" id="token-28-0" morph="none" pos="word" start_char="2980">But</TOKEN>
<TOKEN end_char="2990" id="token-28-1" morph="none" pos="word" start_char="2984">despite</TOKEN>
<TOKEN end_char="2995" id="token-28-2" morph="none" pos="word" start_char="2992">that</TOKEN>
<TOKEN end_char="3006" id="token-28-3" morph="none" pos="word" start_char="2997">impressive</TOKEN>
<TOKEN end_char="3016" id="token-28-4" morph="none" pos="word" start_char="3008">statistic</TOKEN>
<TOKEN end_char="3017" id="token-28-5" morph="none" pos="punct" start_char="3017">,</TOKEN>
<TOKEN end_char="3019" id="token-28-6" morph="none" pos="word" start_char="3019">a</TOKEN>
<TOKEN end_char="3024" id="token-28-7" morph="none" pos="word" start_char="3021">test</TOKEN>
<TOKEN end_char="3029" id="token-28-8" morph="none" pos="word" start_char="3026">like</TOKEN>
<TOKEN end_char="3034" id="token-28-9" morph="none" pos="word" start_char="3031">that</TOKEN>
<TOKEN end_char="3037" id="token-28-10" morph="none" pos="word" start_char="3036">is</TOKEN>
<TOKEN end_char="3041" id="token-28-11" morph="none" pos="word" start_char="3039">not</TOKEN>
<TOKEN end_char="3044" id="token-28-12" morph="none" pos="word" start_char="3043">99</TOKEN>
<TOKEN end_char="3045" id="token-28-13" morph="none" pos="punct" start_char="3045">%</TOKEN>
<TOKEN end_char="3053" id="token-28-14" morph="none" pos="word" start_char="3047">correct</TOKEN>
<TOKEN end_char="3054" id="token-28-15" morph="none" pos="punct" start_char="3054">,</TOKEN>
<TOKEN end_char="3058" id="token-28-16" morph="none" pos="word" start_char="3056">and</TOKEN>
<TOKEN end_char="3061" id="token-28-17" morph="none" pos="word" start_char="3060">in</TOKEN>
<TOKEN end_char="3066" id="token-28-18" morph="none" pos="word" start_char="3063">fact</TOKEN>
<TOKEN end_char="3069" id="token-28-19" morph="none" pos="word" start_char="3068">in</TOKEN>
<TOKEN end_char="3074" id="token-28-20" morph="none" pos="word" start_char="3071">some</TOKEN>
<TOKEN end_char="3088" id="token-28-21" morph="none" pos="word" start_char="3076">circumstances</TOKEN>
<TOKEN end_char="3094" id="token-28-22" morph="none" pos="word" start_char="3090">could</TOKEN>
<TOKEN end_char="3097" id="token-28-23" morph="none" pos="word" start_char="3096">be</TOKEN>
<TOKEN end_char="3102" id="token-28-24" morph="none" pos="word" start_char="3099">much</TOKEN>
<TOKEN end_char="3108" id="token-28-25" morph="none" pos="word" start_char="3104">worse</TOKEN>
<TOKEN end_char="3109" id="token-28-26" morph="none" pos="punct" start_char="3109">.</TOKEN>
</SEG>
<SEG end_char="3127" id="segment-29" start_char="3112">
<ORIGINAL_TEXT>Testing Glossary</ORIGINAL_TEXT>
<TOKEN end_char="3118" id="token-29-0" morph="none" pos="word" start_char="3112">Testing</TOKEN>
<TOKEN end_char="3127" id="token-29-1" morph="none" pos="word" start_char="3120">Glossary</TOKEN>
</SEG>
<SEG end_char="3213" id="segment-30" start_char="3130">
<ORIGINAL_TEXT>Antibody  A protein your immune system produces to help protect you from infection.</ORIGINAL_TEXT>
<TOKEN end_char="3137" id="token-30-0" morph="none" pos="word" start_char="3130">Antibody</TOKEN>
<TOKEN end_char="3139" id="token-30-1" morph="none" pos="punct" start_char="3139"></TOKEN>
<TOKEN end_char="3141" id="token-30-2" morph="none" pos="word" start_char="3141">A</TOKEN>
<TOKEN end_char="3149" id="token-30-3" morph="none" pos="word" start_char="3143">protein</TOKEN>
<TOKEN end_char="3154" id="token-30-4" morph="none" pos="word" start_char="3151">your</TOKEN>
<TOKEN end_char="3161" id="token-30-5" morph="none" pos="word" start_char="3156">immune</TOKEN>
<TOKEN end_char="3168" id="token-30-6" morph="none" pos="word" start_char="3163">system</TOKEN>
<TOKEN end_char="3177" id="token-30-7" morph="none" pos="word" start_char="3170">produces</TOKEN>
<TOKEN end_char="3180" id="token-30-8" morph="none" pos="word" start_char="3179">to</TOKEN>
<TOKEN end_char="3185" id="token-30-9" morph="none" pos="word" start_char="3182">help</TOKEN>
<TOKEN end_char="3193" id="token-30-10" morph="none" pos="word" start_char="3187">protect</TOKEN>
<TOKEN end_char="3197" id="token-30-11" morph="none" pos="word" start_char="3195">you</TOKEN>
<TOKEN end_char="3202" id="token-30-12" morph="none" pos="word" start_char="3199">from</TOKEN>
<TOKEN end_char="3212" id="token-30-13" morph="none" pos="word" start_char="3204">infection</TOKEN>
<TOKEN end_char="3213" id="token-30-14" morph="none" pos="punct" start_char="3213">.</TOKEN>
</SEG>
<SEG end_char="3285" id="segment-31" start_char="3215">
<ORIGINAL_TEXT>Antibodies that target the coronavirus indicate you have been infected.</ORIGINAL_TEXT>
<TOKEN end_char="3224" id="token-31-0" morph="none" pos="word" start_char="3215">Antibodies</TOKEN>
<TOKEN end_char="3229" id="token-31-1" morph="none" pos="word" start_char="3226">that</TOKEN>
<TOKEN end_char="3236" id="token-31-2" morph="none" pos="word" start_char="3231">target</TOKEN>
<TOKEN end_char="3240" id="token-31-3" morph="none" pos="word" start_char="3238">the</TOKEN>
<TOKEN end_char="3252" id="token-31-4" morph="none" pos="word" start_char="3242">coronavirus</TOKEN>
<TOKEN end_char="3261" id="token-31-5" morph="none" pos="word" start_char="3254">indicate</TOKEN>
<TOKEN end_char="3265" id="token-31-6" morph="none" pos="word" start_char="3263">you</TOKEN>
<TOKEN end_char="3270" id="token-31-7" morph="none" pos="word" start_char="3267">have</TOKEN>
<TOKEN end_char="3275" id="token-31-8" morph="none" pos="word" start_char="3272">been</TOKEN>
<TOKEN end_char="3284" id="token-31-9" morph="none" pos="word" start_char="3277">infected</TOKEN>
<TOKEN end_char="3285" id="token-31-10" morph="none" pos="punct" start_char="3285">.</TOKEN>
</SEG>
<SEG end_char="3345" id="segment-32" start_char="3287">
<ORIGINAL_TEXT>In many diseases, people who are infected develop immunity.</ORIGINAL_TEXT>
<TOKEN end_char="3288" id="token-32-0" morph="none" pos="word" start_char="3287">In</TOKEN>
<TOKEN end_char="3293" id="token-32-1" morph="none" pos="word" start_char="3290">many</TOKEN>
<TOKEN end_char="3302" id="token-32-2" morph="none" pos="word" start_char="3295">diseases</TOKEN>
<TOKEN end_char="3303" id="token-32-3" morph="none" pos="punct" start_char="3303">,</TOKEN>
<TOKEN end_char="3310" id="token-32-4" morph="none" pos="word" start_char="3305">people</TOKEN>
<TOKEN end_char="3314" id="token-32-5" morph="none" pos="word" start_char="3312">who</TOKEN>
<TOKEN end_char="3318" id="token-32-6" morph="none" pos="word" start_char="3316">are</TOKEN>
<TOKEN end_char="3327" id="token-32-7" morph="none" pos="word" start_char="3320">infected</TOKEN>
<TOKEN end_char="3335" id="token-32-8" morph="none" pos="word" start_char="3329">develop</TOKEN>
<TOKEN end_char="3344" id="token-32-9" morph="none" pos="word" start_char="3337">immunity</TOKEN>
<TOKEN end_char="3345" id="token-32-10" morph="none" pos="punct" start_char="3345">.</TOKEN>
</SEG>
<SEG end_char="3494" id="segment-33" start_char="3347">
<ORIGINAL_TEXT>But scientists don't know enough about the coronavirus to know whether antibodies detected in a blood test show that people are immune to the virus.</ORIGINAL_TEXT>
<TOKEN end_char="3349" id="token-33-0" morph="none" pos="word" start_char="3347">But</TOKEN>
<TOKEN end_char="3360" id="token-33-1" morph="none" pos="word" start_char="3351">scientists</TOKEN>
<TOKEN end_char="3366" id="token-33-2" morph="none" pos="word" start_char="3362">don't</TOKEN>
<TOKEN end_char="3371" id="token-33-3" morph="none" pos="word" start_char="3368">know</TOKEN>
<TOKEN end_char="3378" id="token-33-4" morph="none" pos="word" start_char="3373">enough</TOKEN>
<TOKEN end_char="3384" id="token-33-5" morph="none" pos="word" start_char="3380">about</TOKEN>
<TOKEN end_char="3388" id="token-33-6" morph="none" pos="word" start_char="3386">the</TOKEN>
<TOKEN end_char="3400" id="token-33-7" morph="none" pos="word" start_char="3390">coronavirus</TOKEN>
<TOKEN end_char="3403" id="token-33-8" morph="none" pos="word" start_char="3402">to</TOKEN>
<TOKEN end_char="3408" id="token-33-9" morph="none" pos="word" start_char="3405">know</TOKEN>
<TOKEN end_char="3416" id="token-33-10" morph="none" pos="word" start_char="3410">whether</TOKEN>
<TOKEN end_char="3427" id="token-33-11" morph="none" pos="word" start_char="3418">antibodies</TOKEN>
<TOKEN end_char="3436" id="token-33-12" morph="none" pos="word" start_char="3429">detected</TOKEN>
<TOKEN end_char="3439" id="token-33-13" morph="none" pos="word" start_char="3438">in</TOKEN>
<TOKEN end_char="3441" id="token-33-14" morph="none" pos="word" start_char="3441">a</TOKEN>
<TOKEN end_char="3447" id="token-33-15" morph="none" pos="word" start_char="3443">blood</TOKEN>
<TOKEN end_char="3452" id="token-33-16" morph="none" pos="word" start_char="3449">test</TOKEN>
<TOKEN end_char="3457" id="token-33-17" morph="none" pos="word" start_char="3454">show</TOKEN>
<TOKEN end_char="3462" id="token-33-18" morph="none" pos="word" start_char="3459">that</TOKEN>
<TOKEN end_char="3469" id="token-33-19" morph="none" pos="word" start_char="3464">people</TOKEN>
<TOKEN end_char="3473" id="token-33-20" morph="none" pos="word" start_char="3471">are</TOKEN>
<TOKEN end_char="3480" id="token-33-21" morph="none" pos="word" start_char="3475">immune</TOKEN>
<TOKEN end_char="3483" id="token-33-22" morph="none" pos="word" start_char="3482">to</TOKEN>
<TOKEN end_char="3487" id="token-33-23" morph="none" pos="word" start_char="3485">the</TOKEN>
<TOKEN end_char="3493" id="token-33-24" morph="none" pos="word" start_char="3489">virus</TOKEN>
<TOKEN end_char="3494" id="token-33-25" morph="none" pos="punct" start_char="3494">.</TOKEN>
</SEG>
<SEG end_char="3568" id="segment-34" start_char="3496">
<ORIGINAL_TEXT>If it turns out they are, it's unclear how long that immunity might last.</ORIGINAL_TEXT>
<TOKEN end_char="3497" id="token-34-0" morph="none" pos="word" start_char="3496">If</TOKEN>
<TOKEN end_char="3500" id="token-34-1" morph="none" pos="word" start_char="3499">it</TOKEN>
<TOKEN end_char="3506" id="token-34-2" morph="none" pos="word" start_char="3502">turns</TOKEN>
<TOKEN end_char="3510" id="token-34-3" morph="none" pos="word" start_char="3508">out</TOKEN>
<TOKEN end_char="3515" id="token-34-4" morph="none" pos="word" start_char="3512">they</TOKEN>
<TOKEN end_char="3519" id="token-34-5" morph="none" pos="word" start_char="3517">are</TOKEN>
<TOKEN end_char="3520" id="token-34-6" morph="none" pos="punct" start_char="3520">,</TOKEN>
<TOKEN end_char="3525" id="token-34-7" morph="none" pos="word" start_char="3522">it's</TOKEN>
<TOKEN end_char="3533" id="token-34-8" morph="none" pos="word" start_char="3527">unclear</TOKEN>
<TOKEN end_char="3537" id="token-34-9" morph="none" pos="word" start_char="3535">how</TOKEN>
<TOKEN end_char="3542" id="token-34-10" morph="none" pos="word" start_char="3539">long</TOKEN>
<TOKEN end_char="3547" id="token-34-11" morph="none" pos="word" start_char="3544">that</TOKEN>
<TOKEN end_char="3556" id="token-34-12" morph="none" pos="word" start_char="3549">immunity</TOKEN>
<TOKEN end_char="3562" id="token-34-13" morph="none" pos="word" start_char="3558">might</TOKEN>
<TOKEN end_char="3567" id="token-34-14" morph="none" pos="word" start_char="3564">last</TOKEN>
<TOKEN end_char="3568" id="token-34-15" morph="none" pos="punct" start_char="3568">.</TOKEN>
</SEG>
<SEG end_char="3719" id="segment-35" start_char="3571">
<ORIGINAL_TEXT>False positive  In the context of antibody testing, a result that indicates your body has developed antibodies to a virus when in fact you have not.</ORIGINAL_TEXT>
<TOKEN end_char="3575" id="token-35-0" morph="none" pos="word" start_char="3571">False</TOKEN>
<TOKEN end_char="3584" id="token-35-1" morph="none" pos="word" start_char="3577">positive</TOKEN>
<TOKEN end_char="3586" id="token-35-2" morph="none" pos="punct" start_char="3586"></TOKEN>
<TOKEN end_char="3589" id="token-35-3" morph="none" pos="word" start_char="3588">In</TOKEN>
<TOKEN end_char="3593" id="token-35-4" morph="none" pos="word" start_char="3591">the</TOKEN>
<TOKEN end_char="3601" id="token-35-5" morph="none" pos="word" start_char="3595">context</TOKEN>
<TOKEN end_char="3604" id="token-35-6" morph="none" pos="word" start_char="3603">of</TOKEN>
<TOKEN end_char="3613" id="token-35-7" morph="none" pos="word" start_char="3606">antibody</TOKEN>
<TOKEN end_char="3621" id="token-35-8" morph="none" pos="word" start_char="3615">testing</TOKEN>
<TOKEN end_char="3622" id="token-35-9" morph="none" pos="punct" start_char="3622">,</TOKEN>
<TOKEN end_char="3624" id="token-35-10" morph="none" pos="word" start_char="3624">a</TOKEN>
<TOKEN end_char="3631" id="token-35-11" morph="none" pos="word" start_char="3626">result</TOKEN>
<TOKEN end_char="3636" id="token-35-12" morph="none" pos="word" start_char="3633">that</TOKEN>
<TOKEN end_char="3646" id="token-35-13" morph="none" pos="word" start_char="3638">indicates</TOKEN>
<TOKEN end_char="3651" id="token-35-14" morph="none" pos="word" start_char="3648">your</TOKEN>
<TOKEN end_char="3656" id="token-35-15" morph="none" pos="word" start_char="3653">body</TOKEN>
<TOKEN end_char="3660" id="token-35-16" morph="none" pos="word" start_char="3658">has</TOKEN>
<TOKEN end_char="3670" id="token-35-17" morph="none" pos="word" start_char="3662">developed</TOKEN>
<TOKEN end_char="3681" id="token-35-18" morph="none" pos="word" start_char="3672">antibodies</TOKEN>
<TOKEN end_char="3684" id="token-35-19" morph="none" pos="word" start_char="3683">to</TOKEN>
<TOKEN end_char="3686" id="token-35-20" morph="none" pos="word" start_char="3686">a</TOKEN>
<TOKEN end_char="3692" id="token-35-21" morph="none" pos="word" start_char="3688">virus</TOKEN>
<TOKEN end_char="3697" id="token-35-22" morph="none" pos="word" start_char="3694">when</TOKEN>
<TOKEN end_char="3700" id="token-35-23" morph="none" pos="word" start_char="3699">in</TOKEN>
<TOKEN end_char="3705" id="token-35-24" morph="none" pos="word" start_char="3702">fact</TOKEN>
<TOKEN end_char="3709" id="token-35-25" morph="none" pos="word" start_char="3707">you</TOKEN>
<TOKEN end_char="3714" id="token-35-26" morph="none" pos="word" start_char="3711">have</TOKEN>
<TOKEN end_char="3718" id="token-35-27" morph="none" pos="word" start_char="3716">not</TOKEN>
<TOKEN end_char="3719" id="token-35-28" morph="none" pos="punct" start_char="3719">.</TOKEN>
</SEG>
<SEG end_char="3778" id="segment-36" start_char="3721">
<ORIGINAL_TEXT>False positives can give people a false sense of security.</ORIGINAL_TEXT>
<TOKEN end_char="3725" id="token-36-0" morph="none" pos="word" start_char="3721">False</TOKEN>
<TOKEN end_char="3735" id="token-36-1" morph="none" pos="word" start_char="3727">positives</TOKEN>
<TOKEN end_char="3739" id="token-36-2" morph="none" pos="word" start_char="3737">can</TOKEN>
<TOKEN end_char="3744" id="token-36-3" morph="none" pos="word" start_char="3741">give</TOKEN>
<TOKEN end_char="3751" id="token-36-4" morph="none" pos="word" start_char="3746">people</TOKEN>
<TOKEN end_char="3753" id="token-36-5" morph="none" pos="word" start_char="3753">a</TOKEN>
<TOKEN end_char="3759" id="token-36-6" morph="none" pos="word" start_char="3755">false</TOKEN>
<TOKEN end_char="3765" id="token-36-7" morph="none" pos="word" start_char="3761">sense</TOKEN>
<TOKEN end_char="3768" id="token-36-8" morph="none" pos="word" start_char="3767">of</TOKEN>
<TOKEN end_char="3777" id="token-36-9" morph="none" pos="word" start_char="3770">security</TOKEN>
<TOKEN end_char="3778" id="token-36-10" morph="none" pos="punct" start_char="3778">.</TOKEN>
</SEG>
<SEG end_char="3908" id="segment-37" start_char="3781">
<ORIGINAL_TEXT>False negative  In the context of antibody testing, a result that indicates you do not have the antibodies when in fact you do.</ORIGINAL_TEXT>
<TOKEN end_char="3785" id="token-37-0" morph="none" pos="word" start_char="3781">False</TOKEN>
<TOKEN end_char="3794" id="token-37-1" morph="none" pos="word" start_char="3787">negative</TOKEN>
<TOKEN end_char="3796" id="token-37-2" morph="none" pos="punct" start_char="3796"></TOKEN>
<TOKEN end_char="3799" id="token-37-3" morph="none" pos="word" start_char="3798">In</TOKEN>
<TOKEN end_char="3803" id="token-37-4" morph="none" pos="word" start_char="3801">the</TOKEN>
<TOKEN end_char="3811" id="token-37-5" morph="none" pos="word" start_char="3805">context</TOKEN>
<TOKEN end_char="3814" id="token-37-6" morph="none" pos="word" start_char="3813">of</TOKEN>
<TOKEN end_char="3823" id="token-37-7" morph="none" pos="word" start_char="3816">antibody</TOKEN>
<TOKEN end_char="3831" id="token-37-8" morph="none" pos="word" start_char="3825">testing</TOKEN>
<TOKEN end_char="3832" id="token-37-9" morph="none" pos="punct" start_char="3832">,</TOKEN>
<TOKEN end_char="3834" id="token-37-10" morph="none" pos="word" start_char="3834">a</TOKEN>
<TOKEN end_char="3841" id="token-37-11" morph="none" pos="word" start_char="3836">result</TOKEN>
<TOKEN end_char="3846" id="token-37-12" morph="none" pos="word" start_char="3843">that</TOKEN>
<TOKEN end_char="3856" id="token-37-13" morph="none" pos="word" start_char="3848">indicates</TOKEN>
<TOKEN end_char="3860" id="token-37-14" morph="none" pos="word" start_char="3858">you</TOKEN>
<TOKEN end_char="3863" id="token-37-15" morph="none" pos="word" start_char="3862">do</TOKEN>
<TOKEN end_char="3867" id="token-37-16" morph="none" pos="word" start_char="3865">not</TOKEN>
<TOKEN end_char="3872" id="token-37-17" morph="none" pos="word" start_char="3869">have</TOKEN>
<TOKEN end_char="3876" id="token-37-18" morph="none" pos="word" start_char="3874">the</TOKEN>
<TOKEN end_char="3887" id="token-37-19" morph="none" pos="word" start_char="3878">antibodies</TOKEN>
<TOKEN end_char="3892" id="token-37-20" morph="none" pos="word" start_char="3889">when</TOKEN>
<TOKEN end_char="3895" id="token-37-21" morph="none" pos="word" start_char="3894">in</TOKEN>
<TOKEN end_char="3900" id="token-37-22" morph="none" pos="word" start_char="3897">fact</TOKEN>
<TOKEN end_char="3904" id="token-37-23" morph="none" pos="word" start_char="3902">you</TOKEN>
<TOKEN end_char="3907" id="token-37-24" morph="none" pos="word" start_char="3906">do</TOKEN>
<TOKEN end_char="3908" id="token-37-25" morph="none" pos="punct" start_char="3908">.</TOKEN>
</SEG>
<SEG end_char="3989" id="segment-38" start_char="3910">
<ORIGINAL_TEXT>People with false-negative test results have actually been exposed to the virus.</ORIGINAL_TEXT>
<TOKEN end_char="3915" id="token-38-0" morph="none" pos="word" start_char="3910">People</TOKEN>
<TOKEN end_char="3920" id="token-38-1" morph="none" pos="word" start_char="3917">with</TOKEN>
<TOKEN end_char="3935" id="token-38-2" morph="none" pos="unknown" start_char="3922">false-negative</TOKEN>
<TOKEN end_char="3940" id="token-38-3" morph="none" pos="word" start_char="3937">test</TOKEN>
<TOKEN end_char="3948" id="token-38-4" morph="none" pos="word" start_char="3942">results</TOKEN>
<TOKEN end_char="3953" id="token-38-5" morph="none" pos="word" start_char="3950">have</TOKEN>
<TOKEN end_char="3962" id="token-38-6" morph="none" pos="word" start_char="3955">actually</TOKEN>
<TOKEN end_char="3967" id="token-38-7" morph="none" pos="word" start_char="3964">been</TOKEN>
<TOKEN end_char="3975" id="token-38-8" morph="none" pos="word" start_char="3969">exposed</TOKEN>
<TOKEN end_char="3978" id="token-38-9" morph="none" pos="word" start_char="3977">to</TOKEN>
<TOKEN end_char="3982" id="token-38-10" morph="none" pos="word" start_char="3980">the</TOKEN>
<TOKEN end_char="3988" id="token-38-11" morph="none" pos="word" start_char="3984">virus</TOKEN>
<TOKEN end_char="3989" id="token-38-12" morph="none" pos="punct" start_char="3989">.</TOKEN>
</SEG>
<SEG end_char="4080" id="segment-39" start_char="3992">
<ORIGINAL_TEXT>True positive and true negative  These are test results that accurately reflect reality.</ORIGINAL_TEXT>
<TOKEN end_char="3995" id="token-39-0" morph="none" pos="word" start_char="3992">True</TOKEN>
<TOKEN end_char="4004" id="token-39-1" morph="none" pos="word" start_char="3997">positive</TOKEN>
<TOKEN end_char="4008" id="token-39-2" morph="none" pos="word" start_char="4006">and</TOKEN>
<TOKEN end_char="4013" id="token-39-3" morph="none" pos="word" start_char="4010">true</TOKEN>
<TOKEN end_char="4022" id="token-39-4" morph="none" pos="word" start_char="4015">negative</TOKEN>
<TOKEN end_char="4024" id="token-39-5" morph="none" pos="punct" start_char="4024"></TOKEN>
<TOKEN end_char="4030" id="token-39-6" morph="none" pos="word" start_char="4026">These</TOKEN>
<TOKEN end_char="4034" id="token-39-7" morph="none" pos="word" start_char="4032">are</TOKEN>
<TOKEN end_char="4039" id="token-39-8" morph="none" pos="word" start_char="4036">test</TOKEN>
<TOKEN end_char="4047" id="token-39-9" morph="none" pos="word" start_char="4041">results</TOKEN>
<TOKEN end_char="4052" id="token-39-10" morph="none" pos="word" start_char="4049">that</TOKEN>
<TOKEN end_char="4063" id="token-39-11" morph="none" pos="word" start_char="4054">accurately</TOKEN>
<TOKEN end_char="4071" id="token-39-12" morph="none" pos="word" start_char="4065">reflect</TOKEN>
<TOKEN end_char="4079" id="token-39-13" morph="none" pos="word" start_char="4073">reality</TOKEN>
<TOKEN end_char="4080" id="token-39-14" morph="none" pos="punct" start_char="4080">.</TOKEN>
</SEG>
<SEG end_char="4165" id="segment-40" start_char="4083">
<ORIGINAL_TEXT>Specificity  A term that describes how good a test is at avoiding false positives.</ORIGINAL_TEXT>
<TOKEN end_char="4093" id="token-40-0" morph="none" pos="word" start_char="4083">Specificity</TOKEN>
<TOKEN end_char="4095" id="token-40-1" morph="none" pos="punct" start_char="4095"></TOKEN>
<TOKEN end_char="4097" id="token-40-2" morph="none" pos="word" start_char="4097">A</TOKEN>
<TOKEN end_char="4102" id="token-40-3" morph="none" pos="word" start_char="4099">term</TOKEN>
<TOKEN end_char="4107" id="token-40-4" morph="none" pos="word" start_char="4104">that</TOKEN>
<TOKEN end_char="4117" id="token-40-5" morph="none" pos="word" start_char="4109">describes</TOKEN>
<TOKEN end_char="4121" id="token-40-6" morph="none" pos="word" start_char="4119">how</TOKEN>
<TOKEN end_char="4126" id="token-40-7" morph="none" pos="word" start_char="4123">good</TOKEN>
<TOKEN end_char="4128" id="token-40-8" morph="none" pos="word" start_char="4128">a</TOKEN>
<TOKEN end_char="4133" id="token-40-9" morph="none" pos="word" start_char="4130">test</TOKEN>
<TOKEN end_char="4136" id="token-40-10" morph="none" pos="word" start_char="4135">is</TOKEN>
<TOKEN end_char="4139" id="token-40-11" morph="none" pos="word" start_char="4138">at</TOKEN>
<TOKEN end_char="4148" id="token-40-12" morph="none" pos="word" start_char="4141">avoiding</TOKEN>
<TOKEN end_char="4154" id="token-40-13" morph="none" pos="word" start_char="4150">false</TOKEN>
<TOKEN end_char="4164" id="token-40-14" morph="none" pos="word" start_char="4156">positives</TOKEN>
<TOKEN end_char="4165" id="token-40-15" morph="none" pos="punct" start_char="4165">.</TOKEN>
</SEG>
<SEG end_char="4223" id="segment-41" start_char="4167">
<ORIGINAL_TEXT>A test with 95% specificity has a 5% false-positive rate.</ORIGINAL_TEXT>
<TOKEN end_char="4167" id="token-41-0" morph="none" pos="word" start_char="4167">A</TOKEN>
<TOKEN end_char="4172" id="token-41-1" morph="none" pos="word" start_char="4169">test</TOKEN>
<TOKEN end_char="4177" id="token-41-2" morph="none" pos="word" start_char="4174">with</TOKEN>
<TOKEN end_char="4180" id="token-41-3" morph="none" pos="word" start_char="4179">95</TOKEN>
<TOKEN end_char="4181" id="token-41-4" morph="none" pos="punct" start_char="4181">%</TOKEN>
<TOKEN end_char="4193" id="token-41-5" morph="none" pos="word" start_char="4183">specificity</TOKEN>
<TOKEN end_char="4197" id="token-41-6" morph="none" pos="word" start_char="4195">has</TOKEN>
<TOKEN end_char="4199" id="token-41-7" morph="none" pos="word" start_char="4199">a</TOKEN>
<TOKEN end_char="4201" id="token-41-8" morph="none" pos="word" start_char="4201">5</TOKEN>
<TOKEN end_char="4202" id="token-41-9" morph="none" pos="punct" start_char="4202">%</TOKEN>
<TOKEN end_char="4217" id="token-41-10" morph="none" pos="unknown" start_char="4204">false-positive</TOKEN>
<TOKEN end_char="4222" id="token-41-11" morph="none" pos="word" start_char="4219">rate</TOKEN>
<TOKEN end_char="4223" id="token-41-12" morph="none" pos="punct" start_char="4223">.</TOKEN>
</SEG>
<SEG end_char="4284" id="segment-42" start_char="4225">
<ORIGINAL_TEXT>Tests for the coronavirus range from 90% to 99% specificity.</ORIGINAL_TEXT>
<TOKEN end_char="4229" id="token-42-0" morph="none" pos="word" start_char="4225">Tests</TOKEN>
<TOKEN end_char="4233" id="token-42-1" morph="none" pos="word" start_char="4231">for</TOKEN>
<TOKEN end_char="4237" id="token-42-2" morph="none" pos="word" start_char="4235">the</TOKEN>
<TOKEN end_char="4249" id="token-42-3" morph="none" pos="word" start_char="4239">coronavirus</TOKEN>
<TOKEN end_char="4255" id="token-42-4" morph="none" pos="word" start_char="4251">range</TOKEN>
<TOKEN end_char="4260" id="token-42-5" morph="none" pos="word" start_char="4257">from</TOKEN>
<TOKEN end_char="4263" id="token-42-6" morph="none" pos="word" start_char="4262">90</TOKEN>
<TOKEN end_char="4264" id="token-42-7" morph="none" pos="punct" start_char="4264">%</TOKEN>
<TOKEN end_char="4267" id="token-42-8" morph="none" pos="word" start_char="4266">to</TOKEN>
<TOKEN end_char="4270" id="token-42-9" morph="none" pos="word" start_char="4269">99</TOKEN>
<TOKEN end_char="4271" id="token-42-10" morph="none" pos="punct" start_char="4271">%</TOKEN>
<TOKEN end_char="4283" id="token-42-11" morph="none" pos="word" start_char="4273">specificity</TOKEN>
<TOKEN end_char="4284" id="token-42-12" morph="none" pos="punct" start_char="4284">.</TOKEN>
</SEG>
<SEG end_char="4302" id="segment-43" start_char="4286">
<ORIGINAL_TEXT>Higher is better.</ORIGINAL_TEXT>
<TOKEN end_char="4291" id="token-43-0" morph="none" pos="word" start_char="4286">Higher</TOKEN>
<TOKEN end_char="4294" id="token-43-1" morph="none" pos="word" start_char="4293">is</TOKEN>
<TOKEN end_char="4301" id="token-43-2" morph="none" pos="word" start_char="4296">better</TOKEN>
<TOKEN end_char="4302" id="token-43-3" morph="none" pos="punct" start_char="4302">.</TOKEN>
<TRANSLATED_TEXT>Hogere is beter.</TRANSLATED_TEXT><DETECTED_LANGUAGE>da</DETECTED_LANGUAGE></SEG>
<SEG end_char="4383" id="segment-44" start_char="4305">
<ORIGINAL_TEXT>Sensitivity  A term that describes how good a test is at finding the antibody.</ORIGINAL_TEXT>
<TOKEN end_char="4315" id="token-44-0" morph="none" pos="word" start_char="4305">Sensitivity</TOKEN>
<TOKEN end_char="4317" id="token-44-1" morph="none" pos="punct" start_char="4317"></TOKEN>
<TOKEN end_char="4319" id="token-44-2" morph="none" pos="word" start_char="4319">A</TOKEN>
<TOKEN end_char="4324" id="token-44-3" morph="none" pos="word" start_char="4321">term</TOKEN>
<TOKEN end_char="4329" id="token-44-4" morph="none" pos="word" start_char="4326">that</TOKEN>
<TOKEN end_char="4339" id="token-44-5" morph="none" pos="word" start_char="4331">describes</TOKEN>
<TOKEN end_char="4343" id="token-44-6" morph="none" pos="word" start_char="4341">how</TOKEN>
<TOKEN end_char="4348" id="token-44-7" morph="none" pos="word" start_char="4345">good</TOKEN>
<TOKEN end_char="4350" id="token-44-8" morph="none" pos="word" start_char="4350">a</TOKEN>
<TOKEN end_char="4355" id="token-44-9" morph="none" pos="word" start_char="4352">test</TOKEN>
<TOKEN end_char="4358" id="token-44-10" morph="none" pos="word" start_char="4357">is</TOKEN>
<TOKEN end_char="4361" id="token-44-11" morph="none" pos="word" start_char="4360">at</TOKEN>
<TOKEN end_char="4369" id="token-44-12" morph="none" pos="word" start_char="4363">finding</TOKEN>
<TOKEN end_char="4373" id="token-44-13" morph="none" pos="word" start_char="4371">the</TOKEN>
<TOKEN end_char="4382" id="token-44-14" morph="none" pos="word" start_char="4375">antibody</TOKEN>
<TOKEN end_char="4383" id="token-44-15" morph="none" pos="punct" start_char="4383">.</TOKEN>
</SEG>
<SEG end_char="4484" id="segment-45" start_char="4385">
<ORIGINAL_TEXT>A test with 95% sensitivity will find antibodies that are present in a blood sample 95% of the time.</ORIGINAL_TEXT>
<TOKEN end_char="4385" id="token-45-0" morph="none" pos="word" start_char="4385">A</TOKEN>
<TOKEN end_char="4390" id="token-45-1" morph="none" pos="word" start_char="4387">test</TOKEN>
<TOKEN end_char="4395" id="token-45-2" morph="none" pos="word" start_char="4392">with</TOKEN>
<TOKEN end_char="4398" id="token-45-3" morph="none" pos="word" start_char="4397">95</TOKEN>
<TOKEN end_char="4399" id="token-45-4" morph="none" pos="punct" start_char="4399">%</TOKEN>
<TOKEN end_char="4411" id="token-45-5" morph="none" pos="word" start_char="4401">sensitivity</TOKEN>
<TOKEN end_char="4416" id="token-45-6" morph="none" pos="word" start_char="4413">will</TOKEN>
<TOKEN end_char="4421" id="token-45-7" morph="none" pos="word" start_char="4418">find</TOKEN>
<TOKEN end_char="4432" id="token-45-8" morph="none" pos="word" start_char="4423">antibodies</TOKEN>
<TOKEN end_char="4437" id="token-45-9" morph="none" pos="word" start_char="4434">that</TOKEN>
<TOKEN end_char="4441" id="token-45-10" morph="none" pos="word" start_char="4439">are</TOKEN>
<TOKEN end_char="4449" id="token-45-11" morph="none" pos="word" start_char="4443">present</TOKEN>
<TOKEN end_char="4452" id="token-45-12" morph="none" pos="word" start_char="4451">in</TOKEN>
<TOKEN end_char="4454" id="token-45-13" morph="none" pos="word" start_char="4454">a</TOKEN>
<TOKEN end_char="4460" id="token-45-14" morph="none" pos="word" start_char="4456">blood</TOKEN>
<TOKEN end_char="4467" id="token-45-15" morph="none" pos="word" start_char="4462">sample</TOKEN>
<TOKEN end_char="4470" id="token-45-16" morph="none" pos="word" start_char="4469">95</TOKEN>
<TOKEN end_char="4471" id="token-45-17" morph="none" pos="punct" start_char="4471">%</TOKEN>
<TOKEN end_char="4474" id="token-45-18" morph="none" pos="word" start_char="4473">of</TOKEN>
<TOKEN end_char="4478" id="token-45-19" morph="none" pos="word" start_char="4476">the</TOKEN>
<TOKEN end_char="4483" id="token-45-20" morph="none" pos="word" start_char="4480">time</TOKEN>
<TOKEN end_char="4484" id="token-45-21" morph="none" pos="punct" start_char="4484">.</TOKEN>
</SEG>
<SEG end_char="4538" id="segment-46" start_char="4486">
<ORIGINAL_TEXT>The remaining 5% of results would be false negatives.</ORIGINAL_TEXT>
<TOKEN end_char="4488" id="token-46-0" morph="none" pos="word" start_char="4486">The</TOKEN>
<TOKEN end_char="4498" id="token-46-1" morph="none" pos="word" start_char="4490">remaining</TOKEN>
<TOKEN end_char="4500" id="token-46-2" morph="none" pos="word" start_char="4500">5</TOKEN>
<TOKEN end_char="4501" id="token-46-3" morph="none" pos="punct" start_char="4501">%</TOKEN>
<TOKEN end_char="4504" id="token-46-4" morph="none" pos="word" start_char="4503">of</TOKEN>
<TOKEN end_char="4512" id="token-46-5" morph="none" pos="word" start_char="4506">results</TOKEN>
<TOKEN end_char="4518" id="token-46-6" morph="none" pos="word" start_char="4514">would</TOKEN>
<TOKEN end_char="4521" id="token-46-7" morph="none" pos="word" start_char="4520">be</TOKEN>
<TOKEN end_char="4527" id="token-46-8" morph="none" pos="word" start_char="4523">false</TOKEN>
<TOKEN end_char="4537" id="token-46-9" morph="none" pos="word" start_char="4529">negatives</TOKEN>
<TOKEN end_char="4538" id="token-46-10" morph="none" pos="punct" start_char="4538">.</TOKEN>
</SEG>
<SEG end_char="4650" id="segment-47" start_char="4541">
<ORIGINAL_TEXT>False discovery rate  The chance that a positive result is actually a false positive instead a true positive.</ORIGINAL_TEXT>
<TOKEN end_char="4545" id="token-47-0" morph="none" pos="word" start_char="4541">False</TOKEN>
<TOKEN end_char="4555" id="token-47-1" morph="none" pos="word" start_char="4547">discovery</TOKEN>
<TOKEN end_char="4560" id="token-47-2" morph="none" pos="word" start_char="4557">rate</TOKEN>
<TOKEN end_char="4562" id="token-47-3" morph="none" pos="punct" start_char="4562"></TOKEN>
<TOKEN end_char="4566" id="token-47-4" morph="none" pos="word" start_char="4564">The</TOKEN>
<TOKEN end_char="4573" id="token-47-5" morph="none" pos="word" start_char="4568">chance</TOKEN>
<TOKEN end_char="4578" id="token-47-6" morph="none" pos="word" start_char="4575">that</TOKEN>
<TOKEN end_char="4580" id="token-47-7" morph="none" pos="word" start_char="4580">a</TOKEN>
<TOKEN end_char="4589" id="token-47-8" morph="none" pos="word" start_char="4582">positive</TOKEN>
<TOKEN end_char="4596" id="token-47-9" morph="none" pos="word" start_char="4591">result</TOKEN>
<TOKEN end_char="4599" id="token-47-10" morph="none" pos="word" start_char="4598">is</TOKEN>
<TOKEN end_char="4608" id="token-47-11" morph="none" pos="word" start_char="4601">actually</TOKEN>
<TOKEN end_char="4610" id="token-47-12" morph="none" pos="word" start_char="4610">a</TOKEN>
<TOKEN end_char="4616" id="token-47-13" morph="none" pos="word" start_char="4612">false</TOKEN>
<TOKEN end_char="4625" id="token-47-14" morph="none" pos="word" start_char="4618">positive</TOKEN>
<TOKEN end_char="4633" id="token-47-15" morph="none" pos="word" start_char="4627">instead</TOKEN>
<TOKEN end_char="4635" id="token-47-16" morph="none" pos="word" start_char="4635">a</TOKEN>
<TOKEN end_char="4640" id="token-47-17" morph="none" pos="word" start_char="4637">true</TOKEN>
<TOKEN end_char="4649" id="token-47-18" morph="none" pos="word" start_char="4642">positive</TOKEN>
<TOKEN end_char="4650" id="token-47-19" morph="none" pos="punct" start_char="4650">.</TOKEN>
</SEG>
<SEG end_char="4694" id="segment-48" start_char="4652">
<ORIGINAL_TEXT>This is the figure our calculator produces.</ORIGINAL_TEXT>
<TOKEN end_char="4655" id="token-48-0" morph="none" pos="word" start_char="4652">This</TOKEN>
<TOKEN end_char="4658" id="token-48-1" morph="none" pos="word" start_char="4657">is</TOKEN>
<TOKEN end_char="4662" id="token-48-2" morph="none" pos="word" start_char="4660">the</TOKEN>
<TOKEN end_char="4669" id="token-48-3" morph="none" pos="word" start_char="4664">figure</TOKEN>
<TOKEN end_char="4673" id="token-48-4" morph="none" pos="word" start_char="4671">our</TOKEN>
<TOKEN end_char="4684" id="token-48-5" morph="none" pos="word" start_char="4675">calculator</TOKEN>
<TOKEN end_char="4693" id="token-48-6" morph="none" pos="word" start_char="4686">produces</TOKEN>
<TOKEN end_char="4694" id="token-48-7" morph="none" pos="punct" start_char="4694">.</TOKEN>
</SEG>
<SEG end_char="4865" id="segment-49" start_char="4697">
<ORIGINAL_TEXT>That's because of this counterintuitive fact: The validity of a test depends not only on the technology, but how common the disease is in the population you're sampling.</ORIGINAL_TEXT>
<TOKEN end_char="4702" id="token-49-0" morph="none" pos="word" start_char="4697">That's</TOKEN>
<TOKEN end_char="4710" id="token-49-1" morph="none" pos="word" start_char="4704">because</TOKEN>
<TOKEN end_char="4713" id="token-49-2" morph="none" pos="word" start_char="4712">of</TOKEN>
<TOKEN end_char="4718" id="token-49-3" morph="none" pos="word" start_char="4715">this</TOKEN>
<TOKEN end_char="4735" id="token-49-4" morph="none" pos="word" start_char="4720">counterintuitive</TOKEN>
<TOKEN end_char="4740" id="token-49-5" morph="none" pos="word" start_char="4737">fact</TOKEN>
<TOKEN end_char="4741" id="token-49-6" morph="none" pos="punct" start_char="4741">:</TOKEN>
<TOKEN end_char="4745" id="token-49-7" morph="none" pos="word" start_char="4743">The</TOKEN>
<TOKEN end_char="4754" id="token-49-8" morph="none" pos="word" start_char="4747">validity</TOKEN>
<TOKEN end_char="4757" id="token-49-9" morph="none" pos="word" start_char="4756">of</TOKEN>
<TOKEN end_char="4759" id="token-49-10" morph="none" pos="word" start_char="4759">a</TOKEN>
<TOKEN end_char="4764" id="token-49-11" morph="none" pos="word" start_char="4761">test</TOKEN>
<TOKEN end_char="4772" id="token-49-12" morph="none" pos="word" start_char="4766">depends</TOKEN>
<TOKEN end_char="4776" id="token-49-13" morph="none" pos="word" start_char="4774">not</TOKEN>
<TOKEN end_char="4781" id="token-49-14" morph="none" pos="word" start_char="4778">only</TOKEN>
<TOKEN end_char="4784" id="token-49-15" morph="none" pos="word" start_char="4783">on</TOKEN>
<TOKEN end_char="4788" id="token-49-16" morph="none" pos="word" start_char="4786">the</TOKEN>
<TOKEN end_char="4799" id="token-49-17" morph="none" pos="word" start_char="4790">technology</TOKEN>
<TOKEN end_char="4800" id="token-49-18" morph="none" pos="punct" start_char="4800">,</TOKEN>
<TOKEN end_char="4804" id="token-49-19" morph="none" pos="word" start_char="4802">but</TOKEN>
<TOKEN end_char="4808" id="token-49-20" morph="none" pos="word" start_char="4806">how</TOKEN>
<TOKEN end_char="4815" id="token-49-21" morph="none" pos="word" start_char="4810">common</TOKEN>
<TOKEN end_char="4819" id="token-49-22" morph="none" pos="word" start_char="4817">the</TOKEN>
<TOKEN end_char="4827" id="token-49-23" morph="none" pos="word" start_char="4821">disease</TOKEN>
<TOKEN end_char="4830" id="token-49-24" morph="none" pos="word" start_char="4829">is</TOKEN>
<TOKEN end_char="4833" id="token-49-25" morph="none" pos="word" start_char="4832">in</TOKEN>
<TOKEN end_char="4837" id="token-49-26" morph="none" pos="word" start_char="4835">the</TOKEN>
<TOKEN end_char="4848" id="token-49-27" morph="none" pos="word" start_char="4839">population</TOKEN>
<TOKEN end_char="4855" id="token-49-28" morph="none" pos="word" start_char="4850">you're</TOKEN>
<TOKEN end_char="4864" id="token-49-29" morph="none" pos="word" start_char="4857">sampling</TOKEN>
<TOKEN end_char="4865" id="token-49-30" morph="none" pos="punct" start_char="4865">.</TOKEN>
</SEG>
<SEG end_char="4913" id="segment-50" start_char="4868">
<ORIGINAL_TEXT>"It is kind of a strange thing," admits Dr. H.</ORIGINAL_TEXT>
<TOKEN end_char="4868" id="token-50-0" morph="none" pos="punct" start_char="4868">"</TOKEN>
<TOKEN end_char="4870" id="token-50-1" morph="none" pos="word" start_char="4869">It</TOKEN>
<TOKEN end_char="4873" id="token-50-2" morph="none" pos="word" start_char="4872">is</TOKEN>
<TOKEN end_char="4878" id="token-50-3" morph="none" pos="word" start_char="4875">kind</TOKEN>
<TOKEN end_char="4881" id="token-50-4" morph="none" pos="word" start_char="4880">of</TOKEN>
<TOKEN end_char="4883" id="token-50-5" morph="none" pos="word" start_char="4883">a</TOKEN>
<TOKEN end_char="4891" id="token-50-6" morph="none" pos="word" start_char="4885">strange</TOKEN>
<TOKEN end_char="4897" id="token-50-7" morph="none" pos="word" start_char="4893">thing</TOKEN>
<TOKEN end_char="4899" id="token-50-8" morph="none" pos="punct" start_char="4898">,"</TOKEN>
<TOKEN end_char="4906" id="token-50-9" morph="none" pos="word" start_char="4901">admits</TOKEN>
<TOKEN end_char="4909" id="token-50-10" morph="none" pos="word" start_char="4908">Dr</TOKEN>
<TOKEN end_char="4910" id="token-50-11" morph="none" pos="punct" start_char="4910">.</TOKEN>
<TOKEN end_char="4912" id="token-50-12" morph="none" pos="word" start_char="4912">H</TOKEN>
<TOKEN end_char="4913" id="token-50-13" morph="none" pos="punct" start_char="4913">.</TOKEN>
</SEG>
<SEG end_char="5034" id="segment-51" start_char="4915">
<ORIGINAL_TEXT>Gilbert Welch, a scientist at Brigham and Women's Hospital in Boston who studies issues surrounding tests and screening.</ORIGINAL_TEXT>
<TOKEN end_char="4921" id="token-51-0" morph="none" pos="word" start_char="4915">Gilbert</TOKEN>
<TOKEN end_char="4927" id="token-51-1" morph="none" pos="word" start_char="4923">Welch</TOKEN>
<TOKEN end_char="4928" id="token-51-2" morph="none" pos="punct" start_char="4928">,</TOKEN>
<TOKEN end_char="4930" id="token-51-3" morph="none" pos="word" start_char="4930">a</TOKEN>
<TOKEN end_char="4940" id="token-51-4" morph="none" pos="word" start_char="4932">scientist</TOKEN>
<TOKEN end_char="4943" id="token-51-5" morph="none" pos="word" start_char="4942">at</TOKEN>
<TOKEN end_char="4951" id="token-51-6" morph="none" pos="word" start_char="4945">Brigham</TOKEN>
<TOKEN end_char="4955" id="token-51-7" morph="none" pos="word" start_char="4953">and</TOKEN>
<TOKEN end_char="4963" id="token-51-8" morph="none" pos="word" start_char="4957">Women's</TOKEN>
<TOKEN end_char="4972" id="token-51-9" morph="none" pos="word" start_char="4965">Hospital</TOKEN>
<TOKEN end_char="4975" id="token-51-10" morph="none" pos="word" start_char="4974">in</TOKEN>
<TOKEN end_char="4982" id="token-51-11" morph="none" pos="word" start_char="4977">Boston</TOKEN>
<TOKEN end_char="4986" id="token-51-12" morph="none" pos="word" start_char="4984">who</TOKEN>
<TOKEN end_char="4994" id="token-51-13" morph="none" pos="word" start_char="4988">studies</TOKEN>
<TOKEN end_char="5001" id="token-51-14" morph="none" pos="word" start_char="4996">issues</TOKEN>
<TOKEN end_char="5013" id="token-51-15" morph="none" pos="word" start_char="5003">surrounding</TOKEN>
<TOKEN end_char="5019" id="token-51-16" morph="none" pos="word" start_char="5015">tests</TOKEN>
<TOKEN end_char="5023" id="token-51-17" morph="none" pos="word" start_char="5021">and</TOKEN>
<TOKEN end_char="5033" id="token-51-18" morph="none" pos="word" start_char="5025">screening</TOKEN>
<TOKEN end_char="5034" id="token-51-19" morph="none" pos="punct" start_char="5034">.</TOKEN>
</SEG>
<SEG end_char="5134" id="segment-52" start_char="5036">
<ORIGINAL_TEXT>"An antibody test is much more likely to be wrong in a population with very little COVID exposure."</ORIGINAL_TEXT>
<TOKEN end_char="5036" id="token-52-0" morph="none" pos="punct" start_char="5036">"</TOKEN>
<TOKEN end_char="5038" id="token-52-1" morph="none" pos="word" start_char="5037">An</TOKEN>
<TOKEN end_char="5047" id="token-52-2" morph="none" pos="word" start_char="5040">antibody</TOKEN>
<TOKEN end_char="5052" id="token-52-3" morph="none" pos="word" start_char="5049">test</TOKEN>
<TOKEN end_char="5055" id="token-52-4" morph="none" pos="word" start_char="5054">is</TOKEN>
<TOKEN end_char="5060" id="token-52-5" morph="none" pos="word" start_char="5057">much</TOKEN>
<TOKEN end_char="5065" id="token-52-6" morph="none" pos="word" start_char="5062">more</TOKEN>
<TOKEN end_char="5072" id="token-52-7" morph="none" pos="word" start_char="5067">likely</TOKEN>
<TOKEN end_char="5075" id="token-52-8" morph="none" pos="word" start_char="5074">to</TOKEN>
<TOKEN end_char="5078" id="token-52-9" morph="none" pos="word" start_char="5077">be</TOKEN>
<TOKEN end_char="5084" id="token-52-10" morph="none" pos="word" start_char="5080">wrong</TOKEN>
<TOKEN end_char="5087" id="token-52-11" morph="none" pos="word" start_char="5086">in</TOKEN>
<TOKEN end_char="5089" id="token-52-12" morph="none" pos="word" start_char="5089">a</TOKEN>
<TOKEN end_char="5100" id="token-52-13" morph="none" pos="word" start_char="5091">population</TOKEN>
<TOKEN end_char="5105" id="token-52-14" morph="none" pos="word" start_char="5102">with</TOKEN>
<TOKEN end_char="5110" id="token-52-15" morph="none" pos="word" start_char="5107">very</TOKEN>
<TOKEN end_char="5117" id="token-52-16" morph="none" pos="word" start_char="5112">little</TOKEN>
<TOKEN end_char="5123" id="token-52-17" morph="none" pos="word" start_char="5119">COVID</TOKEN>
<TOKEN end_char="5132" id="token-52-18" morph="none" pos="word" start_char="5125">exposure</TOKEN>
<TOKEN end_char="5134" id="token-52-19" morph="none" pos="punct" start_char="5133">."</TOKEN>
</SEG>
<SEG end_char="5213" id="segment-53" start_char="5137">
<ORIGINAL_TEXT>This is a result of statistics, rather than the technology of any given test.</ORIGINAL_TEXT>
<TOKEN end_char="5140" id="token-53-0" morph="none" pos="word" start_char="5137">This</TOKEN>
<TOKEN end_char="5143" id="token-53-1" morph="none" pos="word" start_char="5142">is</TOKEN>
<TOKEN end_char="5145" id="token-53-2" morph="none" pos="word" start_char="5145">a</TOKEN>
<TOKEN end_char="5152" id="token-53-3" morph="none" pos="word" start_char="5147">result</TOKEN>
<TOKEN end_char="5155" id="token-53-4" morph="none" pos="word" start_char="5154">of</TOKEN>
<TOKEN end_char="5166" id="token-53-5" morph="none" pos="word" start_char="5157">statistics</TOKEN>
<TOKEN end_char="5167" id="token-53-6" morph="none" pos="punct" start_char="5167">,</TOKEN>
<TOKEN end_char="5174" id="token-53-7" morph="none" pos="word" start_char="5169">rather</TOKEN>
<TOKEN end_char="5179" id="token-53-8" morph="none" pos="word" start_char="5176">than</TOKEN>
<TOKEN end_char="5183" id="token-53-9" morph="none" pos="word" start_char="5181">the</TOKEN>
<TOKEN end_char="5194" id="token-53-10" morph="none" pos="word" start_char="5185">technology</TOKEN>
<TOKEN end_char="5197" id="token-53-11" morph="none" pos="word" start_char="5196">of</TOKEN>
<TOKEN end_char="5201" id="token-53-12" morph="none" pos="word" start_char="5199">any</TOKEN>
<TOKEN end_char="5207" id="token-53-13" morph="none" pos="word" start_char="5203">given</TOKEN>
<TOKEN end_char="5212" id="token-53-14" morph="none" pos="word" start_char="5209">test</TOKEN>
<TOKEN end_char="5213" id="token-53-15" morph="none" pos="punct" start_char="5213">.</TOKEN>
</SEG>
<SEG end_char="5249" id="segment-54" start_char="5216">
<ORIGINAL_TEXT>Here's a simple way to look at it.</ORIGINAL_TEXT>
<TOKEN end_char="5221" id="token-54-0" morph="none" pos="word" start_char="5216">Here's</TOKEN>
<TOKEN end_char="5223" id="token-54-1" morph="none" pos="word" start_char="5223">a</TOKEN>
<TOKEN end_char="5230" id="token-54-2" morph="none" pos="word" start_char="5225">simple</TOKEN>
<TOKEN end_char="5234" id="token-54-3" morph="none" pos="word" start_char="5232">way</TOKEN>
<TOKEN end_char="5237" id="token-54-4" morph="none" pos="word" start_char="5236">to</TOKEN>
<TOKEN end_char="5242" id="token-54-5" morph="none" pos="word" start_char="5239">look</TOKEN>
<TOKEN end_char="5245" id="token-54-6" morph="none" pos="word" start_char="5244">at</TOKEN>
<TOKEN end_char="5248" id="token-54-7" morph="none" pos="word" start_char="5247">it</TOKEN>
<TOKEN end_char="5249" id="token-54-8" morph="none" pos="punct" start_char="5249">.</TOKEN>
</SEG>
<SEG end_char="5343" id="segment-55" start_char="5251">
<ORIGINAL_TEXT>Say you are running a test that gives five falsely positive results in a group of 100 people.</ORIGINAL_TEXT>
<TOKEN end_char="5253" id="token-55-0" morph="none" pos="word" start_char="5251">Say</TOKEN>
<TOKEN end_char="5257" id="token-55-1" morph="none" pos="word" start_char="5255">you</TOKEN>
<TOKEN end_char="5261" id="token-55-2" morph="none" pos="word" start_char="5259">are</TOKEN>
<TOKEN end_char="5269" id="token-55-3" morph="none" pos="word" start_char="5263">running</TOKEN>
<TOKEN end_char="5271" id="token-55-4" morph="none" pos="word" start_char="5271">a</TOKEN>
<TOKEN end_char="5276" id="token-55-5" morph="none" pos="word" start_char="5273">test</TOKEN>
<TOKEN end_char="5281" id="token-55-6" morph="none" pos="word" start_char="5278">that</TOKEN>
<TOKEN end_char="5287" id="token-55-7" morph="none" pos="word" start_char="5283">gives</TOKEN>
<TOKEN end_char="5292" id="token-55-8" morph="none" pos="word" start_char="5289">five</TOKEN>
<TOKEN end_char="5300" id="token-55-9" morph="none" pos="word" start_char="5294">falsely</TOKEN>
<TOKEN end_char="5309" id="token-55-10" morph="none" pos="word" start_char="5302">positive</TOKEN>
<TOKEN end_char="5317" id="token-55-11" morph="none" pos="word" start_char="5311">results</TOKEN>
<TOKEN end_char="5320" id="token-55-12" morph="none" pos="word" start_char="5319">in</TOKEN>
<TOKEN end_char="5322" id="token-55-13" morph="none" pos="word" start_char="5322">a</TOKEN>
<TOKEN end_char="5328" id="token-55-14" morph="none" pos="word" start_char="5324">group</TOKEN>
<TOKEN end_char="5331" id="token-55-15" morph="none" pos="word" start_char="5330">of</TOKEN>
<TOKEN end_char="5335" id="token-55-16" morph="none" pos="word" start_char="5333">100</TOKEN>
<TOKEN end_char="5342" id="token-55-17" morph="none" pos="word" start_char="5337">people</TOKEN>
<TOKEN end_char="5343" id="token-55-18" morph="none" pos="punct" start_char="5343">.</TOKEN>
</SEG>
<SEG end_char="5371" id="segment-56" start_char="5345">
<ORIGINAL_TEXT>That doesn't sound too bad.</ORIGINAL_TEXT>
<TOKEN end_char="5348" id="token-56-0" morph="none" pos="word" start_char="5345">That</TOKEN>
<TOKEN end_char="5356" id="token-56-1" morph="none" pos="word" start_char="5350">doesn't</TOKEN>
<TOKEN end_char="5362" id="token-56-2" morph="none" pos="word" start_char="5358">sound</TOKEN>
<TOKEN end_char="5366" id="token-56-3" morph="none" pos="word" start_char="5364">too</TOKEN>
<TOKEN end_char="5370" id="token-56-4" morph="none" pos="word" start_char="5368">bad</TOKEN>
<TOKEN end_char="5371" id="token-56-5" morph="none" pos="punct" start_char="5371">.</TOKEN>
</SEG>
<SEG end_char="5390" id="segment-57" start_char="5373">
<ORIGINAL_TEXT>But consider this.</ORIGINAL_TEXT>
<TOKEN end_char="5375" id="token-57-0" morph="none" pos="word" start_char="5373">But</TOKEN>
<TOKEN end_char="5384" id="token-57-1" morph="none" pos="word" start_char="5377">consider</TOKEN>
<TOKEN end_char="5389" id="token-57-2" morph="none" pos="word" start_char="5386">this</TOKEN>
<TOKEN end_char="5390" id="token-57-3" morph="none" pos="punct" start_char="5390">.</TOKEN>
</SEG>
<SEG end_char="5564" id="segment-58" start_char="5392">
<ORIGINAL_TEXT>If 5% of those 100 people were actually infected with the coronavirus, you should get five correct test results (true positives), along with the five false positive results.</ORIGINAL_TEXT>
<TOKEN end_char="5393" id="token-58-0" morph="none" pos="word" start_char="5392">If</TOKEN>
<TOKEN end_char="5395" id="token-58-1" morph="none" pos="word" start_char="5395">5</TOKEN>
<TOKEN end_char="5396" id="token-58-2" morph="none" pos="punct" start_char="5396">%</TOKEN>
<TOKEN end_char="5399" id="token-58-3" morph="none" pos="word" start_char="5398">of</TOKEN>
<TOKEN end_char="5405" id="token-58-4" morph="none" pos="word" start_char="5401">those</TOKEN>
<TOKEN end_char="5409" id="token-58-5" morph="none" pos="word" start_char="5407">100</TOKEN>
<TOKEN end_char="5416" id="token-58-6" morph="none" pos="word" start_char="5411">people</TOKEN>
<TOKEN end_char="5421" id="token-58-7" morph="none" pos="word" start_char="5418">were</TOKEN>
<TOKEN end_char="5430" id="token-58-8" morph="none" pos="word" start_char="5423">actually</TOKEN>
<TOKEN end_char="5439" id="token-58-9" morph="none" pos="word" start_char="5432">infected</TOKEN>
<TOKEN end_char="5444" id="token-58-10" morph="none" pos="word" start_char="5441">with</TOKEN>
<TOKEN end_char="5448" id="token-58-11" morph="none" pos="word" start_char="5446">the</TOKEN>
<TOKEN end_char="5460" id="token-58-12" morph="none" pos="word" start_char="5450">coronavirus</TOKEN>
<TOKEN end_char="5461" id="token-58-13" morph="none" pos="punct" start_char="5461">,</TOKEN>
<TOKEN end_char="5465" id="token-58-14" morph="none" pos="word" start_char="5463">you</TOKEN>
<TOKEN end_char="5472" id="token-58-15" morph="none" pos="word" start_char="5467">should</TOKEN>
<TOKEN end_char="5476" id="token-58-16" morph="none" pos="word" start_char="5474">get</TOKEN>
<TOKEN end_char="5481" id="token-58-17" morph="none" pos="word" start_char="5478">five</TOKEN>
<TOKEN end_char="5489" id="token-58-18" morph="none" pos="word" start_char="5483">correct</TOKEN>
<TOKEN end_char="5494" id="token-58-19" morph="none" pos="word" start_char="5491">test</TOKEN>
<TOKEN end_char="5502" id="token-58-20" morph="none" pos="word" start_char="5496">results</TOKEN>
<TOKEN end_char="5504" id="token-58-21" morph="none" pos="punct" start_char="5504">(</TOKEN>
<TOKEN end_char="5508" id="token-58-22" morph="none" pos="word" start_char="5505">true</TOKEN>
<TOKEN end_char="5518" id="token-58-23" morph="none" pos="word" start_char="5510">positives</TOKEN>
<TOKEN end_char="5520" id="token-58-24" morph="none" pos="punct" start_char="5519">),</TOKEN>
<TOKEN end_char="5526" id="token-58-25" morph="none" pos="word" start_char="5522">along</TOKEN>
<TOKEN end_char="5531" id="token-58-26" morph="none" pos="word" start_char="5528">with</TOKEN>
<TOKEN end_char="5535" id="token-58-27" morph="none" pos="word" start_char="5533">the</TOKEN>
<TOKEN end_char="5540" id="token-58-28" morph="none" pos="word" start_char="5537">five</TOKEN>
<TOKEN end_char="5546" id="token-58-29" morph="none" pos="word" start_char="5542">false</TOKEN>
<TOKEN end_char="5555" id="token-58-30" morph="none" pos="word" start_char="5548">positive</TOKEN>
<TOKEN end_char="5563" id="token-58-31" morph="none" pos="word" start_char="5557">results</TOKEN>
<TOKEN end_char="5564" id="token-58-32" morph="none" pos="punct" start_char="5564">.</TOKEN>
</SEG>
<SEG end_char="5703" id="segment-59" start_char="5567">
<ORIGINAL_TEXT>While the manufacturer can rightly claim the test is 95% specific, in this population "the test will be wrong half the time," Welch says.</ORIGINAL_TEXT>
<TOKEN end_char="5571" id="token-59-0" morph="none" pos="word" start_char="5567">While</TOKEN>
<TOKEN end_char="5575" id="token-59-1" morph="none" pos="word" start_char="5573">the</TOKEN>
<TOKEN end_char="5588" id="token-59-2" morph="none" pos="word" start_char="5577">manufacturer</TOKEN>
<TOKEN end_char="5592" id="token-59-3" morph="none" pos="word" start_char="5590">can</TOKEN>
<TOKEN end_char="5600" id="token-59-4" morph="none" pos="word" start_char="5594">rightly</TOKEN>
<TOKEN end_char="5606" id="token-59-5" morph="none" pos="word" start_char="5602">claim</TOKEN>
<TOKEN end_char="5610" id="token-59-6" morph="none" pos="word" start_char="5608">the</TOKEN>
<TOKEN end_char="5615" id="token-59-7" morph="none" pos="word" start_char="5612">test</TOKEN>
<TOKEN end_char="5618" id="token-59-8" morph="none" pos="word" start_char="5617">is</TOKEN>
<TOKEN end_char="5621" id="token-59-9" morph="none" pos="word" start_char="5620">95</TOKEN>
<TOKEN end_char="5622" id="token-59-10" morph="none" pos="punct" start_char="5622">%</TOKEN>
<TOKEN end_char="5631" id="token-59-11" morph="none" pos="word" start_char="5624">specific</TOKEN>
<TOKEN end_char="5632" id="token-59-12" morph="none" pos="punct" start_char="5632">,</TOKEN>
<TOKEN end_char="5635" id="token-59-13" morph="none" pos="word" start_char="5634">in</TOKEN>
<TOKEN end_char="5640" id="token-59-14" morph="none" pos="word" start_char="5637">this</TOKEN>
<TOKEN end_char="5651" id="token-59-15" morph="none" pos="word" start_char="5642">population</TOKEN>
<TOKEN end_char="5653" id="token-59-16" morph="none" pos="punct" start_char="5653">"</TOKEN>
<TOKEN end_char="5656" id="token-59-17" morph="none" pos="word" start_char="5654">the</TOKEN>
<TOKEN end_char="5661" id="token-59-18" morph="none" pos="word" start_char="5658">test</TOKEN>
<TOKEN end_char="5666" id="token-59-19" morph="none" pos="word" start_char="5663">will</TOKEN>
<TOKEN end_char="5669" id="token-59-20" morph="none" pos="word" start_char="5668">be</TOKEN>
<TOKEN end_char="5675" id="token-59-21" morph="none" pos="word" start_char="5671">wrong</TOKEN>
<TOKEN end_char="5680" id="token-59-22" morph="none" pos="word" start_char="5677">half</TOKEN>
<TOKEN end_char="5684" id="token-59-23" morph="none" pos="word" start_char="5682">the</TOKEN>
<TOKEN end_char="5689" id="token-59-24" morph="none" pos="word" start_char="5686">time</TOKEN>
<TOKEN end_char="5691" id="token-59-25" morph="none" pos="punct" start_char="5690">,"</TOKEN>
<TOKEN end_char="5697" id="token-59-26" morph="none" pos="word" start_char="5693">Welch</TOKEN>
<TOKEN end_char="5702" id="token-59-27" morph="none" pos="word" start_char="5699">says</TOKEN>
<TOKEN end_char="5703" id="token-59-28" morph="none" pos="punct" start_char="5703">.</TOKEN>
</SEG>
<SEG end_char="5748" id="segment-60" start_char="5705">
<ORIGINAL_TEXT>"Half the people will be falsely reassured."</ORIGINAL_TEXT>
<TOKEN end_char="5705" id="token-60-0" morph="none" pos="punct" start_char="5705">"</TOKEN>
<TOKEN end_char="5709" id="token-60-1" morph="none" pos="word" start_char="5706">Half</TOKEN>
<TOKEN end_char="5713" id="token-60-2" morph="none" pos="word" start_char="5711">the</TOKEN>
<TOKEN end_char="5720" id="token-60-3" morph="none" pos="word" start_char="5715">people</TOKEN>
<TOKEN end_char="5725" id="token-60-4" morph="none" pos="word" start_char="5722">will</TOKEN>
<TOKEN end_char="5728" id="token-60-5" morph="none" pos="word" start_char="5727">be</TOKEN>
<TOKEN end_char="5736" id="token-60-6" morph="none" pos="word" start_char="5730">falsely</TOKEN>
<TOKEN end_char="5746" id="token-60-7" morph="none" pos="word" start_char="5738">reassured</TOKEN>
<TOKEN end_char="5748" id="token-60-8" morph="none" pos="punct" start_char="5747">."</TOKEN>
</SEG>
<SEG end_char="5793" id="segment-61" start_char="5751">
<ORIGINAL_TEXT>And test results can be considerably worse.</ORIGINAL_TEXT>
<TOKEN end_char="5753" id="token-61-0" morph="none" pos="word" start_char="5751">And</TOKEN>
<TOKEN end_char="5758" id="token-61-1" morph="none" pos="word" start_char="5755">test</TOKEN>
<TOKEN end_char="5766" id="token-61-2" morph="none" pos="word" start_char="5760">results</TOKEN>
<TOKEN end_char="5770" id="token-61-3" morph="none" pos="word" start_char="5768">can</TOKEN>
<TOKEN end_char="5773" id="token-61-4" morph="none" pos="word" start_char="5772">be</TOKEN>
<TOKEN end_char="5786" id="token-61-5" morph="none" pos="word" start_char="5775">considerably</TOKEN>
<TOKEN end_char="5792" id="token-61-6" morph="none" pos="word" start_char="5788">worse</TOKEN>
<TOKEN end_char="5793" id="token-61-7" morph="none" pos="punct" start_char="5793">.</TOKEN>
</SEG>
<SEG end_char="6105" id="segment-62" start_char="5796">
<ORIGINAL_TEXT>The Food and Drug Administration does not regulate these tests, but White House coronavirus task force coordinator Dr. Deborah Birx has said that she expects manufacturers to achieve a standard of 90% specificity (and 90% sensitivity, another measure of test performance that's less important in this context).</ORIGINAL_TEXT>
<TOKEN end_char="5798" id="token-62-0" morph="none" pos="word" start_char="5796">The</TOKEN>
<TOKEN end_char="5803" id="token-62-1" morph="none" pos="word" start_char="5800">Food</TOKEN>
<TOKEN end_char="5807" id="token-62-2" morph="none" pos="word" start_char="5805">and</TOKEN>
<TOKEN end_char="5812" id="token-62-3" morph="none" pos="word" start_char="5809">Drug</TOKEN>
<TOKEN end_char="5827" id="token-62-4" morph="none" pos="word" start_char="5814">Administration</TOKEN>
<TOKEN end_char="5832" id="token-62-5" morph="none" pos="word" start_char="5829">does</TOKEN>
<TOKEN end_char="5836" id="token-62-6" morph="none" pos="word" start_char="5834">not</TOKEN>
<TOKEN end_char="5845" id="token-62-7" morph="none" pos="word" start_char="5838">regulate</TOKEN>
<TOKEN end_char="5851" id="token-62-8" morph="none" pos="word" start_char="5847">these</TOKEN>
<TOKEN end_char="5857" id="token-62-9" morph="none" pos="word" start_char="5853">tests</TOKEN>
<TOKEN end_char="5858" id="token-62-10" morph="none" pos="punct" start_char="5858">,</TOKEN>
<TOKEN end_char="5862" id="token-62-11" morph="none" pos="word" start_char="5860">but</TOKEN>
<TOKEN end_char="5868" id="token-62-12" morph="none" pos="word" start_char="5864">White</TOKEN>
<TOKEN end_char="5874" id="token-62-13" morph="none" pos="word" start_char="5870">House</TOKEN>
<TOKEN end_char="5886" id="token-62-14" morph="none" pos="word" start_char="5876">coronavirus</TOKEN>
<TOKEN end_char="5891" id="token-62-15" morph="none" pos="word" start_char="5888">task</TOKEN>
<TOKEN end_char="5897" id="token-62-16" morph="none" pos="word" start_char="5893">force</TOKEN>
<TOKEN end_char="5909" id="token-62-17" morph="none" pos="word" start_char="5899">coordinator</TOKEN>
<TOKEN end_char="5912" id="token-62-18" morph="none" pos="word" start_char="5911">Dr</TOKEN>
<TOKEN end_char="5913" id="token-62-19" morph="none" pos="punct" start_char="5913">.</TOKEN>
<TOKEN end_char="5921" id="token-62-20" morph="none" pos="word" start_char="5915">Deborah</TOKEN>
<TOKEN end_char="5926" id="token-62-21" morph="none" pos="word" start_char="5923">Birx</TOKEN>
<TOKEN end_char="5930" id="token-62-22" morph="none" pos="word" start_char="5928">has</TOKEN>
<TOKEN end_char="5935" id="token-62-23" morph="none" pos="word" start_char="5932">said</TOKEN>
<TOKEN end_char="5940" id="token-62-24" morph="none" pos="word" start_char="5937">that</TOKEN>
<TOKEN end_char="5944" id="token-62-25" morph="none" pos="word" start_char="5942">she</TOKEN>
<TOKEN end_char="5952" id="token-62-26" morph="none" pos="word" start_char="5946">expects</TOKEN>
<TOKEN end_char="5966" id="token-62-27" morph="none" pos="word" start_char="5954">manufacturers</TOKEN>
<TOKEN end_char="5969" id="token-62-28" morph="none" pos="word" start_char="5968">to</TOKEN>
<TOKEN end_char="5977" id="token-62-29" morph="none" pos="word" start_char="5971">achieve</TOKEN>
<TOKEN end_char="5979" id="token-62-30" morph="none" pos="word" start_char="5979">a</TOKEN>
<TOKEN end_char="5988" id="token-62-31" morph="none" pos="word" start_char="5981">standard</TOKEN>
<TOKEN end_char="5991" id="token-62-32" morph="none" pos="word" start_char="5990">of</TOKEN>
<TOKEN end_char="5994" id="token-62-33" morph="none" pos="word" start_char="5993">90</TOKEN>
<TOKEN end_char="5995" id="token-62-34" morph="none" pos="punct" start_char="5995">%</TOKEN>
<TOKEN end_char="6007" id="token-62-35" morph="none" pos="word" start_char="5997">specificity</TOKEN>
<TOKEN end_char="6009" id="token-62-36" morph="none" pos="punct" start_char="6009">(</TOKEN>
<TOKEN end_char="6012" id="token-62-37" morph="none" pos="word" start_char="6010">and</TOKEN>
<TOKEN end_char="6015" id="token-62-38" morph="none" pos="word" start_char="6014">90</TOKEN>
<TOKEN end_char="6016" id="token-62-39" morph="none" pos="punct" start_char="6016">%</TOKEN>
<TOKEN end_char="6028" id="token-62-40" morph="none" pos="word" start_char="6018">sensitivity</TOKEN>
<TOKEN end_char="6029" id="token-62-41" morph="none" pos="punct" start_char="6029">,</TOKEN>
<TOKEN end_char="6037" id="token-62-42" morph="none" pos="word" start_char="6031">another</TOKEN>
<TOKEN end_char="6045" id="token-62-43" morph="none" pos="word" start_char="6039">measure</TOKEN>
<TOKEN end_char="6048" id="token-62-44" morph="none" pos="word" start_char="6047">of</TOKEN>
<TOKEN end_char="6053" id="token-62-45" morph="none" pos="word" start_char="6050">test</TOKEN>
<TOKEN end_char="6065" id="token-62-46" morph="none" pos="word" start_char="6055">performance</TOKEN>
<TOKEN end_char="6072" id="token-62-47" morph="none" pos="word" start_char="6067">that's</TOKEN>
<TOKEN end_char="6077" id="token-62-48" morph="none" pos="word" start_char="6074">less</TOKEN>
<TOKEN end_char="6087" id="token-62-49" morph="none" pos="word" start_char="6079">important</TOKEN>
<TOKEN end_char="6090" id="token-62-50" morph="none" pos="word" start_char="6089">in</TOKEN>
<TOKEN end_char="6095" id="token-62-51" morph="none" pos="word" start_char="6092">this</TOKEN>
<TOKEN end_char="6103" id="token-62-52" morph="none" pos="word" start_char="6097">context</TOKEN>
<TOKEN end_char="6105" id="token-62-53" morph="none" pos="punct" start_char="6104">).</TOKEN>
</SEG>
<SEG end_char="6236" id="segment-63" start_char="6108">
<ORIGINAL_TEXT>Here's what would happen if you used a test with 90% specificity in a population in which only 1% of the people have coronavirus.</ORIGINAL_TEXT>
<TOKEN end_char="6113" id="token-63-0" morph="none" pos="word" start_char="6108">Here's</TOKEN>
<TOKEN end_char="6118" id="token-63-1" morph="none" pos="word" start_char="6115">what</TOKEN>
<TOKEN end_char="6124" id="token-63-2" morph="none" pos="word" start_char="6120">would</TOKEN>
<TOKEN end_char="6131" id="token-63-3" morph="none" pos="word" start_char="6126">happen</TOKEN>
<TOKEN end_char="6134" id="token-63-4" morph="none" pos="word" start_char="6133">if</TOKEN>
<TOKEN end_char="6138" id="token-63-5" morph="none" pos="word" start_char="6136">you</TOKEN>
<TOKEN end_char="6143" id="token-63-6" morph="none" pos="word" start_char="6140">used</TOKEN>
<TOKEN end_char="6145" id="token-63-7" morph="none" pos="word" start_char="6145">a</TOKEN>
<TOKEN end_char="6150" id="token-63-8" morph="none" pos="word" start_char="6147">test</TOKEN>
<TOKEN end_char="6155" id="token-63-9" morph="none" pos="word" start_char="6152">with</TOKEN>
<TOKEN end_char="6158" id="token-63-10" morph="none" pos="word" start_char="6157">90</TOKEN>
<TOKEN end_char="6159" id="token-63-11" morph="none" pos="punct" start_char="6159">%</TOKEN>
<TOKEN end_char="6171" id="token-63-12" morph="none" pos="word" start_char="6161">specificity</TOKEN>
<TOKEN end_char="6174" id="token-63-13" morph="none" pos="word" start_char="6173">in</TOKEN>
<TOKEN end_char="6176" id="token-63-14" morph="none" pos="word" start_char="6176">a</TOKEN>
<TOKEN end_char="6187" id="token-63-15" morph="none" pos="word" start_char="6178">population</TOKEN>
<TOKEN end_char="6190" id="token-63-16" morph="none" pos="word" start_char="6189">in</TOKEN>
<TOKEN end_char="6196" id="token-63-17" morph="none" pos="word" start_char="6192">which</TOKEN>
<TOKEN end_char="6201" id="token-63-18" morph="none" pos="word" start_char="6198">only</TOKEN>
<TOKEN end_char="6203" id="token-63-19" morph="none" pos="word" start_char="6203">1</TOKEN>
<TOKEN end_char="6204" id="token-63-20" morph="none" pos="punct" start_char="6204">%</TOKEN>
<TOKEN end_char="6207" id="token-63-21" morph="none" pos="word" start_char="6206">of</TOKEN>
<TOKEN end_char="6211" id="token-63-22" morph="none" pos="word" start_char="6209">the</TOKEN>
<TOKEN end_char="6218" id="token-63-23" morph="none" pos="word" start_char="6213">people</TOKEN>
<TOKEN end_char="6223" id="token-63-24" morph="none" pos="word" start_char="6220">have</TOKEN>
<TOKEN end_char="6235" id="token-63-25" morph="none" pos="word" start_char="6225">coronavirus</TOKEN>
<TOKEN end_char="6236" id="token-63-26" morph="none" pos="punct" start_char="6236">.</TOKEN>
</SEG>
<SEG end_char="6321" id="segment-64" start_char="6238">
<ORIGINAL_TEXT>Nobody knows for sure, but that could be the situation in many parts of the country.</ORIGINAL_TEXT>
<TOKEN end_char="6243" id="token-64-0" morph="none" pos="word" start_char="6238">Nobody</TOKEN>
<TOKEN end_char="6249" id="token-64-1" morph="none" pos="word" start_char="6245">knows</TOKEN>
<TOKEN end_char="6253" id="token-64-2" morph="none" pos="word" start_char="6251">for</TOKEN>
<TOKEN end_char="6258" id="token-64-3" morph="none" pos="word" start_char="6255">sure</TOKEN>
<TOKEN end_char="6259" id="token-64-4" morph="none" pos="punct" start_char="6259">,</TOKEN>
<TOKEN end_char="6263" id="token-64-5" morph="none" pos="word" start_char="6261">but</TOKEN>
<TOKEN end_char="6268" id="token-64-6" morph="none" pos="word" start_char="6265">that</TOKEN>
<TOKEN end_char="6274" id="token-64-7" morph="none" pos="word" start_char="6270">could</TOKEN>
<TOKEN end_char="6277" id="token-64-8" morph="none" pos="word" start_char="6276">be</TOKEN>
<TOKEN end_char="6281" id="token-64-9" morph="none" pos="word" start_char="6279">the</TOKEN>
<TOKEN end_char="6291" id="token-64-10" morph="none" pos="word" start_char="6283">situation</TOKEN>
<TOKEN end_char="6294" id="token-64-11" morph="none" pos="word" start_char="6293">in</TOKEN>
<TOKEN end_char="6299" id="token-64-12" morph="none" pos="word" start_char="6296">many</TOKEN>
<TOKEN end_char="6305" id="token-64-13" morph="none" pos="word" start_char="6301">parts</TOKEN>
<TOKEN end_char="6308" id="token-64-14" morph="none" pos="word" start_char="6307">of</TOKEN>
<TOKEN end_char="6312" id="token-64-15" morph="none" pos="word" start_char="6310">the</TOKEN>
<TOKEN end_char="6320" id="token-64-16" morph="none" pos="word" start_char="6314">country</TOKEN>
<TOKEN end_char="6321" id="token-64-17" morph="none" pos="punct" start_char="6321">.</TOKEN>
</SEG>
<SEG end_char="6428" id="segment-65" start_char="6324">
<ORIGINAL_TEXT>In that instance, more than 90% of the positive results would be false positives, and falsely reassuring.</ORIGINAL_TEXT>
<TOKEN end_char="6325" id="token-65-0" morph="none" pos="word" start_char="6324">In</TOKEN>
<TOKEN end_char="6330" id="token-65-1" morph="none" pos="word" start_char="6327">that</TOKEN>
<TOKEN end_char="6339" id="token-65-2" morph="none" pos="word" start_char="6332">instance</TOKEN>
<TOKEN end_char="6340" id="token-65-3" morph="none" pos="punct" start_char="6340">,</TOKEN>
<TOKEN end_char="6345" id="token-65-4" morph="none" pos="word" start_char="6342">more</TOKEN>
<TOKEN end_char="6350" id="token-65-5" morph="none" pos="word" start_char="6347">than</TOKEN>
<TOKEN end_char="6353" id="token-65-6" morph="none" pos="word" start_char="6352">90</TOKEN>
<TOKEN end_char="6354" id="token-65-7" morph="none" pos="punct" start_char="6354">%</TOKEN>
<TOKEN end_char="6357" id="token-65-8" morph="none" pos="word" start_char="6356">of</TOKEN>
<TOKEN end_char="6361" id="token-65-9" morph="none" pos="word" start_char="6359">the</TOKEN>
<TOKEN end_char="6370" id="token-65-10" morph="none" pos="word" start_char="6363">positive</TOKEN>
<TOKEN end_char="6378" id="token-65-11" morph="none" pos="word" start_char="6372">results</TOKEN>
<TOKEN end_char="6384" id="token-65-12" morph="none" pos="word" start_char="6380">would</TOKEN>
<TOKEN end_char="6387" id="token-65-13" morph="none" pos="word" start_char="6386">be</TOKEN>
<TOKEN end_char="6393" id="token-65-14" morph="none" pos="word" start_char="6389">false</TOKEN>
<TOKEN end_char="6403" id="token-65-15" morph="none" pos="word" start_char="6395">positives</TOKEN>
<TOKEN end_char="6404" id="token-65-16" morph="none" pos="punct" start_char="6404">,</TOKEN>
<TOKEN end_char="6408" id="token-65-17" morph="none" pos="word" start_char="6406">and</TOKEN>
<TOKEN end_char="6416" id="token-65-18" morph="none" pos="word" start_char="6410">falsely</TOKEN>
<TOKEN end_char="6427" id="token-65-19" morph="none" pos="word" start_char="6418">reassuring</TOKEN>
<TOKEN end_char="6428" id="token-65-20" morph="none" pos="punct" start_char="6428">.</TOKEN>
</SEG>
<SEG end_char="6492" id="segment-66" start_char="6430">
<ORIGINAL_TEXT>(You can run your own examples on the calculator on this page).</ORIGINAL_TEXT>
<TOKEN end_char="6430" id="token-66-0" morph="none" pos="punct" start_char="6430">(</TOKEN>
<TOKEN end_char="6433" id="token-66-1" morph="none" pos="word" start_char="6431">You</TOKEN>
<TOKEN end_char="6437" id="token-66-2" morph="none" pos="word" start_char="6435">can</TOKEN>
<TOKEN end_char="6441" id="token-66-3" morph="none" pos="word" start_char="6439">run</TOKEN>
<TOKEN end_char="6446" id="token-66-4" morph="none" pos="word" start_char="6443">your</TOKEN>
<TOKEN end_char="6450" id="token-66-5" morph="none" pos="word" start_char="6448">own</TOKEN>
<TOKEN end_char="6459" id="token-66-6" morph="none" pos="word" start_char="6452">examples</TOKEN>
<TOKEN end_char="6462" id="token-66-7" morph="none" pos="word" start_char="6461">on</TOKEN>
<TOKEN end_char="6466" id="token-66-8" morph="none" pos="word" start_char="6464">the</TOKEN>
<TOKEN end_char="6477" id="token-66-9" morph="none" pos="word" start_char="6468">calculator</TOKEN>
<TOKEN end_char="6480" id="token-66-10" morph="none" pos="word" start_char="6479">on</TOKEN>
<TOKEN end_char="6485" id="token-66-11" morph="none" pos="word" start_char="6482">this</TOKEN>
<TOKEN end_char="6490" id="token-66-12" morph="none" pos="word" start_char="6487">page</TOKEN>
<TOKEN end_char="6492" id="token-66-13" morph="none" pos="punct" start_char="6491">).</TOKEN>
</SEG>
<SEG end_char="6586" id="segment-67" start_char="6495">
<ORIGINAL_TEXT>Use the sliders to see how prevalence and false positive rates affect antibody test results.</ORIGINAL_TEXT>
<TOKEN end_char="6497" id="token-67-0" morph="none" pos="word" start_char="6495">Use</TOKEN>
<TOKEN end_char="6501" id="token-67-1" morph="none" pos="word" start_char="6499">the</TOKEN>
<TOKEN end_char="6509" id="token-67-2" morph="none" pos="word" start_char="6503">sliders</TOKEN>
<TOKEN end_char="6512" id="token-67-3" morph="none" pos="word" start_char="6511">to</TOKEN>
<TOKEN end_char="6516" id="token-67-4" morph="none" pos="word" start_char="6514">see</TOKEN>
<TOKEN end_char="6520" id="token-67-5" morph="none" pos="word" start_char="6518">how</TOKEN>
<TOKEN end_char="6531" id="token-67-6" morph="none" pos="word" start_char="6522">prevalence</TOKEN>
<TOKEN end_char="6535" id="token-67-7" morph="none" pos="word" start_char="6533">and</TOKEN>
<TOKEN end_char="6541" id="token-67-8" morph="none" pos="word" start_char="6537">false</TOKEN>
<TOKEN end_char="6550" id="token-67-9" morph="none" pos="word" start_char="6543">positive</TOKEN>
<TOKEN end_char="6556" id="token-67-10" morph="none" pos="word" start_char="6552">rates</TOKEN>
<TOKEN end_char="6563" id="token-67-11" morph="none" pos="word" start_char="6558">affect</TOKEN>
<TOKEN end_char="6572" id="token-67-12" morph="none" pos="word" start_char="6565">antibody</TOKEN>
<TOKEN end_char="6577" id="token-67-13" morph="none" pos="word" start_char="6574">test</TOKEN>
<TOKEN end_char="6585" id="token-67-14" morph="none" pos="word" start_char="6579">results</TOKEN>
<TOKEN end_char="6586" id="token-67-15" morph="none" pos="punct" start_char="6586">.</TOKEN>
</SEG>
<SEG end_char="6704" id="segment-68" start_char="6588">
<ORIGINAL_TEXT>Test sensitivity (the ability to correctly identify individuals with the antibody) is set at 90% for this calculator.</ORIGINAL_TEXT>
<TOKEN end_char="6591" id="token-68-0" morph="none" pos="word" start_char="6588">Test</TOKEN>
<TOKEN end_char="6603" id="token-68-1" morph="none" pos="word" start_char="6593">sensitivity</TOKEN>
<TOKEN end_char="6605" id="token-68-2" morph="none" pos="punct" start_char="6605">(</TOKEN>
<TOKEN end_char="6608" id="token-68-3" morph="none" pos="word" start_char="6606">the</TOKEN>
<TOKEN end_char="6616" id="token-68-4" morph="none" pos="word" start_char="6610">ability</TOKEN>
<TOKEN end_char="6619" id="token-68-5" morph="none" pos="word" start_char="6618">to</TOKEN>
<TOKEN end_char="6629" id="token-68-6" morph="none" pos="word" start_char="6621">correctly</TOKEN>
<TOKEN end_char="6638" id="token-68-7" morph="none" pos="word" start_char="6631">identify</TOKEN>
<TOKEN end_char="6650" id="token-68-8" morph="none" pos="word" start_char="6640">individuals</TOKEN>
<TOKEN end_char="6655" id="token-68-9" morph="none" pos="word" start_char="6652">with</TOKEN>
<TOKEN end_char="6659" id="token-68-10" morph="none" pos="word" start_char="6657">the</TOKEN>
<TOKEN end_char="6668" id="token-68-11" morph="none" pos="word" start_char="6661">antibody</TOKEN>
<TOKEN end_char="6669" id="token-68-12" morph="none" pos="punct" start_char="6669">)</TOKEN>
<TOKEN end_char="6672" id="token-68-13" morph="none" pos="word" start_char="6671">is</TOKEN>
<TOKEN end_char="6676" id="token-68-14" morph="none" pos="word" start_char="6674">set</TOKEN>
<TOKEN end_char="6679" id="token-68-15" morph="none" pos="word" start_char="6678">at</TOKEN>
<TOKEN end_char="6682" id="token-68-16" morph="none" pos="word" start_char="6681">90</TOKEN>
<TOKEN end_char="6683" id="token-68-17" morph="none" pos="punct" start_char="6683">%</TOKEN>
<TOKEN end_char="6687" id="token-68-18" morph="none" pos="word" start_char="6685">for</TOKEN>
<TOKEN end_char="6692" id="token-68-19" morph="none" pos="word" start_char="6689">this</TOKEN>
<TOKEN end_char="6703" id="token-68-20" morph="none" pos="word" start_char="6694">calculator</TOKEN>
<TOKEN end_char="6704" id="token-68-21" morph="none" pos="punct" start_char="6704">.</TOKEN>
</SEG>
<SEG end_char="6753" id="segment-69" start_char="6708">
<ORIGINAL_TEXT>Chance that a positive antibody test is wrong:</ORIGINAL_TEXT>
<TOKEN end_char="6713" id="token-69-0" morph="none" pos="word" start_char="6708">Chance</TOKEN>
<TOKEN end_char="6718" id="token-69-1" morph="none" pos="word" start_char="6715">that</TOKEN>
<TOKEN end_char="6720" id="token-69-2" morph="none" pos="word" start_char="6720">a</TOKEN>
<TOKEN end_char="6729" id="token-69-3" morph="none" pos="word" start_char="6722">positive</TOKEN>
<TOKEN end_char="6738" id="token-69-4" morph="none" pos="word" start_char="6731">antibody</TOKEN>
<TOKEN end_char="6743" id="token-69-5" morph="none" pos="word" start_char="6740">test</TOKEN>
<TOKEN end_char="6746" id="token-69-6" morph="none" pos="word" start_char="6745">is</TOKEN>
<TOKEN end_char="6752" id="token-69-7" morph="none" pos="word" start_char="6748">wrong</TOKEN>
<TOKEN end_char="6753" id="token-69-8" morph="none" pos="punct" start_char="6753">:</TOKEN>
</SEG>
<SEG end_char="6815" id="segment-70" start_char="6756">
<ORIGINAL_TEXT>Tests being put to use now vary widely in their specificity.</ORIGINAL_TEXT>
<TOKEN end_char="6760" id="token-70-0" morph="none" pos="word" start_char="6756">Tests</TOKEN>
<TOKEN end_char="6766" id="token-70-1" morph="none" pos="word" start_char="6762">being</TOKEN>
<TOKEN end_char="6770" id="token-70-2" morph="none" pos="word" start_char="6768">put</TOKEN>
<TOKEN end_char="6773" id="token-70-3" morph="none" pos="word" start_char="6772">to</TOKEN>
<TOKEN end_char="6777" id="token-70-4" morph="none" pos="word" start_char="6775">use</TOKEN>
<TOKEN end_char="6781" id="token-70-5" morph="none" pos="word" start_char="6779">now</TOKEN>
<TOKEN end_char="6786" id="token-70-6" morph="none" pos="word" start_char="6783">vary</TOKEN>
<TOKEN end_char="6793" id="token-70-7" morph="none" pos="word" start_char="6788">widely</TOKEN>
<TOKEN end_char="6796" id="token-70-8" morph="none" pos="word" start_char="6795">in</TOKEN>
<TOKEN end_char="6802" id="token-70-9" morph="none" pos="word" start_char="6798">their</TOKEN>
<TOKEN end_char="6814" id="token-70-10" morph="none" pos="word" start_char="6804">specificity</TOKEN>
<TOKEN end_char="6815" id="token-70-11" morph="none" pos="punct" start_char="6815">.</TOKEN>
</SEG>
<SEG end_char="6904" id="segment-71" start_char="6818">
<ORIGINAL_TEXT>Lab giant Becton Dickinson says its coronavirus antibody test has a specificity of 91%.</ORIGINAL_TEXT>
<TOKEN end_char="6820" id="token-71-0" morph="none" pos="word" start_char="6818">Lab</TOKEN>
<TOKEN end_char="6826" id="token-71-1" morph="none" pos="word" start_char="6822">giant</TOKEN>
<TOKEN end_char="6833" id="token-71-2" morph="none" pos="word" start_char="6828">Becton</TOKEN>
<TOKEN end_char="6843" id="token-71-3" morph="none" pos="word" start_char="6835">Dickinson</TOKEN>
<TOKEN end_char="6848" id="token-71-4" morph="none" pos="word" start_char="6845">says</TOKEN>
<TOKEN end_char="6852" id="token-71-5" morph="none" pos="word" start_char="6850">its</TOKEN>
<TOKEN end_char="6864" id="token-71-6" morph="none" pos="word" start_char="6854">coronavirus</TOKEN>
<TOKEN end_char="6873" id="token-71-7" morph="none" pos="word" start_char="6866">antibody</TOKEN>
<TOKEN end_char="6878" id="token-71-8" morph="none" pos="word" start_char="6875">test</TOKEN>
<TOKEN end_char="6882" id="token-71-9" morph="none" pos="word" start_char="6880">has</TOKEN>
<TOKEN end_char="6884" id="token-71-10" morph="none" pos="word" start_char="6884">a</TOKEN>
<TOKEN end_char="6896" id="token-71-11" morph="none" pos="word" start_char="6886">specificity</TOKEN>
<TOKEN end_char="6899" id="token-71-12" morph="none" pos="word" start_char="6898">of</TOKEN>
<TOKEN end_char="6902" id="token-71-13" morph="none" pos="word" start_char="6901">91</TOKEN>
<TOKEN end_char="6904" id="token-71-14" morph="none" pos="punct" start_char="6903">%.</TOKEN>
</SEG>
<SEG end_char="6961" id="segment-72" start_char="6906">
<ORIGINAL_TEXT>Emory University says its test has a specificity of 95%.</ORIGINAL_TEXT>
<TOKEN end_char="6910" id="token-72-0" morph="none" pos="word" start_char="6906">Emory</TOKEN>
<TOKEN end_char="6921" id="token-72-1" morph="none" pos="word" start_char="6912">University</TOKEN>
<TOKEN end_char="6926" id="token-72-2" morph="none" pos="word" start_char="6923">says</TOKEN>
<TOKEN end_char="6930" id="token-72-3" morph="none" pos="word" start_char="6928">its</TOKEN>
<TOKEN end_char="6935" id="token-72-4" morph="none" pos="word" start_char="6932">test</TOKEN>
<TOKEN end_char="6939" id="token-72-5" morph="none" pos="word" start_char="6937">has</TOKEN>
<TOKEN end_char="6941" id="token-72-6" morph="none" pos="word" start_char="6941">a</TOKEN>
<TOKEN end_char="6953" id="token-72-7" morph="none" pos="word" start_char="6943">specificity</TOKEN>
<TOKEN end_char="6956" id="token-72-8" morph="none" pos="word" start_char="6955">of</TOKEN>
<TOKEN end_char="6959" id="token-72-9" morph="none" pos="word" start_char="6958">95</TOKEN>
<TOKEN end_char="6961" id="token-72-10" morph="none" pos="punct" start_char="6960">%.</TOKEN>
</SEG>
<SEG end_char="7081" id="segment-73" start_char="6963">
<ORIGINAL_TEXT>Stanford University says its test demonstrated 100% specificity, at least in a preliminary study involving 100 samples.</ORIGINAL_TEXT>
<TOKEN end_char="6970" id="token-73-0" morph="none" pos="word" start_char="6963">Stanford</TOKEN>
<TOKEN end_char="6981" id="token-73-1" morph="none" pos="word" start_char="6972">University</TOKEN>
<TOKEN end_char="6986" id="token-73-2" morph="none" pos="word" start_char="6983">says</TOKEN>
<TOKEN end_char="6990" id="token-73-3" morph="none" pos="word" start_char="6988">its</TOKEN>
<TOKEN end_char="6995" id="token-73-4" morph="none" pos="word" start_char="6992">test</TOKEN>
<TOKEN end_char="7008" id="token-73-5" morph="none" pos="word" start_char="6997">demonstrated</TOKEN>
<TOKEN end_char="7012" id="token-73-6" morph="none" pos="word" start_char="7010">100</TOKEN>
<TOKEN end_char="7013" id="token-73-7" morph="none" pos="punct" start_char="7013">%</TOKEN>
<TOKEN end_char="7025" id="token-73-8" morph="none" pos="word" start_char="7015">specificity</TOKEN>
<TOKEN end_char="7026" id="token-73-9" morph="none" pos="punct" start_char="7026">,</TOKEN>
<TOKEN end_char="7029" id="token-73-10" morph="none" pos="word" start_char="7028">at</TOKEN>
<TOKEN end_char="7035" id="token-73-11" morph="none" pos="word" start_char="7031">least</TOKEN>
<TOKEN end_char="7038" id="token-73-12" morph="none" pos="word" start_char="7037">in</TOKEN>
<TOKEN end_char="7040" id="token-73-13" morph="none" pos="word" start_char="7040">a</TOKEN>
<TOKEN end_char="7052" id="token-73-14" morph="none" pos="word" start_char="7042">preliminary</TOKEN>
<TOKEN end_char="7058" id="token-73-15" morph="none" pos="word" start_char="7054">study</TOKEN>
<TOKEN end_char="7068" id="token-73-16" morph="none" pos="word" start_char="7060">involving</TOKEN>
<TOKEN end_char="7072" id="token-73-17" morph="none" pos="word" start_char="7070">100</TOKEN>
<TOKEN end_char="7080" id="token-73-18" morph="none" pos="word" start_char="7074">samples</TOKEN>
<TOKEN end_char="7081" id="token-73-19" morph="none" pos="punct" start_char="7081">.</TOKEN>
</SEG>
<SEG end_char="7260" id="segment-74" start_char="7083">
<ORIGINAL_TEXT>The Centers for Disease Control and Prevention has been developing an antibody test since the early days of the epidemic, but has not yet published results about its performance.</ORIGINAL_TEXT>
<TOKEN end_char="7085" id="token-74-0" morph="none" pos="word" start_char="7083">The</TOKEN>
<TOKEN end_char="7093" id="token-74-1" morph="none" pos="word" start_char="7087">Centers</TOKEN>
<TOKEN end_char="7097" id="token-74-2" morph="none" pos="word" start_char="7095">for</TOKEN>
<TOKEN end_char="7105" id="token-74-3" morph="none" pos="word" start_char="7099">Disease</TOKEN>
<TOKEN end_char="7113" id="token-74-4" morph="none" pos="word" start_char="7107">Control</TOKEN>
<TOKEN end_char="7117" id="token-74-5" morph="none" pos="word" start_char="7115">and</TOKEN>
<TOKEN end_char="7128" id="token-74-6" morph="none" pos="word" start_char="7119">Prevention</TOKEN>
<TOKEN end_char="7132" id="token-74-7" morph="none" pos="word" start_char="7130">has</TOKEN>
<TOKEN end_char="7137" id="token-74-8" morph="none" pos="word" start_char="7134">been</TOKEN>
<TOKEN end_char="7148" id="token-74-9" morph="none" pos="word" start_char="7139">developing</TOKEN>
<TOKEN end_char="7151" id="token-74-10" morph="none" pos="word" start_char="7150">an</TOKEN>
<TOKEN end_char="7160" id="token-74-11" morph="none" pos="word" start_char="7153">antibody</TOKEN>
<TOKEN end_char="7165" id="token-74-12" morph="none" pos="word" start_char="7162">test</TOKEN>
<TOKEN end_char="7171" id="token-74-13" morph="none" pos="word" start_char="7167">since</TOKEN>
<TOKEN end_char="7175" id="token-74-14" morph="none" pos="word" start_char="7173">the</TOKEN>
<TOKEN end_char="7181" id="token-74-15" morph="none" pos="word" start_char="7177">early</TOKEN>
<TOKEN end_char="7186" id="token-74-16" morph="none" pos="word" start_char="7183">days</TOKEN>
<TOKEN end_char="7189" id="token-74-17" morph="none" pos="word" start_char="7188">of</TOKEN>
<TOKEN end_char="7193" id="token-74-18" morph="none" pos="word" start_char="7191">the</TOKEN>
<TOKEN end_char="7202" id="token-74-19" morph="none" pos="word" start_char="7195">epidemic</TOKEN>
<TOKEN end_char="7203" id="token-74-20" morph="none" pos="punct" start_char="7203">,</TOKEN>
<TOKEN end_char="7207" id="token-74-21" morph="none" pos="word" start_char="7205">but</TOKEN>
<TOKEN end_char="7211" id="token-74-22" morph="none" pos="word" start_char="7209">has</TOKEN>
<TOKEN end_char="7215" id="token-74-23" morph="none" pos="word" start_char="7213">not</TOKEN>
<TOKEN end_char="7219" id="token-74-24" morph="none" pos="word" start_char="7217">yet</TOKEN>
<TOKEN end_char="7229" id="token-74-25" morph="none" pos="word" start_char="7221">published</TOKEN>
<TOKEN end_char="7237" id="token-74-26" morph="none" pos="word" start_char="7231">results</TOKEN>
<TOKEN end_char="7243" id="token-74-27" morph="none" pos="word" start_char="7239">about</TOKEN>
<TOKEN end_char="7247" id="token-74-28" morph="none" pos="word" start_char="7245">its</TOKEN>
<TOKEN end_char="7259" id="token-74-29" morph="none" pos="word" start_char="7249">performance</TOKEN>
<TOKEN end_char="7260" id="token-74-30" morph="none" pos="punct" start_char="7260">.</TOKEN>
</SEG>
<SEG end_char="7474" id="segment-75" start_char="7263">
<ORIGINAL_TEXT>One way to limit the problem of false-positive results is to focus on populations where the disease is more common  in other words, in situations where true positives are much more frequent than false positives.</ORIGINAL_TEXT>
<TOKEN end_char="7265" id="token-75-0" morph="none" pos="word" start_char="7263">One</TOKEN>
<TOKEN end_char="7269" id="token-75-1" morph="none" pos="word" start_char="7267">way</TOKEN>
<TOKEN end_char="7272" id="token-75-2" morph="none" pos="word" start_char="7271">to</TOKEN>
<TOKEN end_char="7278" id="token-75-3" morph="none" pos="word" start_char="7274">limit</TOKEN>
<TOKEN end_char="7282" id="token-75-4" morph="none" pos="word" start_char="7280">the</TOKEN>
<TOKEN end_char="7290" id="token-75-5" morph="none" pos="word" start_char="7284">problem</TOKEN>
<TOKEN end_char="7293" id="token-75-6" morph="none" pos="word" start_char="7292">of</TOKEN>
<TOKEN end_char="7308" id="token-75-7" morph="none" pos="unknown" start_char="7295">false-positive</TOKEN>
<TOKEN end_char="7316" id="token-75-8" morph="none" pos="word" start_char="7310">results</TOKEN>
<TOKEN end_char="7319" id="token-75-9" morph="none" pos="word" start_char="7318">is</TOKEN>
<TOKEN end_char="7322" id="token-75-10" morph="none" pos="word" start_char="7321">to</TOKEN>
<TOKEN end_char="7328" id="token-75-11" morph="none" pos="word" start_char="7324">focus</TOKEN>
<TOKEN end_char="7331" id="token-75-12" morph="none" pos="word" start_char="7330">on</TOKEN>
<TOKEN end_char="7343" id="token-75-13" morph="none" pos="word" start_char="7333">populations</TOKEN>
<TOKEN end_char="7349" id="token-75-14" morph="none" pos="word" start_char="7345">where</TOKEN>
<TOKEN end_char="7353" id="token-75-15" morph="none" pos="word" start_char="7351">the</TOKEN>
<TOKEN end_char="7361" id="token-75-16" morph="none" pos="word" start_char="7355">disease</TOKEN>
<TOKEN end_char="7364" id="token-75-17" morph="none" pos="word" start_char="7363">is</TOKEN>
<TOKEN end_char="7369" id="token-75-18" morph="none" pos="word" start_char="7366">more</TOKEN>
<TOKEN end_char="7376" id="token-75-19" morph="none" pos="word" start_char="7371">common</TOKEN>
<TOKEN end_char="7378" id="token-75-20" morph="none" pos="punct" start_char="7378"></TOKEN>
<TOKEN end_char="7381" id="token-75-21" morph="none" pos="word" start_char="7380">in</TOKEN>
<TOKEN end_char="7387" id="token-75-22" morph="none" pos="word" start_char="7383">other</TOKEN>
<TOKEN end_char="7393" id="token-75-23" morph="none" pos="word" start_char="7389">words</TOKEN>
<TOKEN end_char="7394" id="token-75-24" morph="none" pos="punct" start_char="7394">,</TOKEN>
<TOKEN end_char="7397" id="token-75-25" morph="none" pos="word" start_char="7396">in</TOKEN>
<TOKEN end_char="7408" id="token-75-26" morph="none" pos="word" start_char="7399">situations</TOKEN>
<TOKEN end_char="7414" id="token-75-27" morph="none" pos="word" start_char="7410">where</TOKEN>
<TOKEN end_char="7419" id="token-75-28" morph="none" pos="word" start_char="7416">true</TOKEN>
<TOKEN end_char="7429" id="token-75-29" morph="none" pos="word" start_char="7421">positives</TOKEN>
<TOKEN end_char="7433" id="token-75-30" morph="none" pos="word" start_char="7431">are</TOKEN>
<TOKEN end_char="7438" id="token-75-31" morph="none" pos="word" start_char="7435">much</TOKEN>
<TOKEN end_char="7443" id="token-75-32" morph="none" pos="word" start_char="7440">more</TOKEN>
<TOKEN end_char="7452" id="token-75-33" morph="none" pos="word" start_char="7445">frequent</TOKEN>
<TOKEN end_char="7457" id="token-75-34" morph="none" pos="word" start_char="7454">than</TOKEN>
<TOKEN end_char="7463" id="token-75-35" morph="none" pos="word" start_char="7459">false</TOKEN>
<TOKEN end_char="7473" id="token-75-36" morph="none" pos="word" start_char="7465">positives</TOKEN>
<TOKEN end_char="7474" id="token-75-37" morph="none" pos="punct" start_char="7474">.</TOKEN>
</SEG>
<SEG end_char="7558" id="segment-76" start_char="7476">
<ORIGINAL_TEXT>That's likely to be the case in hospitals that have cared for coronavirus patients.</ORIGINAL_TEXT>
<TOKEN end_char="7481" id="token-76-0" morph="none" pos="word" start_char="7476">That's</TOKEN>
<TOKEN end_char="7488" id="token-76-1" morph="none" pos="word" start_char="7483">likely</TOKEN>
<TOKEN end_char="7491" id="token-76-2" morph="none" pos="word" start_char="7490">to</TOKEN>
<TOKEN end_char="7494" id="token-76-3" morph="none" pos="word" start_char="7493">be</TOKEN>
<TOKEN end_char="7498" id="token-76-4" morph="none" pos="word" start_char="7496">the</TOKEN>
<TOKEN end_char="7503" id="token-76-5" morph="none" pos="word" start_char="7500">case</TOKEN>
<TOKEN end_char="7506" id="token-76-6" morph="none" pos="word" start_char="7505">in</TOKEN>
<TOKEN end_char="7516" id="token-76-7" morph="none" pos="word" start_char="7508">hospitals</TOKEN>
<TOKEN end_char="7521" id="token-76-8" morph="none" pos="word" start_char="7518">that</TOKEN>
<TOKEN end_char="7526" id="token-76-9" morph="none" pos="word" start_char="7523">have</TOKEN>
<TOKEN end_char="7532" id="token-76-10" morph="none" pos="word" start_char="7528">cared</TOKEN>
<TOKEN end_char="7536" id="token-76-11" morph="none" pos="word" start_char="7534">for</TOKEN>
<TOKEN end_char="7548" id="token-76-12" morph="none" pos="word" start_char="7538">coronavirus</TOKEN>
<TOKEN end_char="7557" id="token-76-13" morph="none" pos="word" start_char="7550">patients</TOKEN>
<TOKEN end_char="7558" id="token-76-14" morph="none" pos="punct" start_char="7558">.</TOKEN>
</SEG>
<SEG end_char="7678" id="segment-77" start_char="7561">
<ORIGINAL_TEXT>"It would be wonderful for health care workers to know their immune status, just to give them peace of mind," says Dr.</ORIGINAL_TEXT>
<TOKEN end_char="7561" id="token-77-0" morph="none" pos="punct" start_char="7561">"</TOKEN>
<TOKEN end_char="7563" id="token-77-1" morph="none" pos="word" start_char="7562">It</TOKEN>
<TOKEN end_char="7569" id="token-77-2" morph="none" pos="word" start_char="7565">would</TOKEN>
<TOKEN end_char="7572" id="token-77-3" morph="none" pos="word" start_char="7571">be</TOKEN>
<TOKEN end_char="7582" id="token-77-4" morph="none" pos="word" start_char="7574">wonderful</TOKEN>
<TOKEN end_char="7586" id="token-77-5" morph="none" pos="word" start_char="7584">for</TOKEN>
<TOKEN end_char="7593" id="token-77-6" morph="none" pos="word" start_char="7588">health</TOKEN>
<TOKEN end_char="7598" id="token-77-7" morph="none" pos="word" start_char="7595">care</TOKEN>
<TOKEN end_char="7606" id="token-77-8" morph="none" pos="word" start_char="7600">workers</TOKEN>
<TOKEN end_char="7609" id="token-77-9" morph="none" pos="word" start_char="7608">to</TOKEN>
<TOKEN end_char="7614" id="token-77-10" morph="none" pos="word" start_char="7611">know</TOKEN>
<TOKEN end_char="7620" id="token-77-11" morph="none" pos="word" start_char="7616">their</TOKEN>
<TOKEN end_char="7627" id="token-77-12" morph="none" pos="word" start_char="7622">immune</TOKEN>
<TOKEN end_char="7634" id="token-77-13" morph="none" pos="word" start_char="7629">status</TOKEN>
<TOKEN end_char="7635" id="token-77-14" morph="none" pos="punct" start_char="7635">,</TOKEN>
<TOKEN end_char="7640" id="token-77-15" morph="none" pos="word" start_char="7637">just</TOKEN>
<TOKEN end_char="7643" id="token-77-16" morph="none" pos="word" start_char="7642">to</TOKEN>
<TOKEN end_char="7648" id="token-77-17" morph="none" pos="word" start_char="7645">give</TOKEN>
<TOKEN end_char="7653" id="token-77-18" morph="none" pos="word" start_char="7650">them</TOKEN>
<TOKEN end_char="7659" id="token-77-19" morph="none" pos="word" start_char="7655">peace</TOKEN>
<TOKEN end_char="7662" id="token-77-20" morph="none" pos="word" start_char="7661">of</TOKEN>
<TOKEN end_char="7667" id="token-77-21" morph="none" pos="word" start_char="7664">mind</TOKEN>
<TOKEN end_char="7669" id="token-77-22" morph="none" pos="punct" start_char="7668">,"</TOKEN>
<TOKEN end_char="7674" id="token-77-23" morph="none" pos="word" start_char="7671">says</TOKEN>
<TOKEN end_char="7677" id="token-77-24" morph="none" pos="word" start_char="7676">Dr</TOKEN>
<TOKEN end_char="7678" id="token-77-25" morph="none" pos="punct" start_char="7678">.</TOKEN>
</SEG>
<SEG end_char="7752" id="segment-78" start_char="7680">
<ORIGINAL_TEXT>Jordan Laser, a pathologist at Northwell Health on Long Island, New York.</ORIGINAL_TEXT>
<TOKEN end_char="7685" id="token-78-0" morph="none" pos="word" start_char="7680">Jordan</TOKEN>
<TOKEN end_char="7691" id="token-78-1" morph="none" pos="word" start_char="7687">Laser</TOKEN>
<TOKEN end_char="7692" id="token-78-2" morph="none" pos="punct" start_char="7692">,</TOKEN>
<TOKEN end_char="7694" id="token-78-3" morph="none" pos="word" start_char="7694">a</TOKEN>
<TOKEN end_char="7706" id="token-78-4" morph="none" pos="word" start_char="7696">pathologist</TOKEN>
<TOKEN end_char="7709" id="token-78-5" morph="none" pos="word" start_char="7708">at</TOKEN>
<TOKEN end_char="7719" id="token-78-6" morph="none" pos="word" start_char="7711">Northwell</TOKEN>
<TOKEN end_char="7726" id="token-78-7" morph="none" pos="word" start_char="7721">Health</TOKEN>
<TOKEN end_char="7729" id="token-78-8" morph="none" pos="word" start_char="7728">on</TOKEN>
<TOKEN end_char="7734" id="token-78-9" morph="none" pos="word" start_char="7731">Long</TOKEN>
<TOKEN end_char="7741" id="token-78-10" morph="none" pos="word" start_char="7736">Island</TOKEN>
<TOKEN end_char="7742" id="token-78-11" morph="none" pos="punct" start_char="7742">,</TOKEN>
<TOKEN end_char="7746" id="token-78-12" morph="none" pos="word" start_char="7744">New</TOKEN>
<TOKEN end_char="7751" id="token-78-13" morph="none" pos="word" start_char="7748">York</TOKEN>
<TOKEN end_char="7752" id="token-78-14" morph="none" pos="punct" start_char="7752">.</TOKEN>
</SEG>
<SEG end_char="7827" id="segment-79" start_char="7755">
<ORIGINAL_TEXT>Even so, Laser says it would still be a mistake to rely on these results.</ORIGINAL_TEXT>
<TOKEN end_char="7758" id="token-79-0" morph="none" pos="word" start_char="7755">Even</TOKEN>
<TOKEN end_char="7761" id="token-79-1" morph="none" pos="word" start_char="7760">so</TOKEN>
<TOKEN end_char="7762" id="token-79-2" morph="none" pos="punct" start_char="7762">,</TOKEN>
<TOKEN end_char="7768" id="token-79-3" morph="none" pos="word" start_char="7764">Laser</TOKEN>
<TOKEN end_char="7773" id="token-79-4" morph="none" pos="word" start_char="7770">says</TOKEN>
<TOKEN end_char="7776" id="token-79-5" morph="none" pos="word" start_char="7775">it</TOKEN>
<TOKEN end_char="7782" id="token-79-6" morph="none" pos="word" start_char="7778">would</TOKEN>
<TOKEN end_char="7788" id="token-79-7" morph="none" pos="word" start_char="7784">still</TOKEN>
<TOKEN end_char="7791" id="token-79-8" morph="none" pos="word" start_char="7790">be</TOKEN>
<TOKEN end_char="7793" id="token-79-9" morph="none" pos="word" start_char="7793">a</TOKEN>
<TOKEN end_char="7801" id="token-79-10" morph="none" pos="word" start_char="7795">mistake</TOKEN>
<TOKEN end_char="7804" id="token-79-11" morph="none" pos="word" start_char="7803">to</TOKEN>
<TOKEN end_char="7809" id="token-79-12" morph="none" pos="word" start_char="7806">rely</TOKEN>
<TOKEN end_char="7812" id="token-79-13" morph="none" pos="word" start_char="7811">on</TOKEN>
<TOKEN end_char="7818" id="token-79-14" morph="none" pos="word" start_char="7814">these</TOKEN>
<TOKEN end_char="7826" id="token-79-15" morph="none" pos="word" start_char="7820">results</TOKEN>
<TOKEN end_char="7827" id="token-79-16" morph="none" pos="punct" start_char="7827">.</TOKEN>
</SEG>
<SEG end_char="7970" id="segment-80" start_char="7830">
<ORIGINAL_TEXT>"Definitely don't use these tests to change your practices in terms of use of personal protective equipment," he would advise his colleagues.</ORIGINAL_TEXT>
<TOKEN end_char="7830" id="token-80-0" morph="none" pos="punct" start_char="7830">"</TOKEN>
<TOKEN end_char="7840" id="token-80-1" morph="none" pos="word" start_char="7831">Definitely</TOKEN>
<TOKEN end_char="7846" id="token-80-2" morph="none" pos="word" start_char="7842">don't</TOKEN>
<TOKEN end_char="7850" id="token-80-3" morph="none" pos="word" start_char="7848">use</TOKEN>
<TOKEN end_char="7856" id="token-80-4" morph="none" pos="word" start_char="7852">these</TOKEN>
<TOKEN end_char="7862" id="token-80-5" morph="none" pos="word" start_char="7858">tests</TOKEN>
<TOKEN end_char="7865" id="token-80-6" morph="none" pos="word" start_char="7864">to</TOKEN>
<TOKEN end_char="7872" id="token-80-7" morph="none" pos="word" start_char="7867">change</TOKEN>
<TOKEN end_char="7877" id="token-80-8" morph="none" pos="word" start_char="7874">your</TOKEN>
<TOKEN end_char="7887" id="token-80-9" morph="none" pos="word" start_char="7879">practices</TOKEN>
<TOKEN end_char="7890" id="token-80-10" morph="none" pos="word" start_char="7889">in</TOKEN>
<TOKEN end_char="7896" id="token-80-11" morph="none" pos="word" start_char="7892">terms</TOKEN>
<TOKEN end_char="7899" id="token-80-12" morph="none" pos="word" start_char="7898">of</TOKEN>
<TOKEN end_char="7903" id="token-80-13" morph="none" pos="word" start_char="7901">use</TOKEN>
<TOKEN end_char="7906" id="token-80-14" morph="none" pos="word" start_char="7905">of</TOKEN>
<TOKEN end_char="7915" id="token-80-15" morph="none" pos="word" start_char="7908">personal</TOKEN>
<TOKEN end_char="7926" id="token-80-16" morph="none" pos="word" start_char="7917">protective</TOKEN>
<TOKEN end_char="7936" id="token-80-17" morph="none" pos="word" start_char="7928">equipment</TOKEN>
<TOKEN end_char="7938" id="token-80-18" morph="none" pos="punct" start_char="7937">,"</TOKEN>
<TOKEN end_char="7941" id="token-80-19" morph="none" pos="word" start_char="7940">he</TOKEN>
<TOKEN end_char="7947" id="token-80-20" morph="none" pos="word" start_char="7943">would</TOKEN>
<TOKEN end_char="7954" id="token-80-21" morph="none" pos="word" start_char="7949">advise</TOKEN>
<TOKEN end_char="7958" id="token-80-22" morph="none" pos="word" start_char="7956">his</TOKEN>
<TOKEN end_char="7969" id="token-80-23" morph="none" pos="word" start_char="7960">colleagues</TOKEN>
<TOKEN end_char="7970" id="token-80-24" morph="none" pos="punct" start_char="7970">.</TOKEN>
</SEG>
<SEG end_char="8060" id="segment-81" start_char="7972">
<ORIGINAL_TEXT>"Definitely do not become comfortable in doing your job in taking care of COVID patients.</ORIGINAL_TEXT>
<TOKEN end_char="7972" id="token-81-0" morph="none" pos="punct" start_char="7972">"</TOKEN>
<TOKEN end_char="7982" id="token-81-1" morph="none" pos="word" start_char="7973">Definitely</TOKEN>
<TOKEN end_char="7985" id="token-81-2" morph="none" pos="word" start_char="7984">do</TOKEN>
<TOKEN end_char="7989" id="token-81-3" morph="none" pos="word" start_char="7987">not</TOKEN>
<TOKEN end_char="7996" id="token-81-4" morph="none" pos="word" start_char="7991">become</TOKEN>
<TOKEN end_char="8008" id="token-81-5" morph="none" pos="word" start_char="7998">comfortable</TOKEN>
<TOKEN end_char="8011" id="token-81-6" morph="none" pos="word" start_char="8010">in</TOKEN>
<TOKEN end_char="8017" id="token-81-7" morph="none" pos="word" start_char="8013">doing</TOKEN>
<TOKEN end_char="8022" id="token-81-8" morph="none" pos="word" start_char="8019">your</TOKEN>
<TOKEN end_char="8026" id="token-81-9" morph="none" pos="word" start_char="8024">job</TOKEN>
<TOKEN end_char="8029" id="token-81-10" morph="none" pos="word" start_char="8028">in</TOKEN>
<TOKEN end_char="8036" id="token-81-11" morph="none" pos="word" start_char="8031">taking</TOKEN>
<TOKEN end_char="8041" id="token-81-12" morph="none" pos="word" start_char="8038">care</TOKEN>
<TOKEN end_char="8044" id="token-81-13" morph="none" pos="word" start_char="8043">of</TOKEN>
<TOKEN end_char="8050" id="token-81-14" morph="none" pos="word" start_char="8046">COVID</TOKEN>
<TOKEN end_char="8059" id="token-81-15" morph="none" pos="word" start_char="8052">patients</TOKEN>
<TOKEN end_char="8060" id="token-81-16" morph="none" pos="punct" start_char="8060">.</TOKEN>
</SEG>
<SEG end_char="8113" id="segment-82" start_char="8062">
<ORIGINAL_TEXT>It really would be more of a psychological benefit."</ORIGINAL_TEXT>
<TOKEN end_char="8063" id="token-82-0" morph="none" pos="word" start_char="8062">It</TOKEN>
<TOKEN end_char="8070" id="token-82-1" morph="none" pos="word" start_char="8065">really</TOKEN>
<TOKEN end_char="8076" id="token-82-2" morph="none" pos="word" start_char="8072">would</TOKEN>
<TOKEN end_char="8079" id="token-82-3" morph="none" pos="word" start_char="8078">be</TOKEN>
<TOKEN end_char="8084" id="token-82-4" morph="none" pos="word" start_char="8081">more</TOKEN>
<TOKEN end_char="8087" id="token-82-5" morph="none" pos="word" start_char="8086">of</TOKEN>
<TOKEN end_char="8089" id="token-82-6" morph="none" pos="word" start_char="8089">a</TOKEN>
<TOKEN end_char="8103" id="token-82-7" morph="none" pos="word" start_char="8091">psychological</TOKEN>
<TOKEN end_char="8111" id="token-82-8" morph="none" pos="word" start_char="8105">benefit</TOKEN>
<TOKEN end_char="8113" id="token-82-9" morph="none" pos="punct" start_char="8112">."</TOKEN>
</SEG>
<SEG end_char="8318" id="segment-83" start_char="8116">
<ORIGINAL_TEXT>Jeremy Gabrysch, the doctor in Austin, is also using these tests to satisfy the curiosity of his patients, not to provide actionable guidance about whether it's safe for people to assume they are immune.</ORIGINAL_TEXT>
<TOKEN end_char="8121" id="token-83-0" morph="none" pos="word" start_char="8116">Jeremy</TOKEN>
<TOKEN end_char="8130" id="token-83-1" morph="none" pos="word" start_char="8123">Gabrysch</TOKEN>
<TOKEN end_char="8131" id="token-83-2" morph="none" pos="punct" start_char="8131">,</TOKEN>
<TOKEN end_char="8135" id="token-83-3" morph="none" pos="word" start_char="8133">the</TOKEN>
<TOKEN end_char="8142" id="token-83-4" morph="none" pos="word" start_char="8137">doctor</TOKEN>
<TOKEN end_char="8145" id="token-83-5" morph="none" pos="word" start_char="8144">in</TOKEN>
<TOKEN end_char="8152" id="token-83-6" morph="none" pos="word" start_char="8147">Austin</TOKEN>
<TOKEN end_char="8153" id="token-83-7" morph="none" pos="punct" start_char="8153">,</TOKEN>
<TOKEN end_char="8156" id="token-83-8" morph="none" pos="word" start_char="8155">is</TOKEN>
<TOKEN end_char="8161" id="token-83-9" morph="none" pos="word" start_char="8158">also</TOKEN>
<TOKEN end_char="8167" id="token-83-10" morph="none" pos="word" start_char="8163">using</TOKEN>
<TOKEN end_char="8173" id="token-83-11" morph="none" pos="word" start_char="8169">these</TOKEN>
<TOKEN end_char="8179" id="token-83-12" morph="none" pos="word" start_char="8175">tests</TOKEN>
<TOKEN end_char="8182" id="token-83-13" morph="none" pos="word" start_char="8181">to</TOKEN>
<TOKEN end_char="8190" id="token-83-14" morph="none" pos="word" start_char="8184">satisfy</TOKEN>
<TOKEN end_char="8194" id="token-83-15" morph="none" pos="word" start_char="8192">the</TOKEN>
<TOKEN end_char="8204" id="token-83-16" morph="none" pos="word" start_char="8196">curiosity</TOKEN>
<TOKEN end_char="8207" id="token-83-17" morph="none" pos="word" start_char="8206">of</TOKEN>
<TOKEN end_char="8211" id="token-83-18" morph="none" pos="word" start_char="8209">his</TOKEN>
<TOKEN end_char="8220" id="token-83-19" morph="none" pos="word" start_char="8213">patients</TOKEN>
<TOKEN end_char="8221" id="token-83-20" morph="none" pos="punct" start_char="8221">,</TOKEN>
<TOKEN end_char="8225" id="token-83-21" morph="none" pos="word" start_char="8223">not</TOKEN>
<TOKEN end_char="8228" id="token-83-22" morph="none" pos="word" start_char="8227">to</TOKEN>
<TOKEN end_char="8236" id="token-83-23" morph="none" pos="word" start_char="8230">provide</TOKEN>
<TOKEN end_char="8247" id="token-83-24" morph="none" pos="word" start_char="8238">actionable</TOKEN>
<TOKEN end_char="8256" id="token-83-25" morph="none" pos="word" start_char="8249">guidance</TOKEN>
<TOKEN end_char="8262" id="token-83-26" morph="none" pos="word" start_char="8258">about</TOKEN>
<TOKEN end_char="8270" id="token-83-27" morph="none" pos="word" start_char="8264">whether</TOKEN>
<TOKEN end_char="8275" id="token-83-28" morph="none" pos="word" start_char="8272">it's</TOKEN>
<TOKEN end_char="8280" id="token-83-29" morph="none" pos="word" start_char="8277">safe</TOKEN>
<TOKEN end_char="8284" id="token-83-30" morph="none" pos="word" start_char="8282">for</TOKEN>
<TOKEN end_char="8291" id="token-83-31" morph="none" pos="word" start_char="8286">people</TOKEN>
<TOKEN end_char="8294" id="token-83-32" morph="none" pos="word" start_char="8293">to</TOKEN>
<TOKEN end_char="8301" id="token-83-33" morph="none" pos="word" start_char="8296">assume</TOKEN>
<TOKEN end_char="8306" id="token-83-34" morph="none" pos="word" start_char="8303">they</TOKEN>
<TOKEN end_char="8310" id="token-83-35" morph="none" pos="word" start_char="8308">are</TOKEN>
<TOKEN end_char="8317" id="token-83-36" morph="none" pos="word" start_char="8312">immune</TOKEN>
<TOKEN end_char="8318" id="token-83-37" morph="none" pos="punct" start_char="8318">.</TOKEN>
</SEG>
<SEG end_char="8421" id="segment-84" start_char="8321">
<ORIGINAL_TEXT>"We don't recommend that people interpret the result that way, and we counsel against that," he says.</ORIGINAL_TEXT>
<TOKEN end_char="8321" id="token-84-0" morph="none" pos="punct" start_char="8321">"</TOKEN>
<TOKEN end_char="8323" id="token-84-1" morph="none" pos="word" start_char="8322">We</TOKEN>
<TOKEN end_char="8329" id="token-84-2" morph="none" pos="word" start_char="8325">don't</TOKEN>
<TOKEN end_char="8339" id="token-84-3" morph="none" pos="word" start_char="8331">recommend</TOKEN>
<TOKEN end_char="8344" id="token-84-4" morph="none" pos="word" start_char="8341">that</TOKEN>
<TOKEN end_char="8351" id="token-84-5" morph="none" pos="word" start_char="8346">people</TOKEN>
<TOKEN end_char="8361" id="token-84-6" morph="none" pos="word" start_char="8353">interpret</TOKEN>
<TOKEN end_char="8365" id="token-84-7" morph="none" pos="word" start_char="8363">the</TOKEN>
<TOKEN end_char="8372" id="token-84-8" morph="none" pos="word" start_char="8367">result</TOKEN>
<TOKEN end_char="8377" id="token-84-9" morph="none" pos="word" start_char="8374">that</TOKEN>
<TOKEN end_char="8381" id="token-84-10" morph="none" pos="word" start_char="8379">way</TOKEN>
<TOKEN end_char="8382" id="token-84-11" morph="none" pos="punct" start_char="8382">,</TOKEN>
<TOKEN end_char="8386" id="token-84-12" morph="none" pos="word" start_char="8384">and</TOKEN>
<TOKEN end_char="8389" id="token-84-13" morph="none" pos="word" start_char="8388">we</TOKEN>
<TOKEN end_char="8397" id="token-84-14" morph="none" pos="word" start_char="8391">counsel</TOKEN>
<TOKEN end_char="8405" id="token-84-15" morph="none" pos="word" start_char="8399">against</TOKEN>
<TOKEN end_char="8410" id="token-84-16" morph="none" pos="word" start_char="8407">that</TOKEN>
<TOKEN end_char="8412" id="token-84-17" morph="none" pos="punct" start_char="8411">,"</TOKEN>
<TOKEN end_char="8415" id="token-84-18" morph="none" pos="word" start_char="8414">he</TOKEN>
<TOKEN end_char="8420" id="token-84-19" morph="none" pos="word" start_char="8417">says</TOKEN>
<TOKEN end_char="8421" id="token-84-20" morph="none" pos="punct" start_char="8421">.</TOKEN>
</SEG>
<SEG end_char="8461" id="segment-85" start_char="8423">
<ORIGINAL_TEXT>"It's not a get-out-of-jail-free card."</ORIGINAL_TEXT>
<TOKEN end_char="8423" id="token-85-0" morph="none" pos="punct" start_char="8423">"</TOKEN>
<TOKEN end_char="8427" id="token-85-1" morph="none" pos="word" start_char="8424">It's</TOKEN>
<TOKEN end_char="8431" id="token-85-2" morph="none" pos="word" start_char="8429">not</TOKEN>
<TOKEN end_char="8433" id="token-85-3" morph="none" pos="word" start_char="8433">a</TOKEN>
<TOKEN end_char="8454" id="token-85-4" morph="none" pos="unknown" start_char="8435">get-out-of-jail-free</TOKEN>
<TOKEN end_char="8459" id="token-85-5" morph="none" pos="word" start_char="8456">card</TOKEN>
<TOKEN end_char="8461" id="token-85-6" morph="none" pos="punct" start_char="8460">."</TOKEN>
</SEG>
<SEG end_char="8646" id="segment-86" start_char="8464">
<ORIGINAL_TEXT>Because of these significant limitations, antibody tests fall short of being the imagined passport that would allow people to get back to business as usual and ignore the coronavirus.</ORIGINAL_TEXT>
<TOKEN end_char="8470" id="token-86-0" morph="none" pos="word" start_char="8464">Because</TOKEN>
<TOKEN end_char="8473" id="token-86-1" morph="none" pos="word" start_char="8472">of</TOKEN>
<TOKEN end_char="8479" id="token-86-2" morph="none" pos="word" start_char="8475">these</TOKEN>
<TOKEN end_char="8491" id="token-86-3" morph="none" pos="word" start_char="8481">significant</TOKEN>
<TOKEN end_char="8503" id="token-86-4" morph="none" pos="word" start_char="8493">limitations</TOKEN>
<TOKEN end_char="8504" id="token-86-5" morph="none" pos="punct" start_char="8504">,</TOKEN>
<TOKEN end_char="8513" id="token-86-6" morph="none" pos="word" start_char="8506">antibody</TOKEN>
<TOKEN end_char="8519" id="token-86-7" morph="none" pos="word" start_char="8515">tests</TOKEN>
<TOKEN end_char="8524" id="token-86-8" morph="none" pos="word" start_char="8521">fall</TOKEN>
<TOKEN end_char="8530" id="token-86-9" morph="none" pos="word" start_char="8526">short</TOKEN>
<TOKEN end_char="8533" id="token-86-10" morph="none" pos="word" start_char="8532">of</TOKEN>
<TOKEN end_char="8539" id="token-86-11" morph="none" pos="word" start_char="8535">being</TOKEN>
<TOKEN end_char="8543" id="token-86-12" morph="none" pos="word" start_char="8541">the</TOKEN>
<TOKEN end_char="8552" id="token-86-13" morph="none" pos="word" start_char="8545">imagined</TOKEN>
<TOKEN end_char="8561" id="token-86-14" morph="none" pos="word" start_char="8554">passport</TOKEN>
<TOKEN end_char="8566" id="token-86-15" morph="none" pos="word" start_char="8563">that</TOKEN>
<TOKEN end_char="8572" id="token-86-16" morph="none" pos="word" start_char="8568">would</TOKEN>
<TOKEN end_char="8578" id="token-86-17" morph="none" pos="word" start_char="8574">allow</TOKEN>
<TOKEN end_char="8585" id="token-86-18" morph="none" pos="word" start_char="8580">people</TOKEN>
<TOKEN end_char="8588" id="token-86-19" morph="none" pos="word" start_char="8587">to</TOKEN>
<TOKEN end_char="8592" id="token-86-20" morph="none" pos="word" start_char="8590">get</TOKEN>
<TOKEN end_char="8597" id="token-86-21" morph="none" pos="word" start_char="8594">back</TOKEN>
<TOKEN end_char="8600" id="token-86-22" morph="none" pos="word" start_char="8599">to</TOKEN>
<TOKEN end_char="8609" id="token-86-23" morph="none" pos="word" start_char="8602">business</TOKEN>
<TOKEN end_char="8612" id="token-86-24" morph="none" pos="word" start_char="8611">as</TOKEN>
<TOKEN end_char="8618" id="token-86-25" morph="none" pos="word" start_char="8614">usual</TOKEN>
<TOKEN end_char="8622" id="token-86-26" morph="none" pos="word" start_char="8620">and</TOKEN>
<TOKEN end_char="8629" id="token-86-27" morph="none" pos="word" start_char="8624">ignore</TOKEN>
<TOKEN end_char="8633" id="token-86-28" morph="none" pos="word" start_char="8631">the</TOKEN>
<TOKEN end_char="8645" id="token-86-29" morph="none" pos="word" start_char="8635">coronavirus</TOKEN>
<TOKEN end_char="8646" id="token-86-30" morph="none" pos="punct" start_char="8646">.</TOKEN>
</SEG>
<SEG end_char="8768" id="segment-87" start_char="8649">
<ORIGINAL_TEXT>But high-quality antibody tests will still be valuable in instances where individual false-positive results matter less.</ORIGINAL_TEXT>
<TOKEN end_char="8651" id="token-87-0" morph="none" pos="word" start_char="8649">But</TOKEN>
<TOKEN end_char="8664" id="token-87-1" morph="none" pos="unknown" start_char="8653">high-quality</TOKEN>
<TOKEN end_char="8673" id="token-87-2" morph="none" pos="word" start_char="8666">antibody</TOKEN>
<TOKEN end_char="8679" id="token-87-3" morph="none" pos="word" start_char="8675">tests</TOKEN>
<TOKEN end_char="8684" id="token-87-4" morph="none" pos="word" start_char="8681">will</TOKEN>
<TOKEN end_char="8690" id="token-87-5" morph="none" pos="word" start_char="8686">still</TOKEN>
<TOKEN end_char="8693" id="token-87-6" morph="none" pos="word" start_char="8692">be</TOKEN>
<TOKEN end_char="8702" id="token-87-7" morph="none" pos="word" start_char="8695">valuable</TOKEN>
<TOKEN end_char="8705" id="token-87-8" morph="none" pos="word" start_char="8704">in</TOKEN>
<TOKEN end_char="8715" id="token-87-9" morph="none" pos="word" start_char="8707">instances</TOKEN>
<TOKEN end_char="8721" id="token-87-10" morph="none" pos="word" start_char="8717">where</TOKEN>
<TOKEN end_char="8732" id="token-87-11" morph="none" pos="word" start_char="8723">individual</TOKEN>
<TOKEN end_char="8747" id="token-87-12" morph="none" pos="unknown" start_char="8734">false-positive</TOKEN>
<TOKEN end_char="8755" id="token-87-13" morph="none" pos="word" start_char="8749">results</TOKEN>
<TOKEN end_char="8762" id="token-87-14" morph="none" pos="word" start_char="8757">matter</TOKEN>
<TOKEN end_char="8767" id="token-87-15" morph="none" pos="word" start_char="8764">less</TOKEN>
<TOKEN end_char="8768" id="token-87-16" morph="none" pos="punct" start_char="8768">.</TOKEN>
</SEG>
<SEG end_char="8860" id="segment-88" start_char="8770">
<ORIGINAL_TEXT>That would be the case for surveys of entire populations, where errors can be factored out.</ORIGINAL_TEXT>
<TOKEN end_char="8773" id="token-88-0" morph="none" pos="word" start_char="8770">That</TOKEN>
<TOKEN end_char="8779" id="token-88-1" morph="none" pos="word" start_char="8775">would</TOKEN>
<TOKEN end_char="8782" id="token-88-2" morph="none" pos="word" start_char="8781">be</TOKEN>
<TOKEN end_char="8786" id="token-88-3" morph="none" pos="word" start_char="8784">the</TOKEN>
<TOKEN end_char="8791" id="token-88-4" morph="none" pos="word" start_char="8788">case</TOKEN>
<TOKEN end_char="8795" id="token-88-5" morph="none" pos="word" start_char="8793">for</TOKEN>
<TOKEN end_char="8803" id="token-88-6" morph="none" pos="word" start_char="8797">surveys</TOKEN>
<TOKEN end_char="8806" id="token-88-7" morph="none" pos="word" start_char="8805">of</TOKEN>
<TOKEN end_char="8813" id="token-88-8" morph="none" pos="word" start_char="8808">entire</TOKEN>
<TOKEN end_char="8825" id="token-88-9" morph="none" pos="word" start_char="8815">populations</TOKEN>
<TOKEN end_char="8826" id="token-88-10" morph="none" pos="punct" start_char="8826">,</TOKEN>
<TOKEN end_char="8832" id="token-88-11" morph="none" pos="word" start_char="8828">where</TOKEN>
<TOKEN end_char="8839" id="token-88-12" morph="none" pos="word" start_char="8834">errors</TOKEN>
<TOKEN end_char="8843" id="token-88-13" morph="none" pos="word" start_char="8841">can</TOKEN>
<TOKEN end_char="8846" id="token-88-14" morph="none" pos="word" start_char="8845">be</TOKEN>
<TOKEN end_char="8855" id="token-88-15" morph="none" pos="word" start_char="8848">factored</TOKEN>
<TOKEN end_char="8859" id="token-88-16" morph="none" pos="word" start_char="8857">out</TOKEN>
<TOKEN end_char="8860" id="token-88-17" morph="none" pos="punct" start_char="8860">.</TOKEN>
</SEG>
<SEG end_char="8994" id="segment-89" start_char="8863">
<ORIGINAL_TEXT>In these studies, antibody tests are being used to answer critical questions about where the coronavirus is and how prevalent it is.</ORIGINAL_TEXT>
<TOKEN end_char="8864" id="token-89-0" morph="none" pos="word" start_char="8863">In</TOKEN>
<TOKEN end_char="8870" id="token-89-1" morph="none" pos="word" start_char="8866">these</TOKEN>
<TOKEN end_char="8878" id="token-89-2" morph="none" pos="word" start_char="8872">studies</TOKEN>
<TOKEN end_char="8879" id="token-89-3" morph="none" pos="punct" start_char="8879">,</TOKEN>
<TOKEN end_char="8888" id="token-89-4" morph="none" pos="word" start_char="8881">antibody</TOKEN>
<TOKEN end_char="8894" id="token-89-5" morph="none" pos="word" start_char="8890">tests</TOKEN>
<TOKEN end_char="8898" id="token-89-6" morph="none" pos="word" start_char="8896">are</TOKEN>
<TOKEN end_char="8904" id="token-89-7" morph="none" pos="word" start_char="8900">being</TOKEN>
<TOKEN end_char="8909" id="token-89-8" morph="none" pos="word" start_char="8906">used</TOKEN>
<TOKEN end_char="8912" id="token-89-9" morph="none" pos="word" start_char="8911">to</TOKEN>
<TOKEN end_char="8919" id="token-89-10" morph="none" pos="word" start_char="8914">answer</TOKEN>
<TOKEN end_char="8928" id="token-89-11" morph="none" pos="word" start_char="8921">critical</TOKEN>
<TOKEN end_char="8938" id="token-89-12" morph="none" pos="word" start_char="8930">questions</TOKEN>
<TOKEN end_char="8944" id="token-89-13" morph="none" pos="word" start_char="8940">about</TOKEN>
<TOKEN end_char="8950" id="token-89-14" morph="none" pos="word" start_char="8946">where</TOKEN>
<TOKEN end_char="8954" id="token-89-15" morph="none" pos="word" start_char="8952">the</TOKEN>
<TOKEN end_char="8966" id="token-89-16" morph="none" pos="word" start_char="8956">coronavirus</TOKEN>
<TOKEN end_char="8969" id="token-89-17" morph="none" pos="word" start_char="8968">is</TOKEN>
<TOKEN end_char="8973" id="token-89-18" morph="none" pos="word" start_char="8971">and</TOKEN>
<TOKEN end_char="8977" id="token-89-19" morph="none" pos="word" start_char="8975">how</TOKEN>
<TOKEN end_char="8987" id="token-89-20" morph="none" pos="word" start_char="8979">prevalent</TOKEN>
<TOKEN end_char="8990" id="token-89-21" morph="none" pos="word" start_char="8989">it</TOKEN>
<TOKEN end_char="8993" id="token-89-22" morph="none" pos="word" start_char="8992">is</TOKEN>
<TOKEN end_char="8994" id="token-89-23" morph="none" pos="punct" start_char="8994">.</TOKEN>
</SEG>
<SEG end_char="9137" id="segment-90" start_char="8996">
<ORIGINAL_TEXT>That information can help officials plan how to let normal lives resume  but across populations as a whole, rather than one person at a time.</ORIGINAL_TEXT>
<TOKEN end_char="8999" id="token-90-0" morph="none" pos="word" start_char="8996">That</TOKEN>
<TOKEN end_char="9011" id="token-90-1" morph="none" pos="word" start_char="9001">information</TOKEN>
<TOKEN end_char="9015" id="token-90-2" morph="none" pos="word" start_char="9013">can</TOKEN>
<TOKEN end_char="9020" id="token-90-3" morph="none" pos="word" start_char="9017">help</TOKEN>
<TOKEN end_char="9030" id="token-90-4" morph="none" pos="word" start_char="9022">officials</TOKEN>
<TOKEN end_char="9035" id="token-90-5" morph="none" pos="word" start_char="9032">plan</TOKEN>
<TOKEN end_char="9039" id="token-90-6" morph="none" pos="word" start_char="9037">how</TOKEN>
<TOKEN end_char="9042" id="token-90-7" morph="none" pos="word" start_char="9041">to</TOKEN>
<TOKEN end_char="9046" id="token-90-8" morph="none" pos="word" start_char="9044">let</TOKEN>
<TOKEN end_char="9053" id="token-90-9" morph="none" pos="word" start_char="9048">normal</TOKEN>
<TOKEN end_char="9059" id="token-90-10" morph="none" pos="word" start_char="9055">lives</TOKEN>
<TOKEN end_char="9066" id="token-90-11" morph="none" pos="word" start_char="9061">resume</TOKEN>
<TOKEN end_char="9068" id="token-90-12" morph="none" pos="punct" start_char="9068"></TOKEN>
<TOKEN end_char="9072" id="token-90-13" morph="none" pos="word" start_char="9070">but</TOKEN>
<TOKEN end_char="9079" id="token-90-14" morph="none" pos="word" start_char="9074">across</TOKEN>
<TOKEN end_char="9091" id="token-90-15" morph="none" pos="word" start_char="9081">populations</TOKEN>
<TOKEN end_char="9094" id="token-90-16" morph="none" pos="word" start_char="9093">as</TOKEN>
<TOKEN end_char="9096" id="token-90-17" morph="none" pos="word" start_char="9096">a</TOKEN>
<TOKEN end_char="9102" id="token-90-18" morph="none" pos="word" start_char="9098">whole</TOKEN>
<TOKEN end_char="9103" id="token-90-19" morph="none" pos="punct" start_char="9103">,</TOKEN>
<TOKEN end_char="9110" id="token-90-20" morph="none" pos="word" start_char="9105">rather</TOKEN>
<TOKEN end_char="9115" id="token-90-21" morph="none" pos="word" start_char="9112">than</TOKEN>
<TOKEN end_char="9119" id="token-90-22" morph="none" pos="word" start_char="9117">one</TOKEN>
<TOKEN end_char="9126" id="token-90-23" morph="none" pos="word" start_char="9121">person</TOKEN>
<TOKEN end_char="9129" id="token-90-24" morph="none" pos="word" start_char="9128">at</TOKEN>
<TOKEN end_char="9131" id="token-90-25" morph="none" pos="word" start_char="9131">a</TOKEN>
<TOKEN end_char="9136" id="token-90-26" morph="none" pos="word" start_char="9133">time</TOKEN>
<TOKEN end_char="9137" id="token-90-27" morph="none" pos="punct" start_char="9137">.</TOKEN>
</SEG>
<SEG end_char="9196" id="segment-91" start_char="9140">
<ORIGINAL_TEXT>You can reach NPR Science Correspondent Richard Harris at</ORIGINAL_TEXT>
<TOKEN end_char="9142" id="token-91-0" morph="none" pos="word" start_char="9140">You</TOKEN>
<TOKEN end_char="9146" id="token-91-1" morph="none" pos="word" start_char="9144">can</TOKEN>
<TOKEN end_char="9152" id="token-91-2" morph="none" pos="word" start_char="9148">reach</TOKEN>
<TOKEN end_char="9156" id="token-91-3" morph="none" pos="word" start_char="9154">NPR</TOKEN>
<TOKEN end_char="9164" id="token-91-4" morph="none" pos="word" start_char="9158">Science</TOKEN>
<TOKEN end_char="9178" id="token-91-5" morph="none" pos="word" start_char="9166">Correspondent</TOKEN>
<TOKEN end_char="9186" id="token-91-6" morph="none" pos="word" start_char="9180">Richard</TOKEN>
<TOKEN end_char="9193" id="token-91-7" morph="none" pos="word" start_char="9188">Harris</TOKEN>
<TOKEN end_char="9196" id="token-91-8" morph="none" pos="word" start_char="9195">at</TOKEN>
</SEG>
<SEG end_char="9214" id="segment-92" start_char="9199">
<ORIGINAL_TEXT>rharris@npr.org.</ORIGINAL_TEXT>
<TOKEN end_char="9213" id="token-92-0" morph="none" pos="unknown" start_char="9199">rharris@npr.org</TOKEN>
<TOKEN end_char="9214" id="token-92-1" morph="none" pos="punct" start_char="9214">.</TOKEN>
<TRANSLATED_TEXT>@npr.org.</TRANSLATED_TEXT><DETECTED_LANGUAGE /></SEG>
<SEG end_char="9300" id="segment-93" start_char="9217">
<ORIGINAL_TEXT>A conversation with California biologist Andrew Cohen inspired this line of inquiry.</ORIGINAL_TEXT>
<TOKEN end_char="9217" id="token-93-0" morph="none" pos="word" start_char="9217">A</TOKEN>
<TOKEN end_char="9230" id="token-93-1" morph="none" pos="word" start_char="9219">conversation</TOKEN>
<TOKEN end_char="9235" id="token-93-2" morph="none" pos="word" start_char="9232">with</TOKEN>
<TOKEN end_char="9246" id="token-93-3" morph="none" pos="word" start_char="9237">California</TOKEN>
<TOKEN end_char="9256" id="token-93-4" morph="none" pos="word" start_char="9248">biologist</TOKEN>
<TOKEN end_char="9263" id="token-93-5" morph="none" pos="word" start_char="9258">Andrew</TOKEN>
<TOKEN end_char="9269" id="token-93-6" morph="none" pos="word" start_char="9265">Cohen</TOKEN>
<TOKEN end_char="9278" id="token-93-7" morph="none" pos="word" start_char="9271">inspired</TOKEN>
<TOKEN end_char="9283" id="token-93-8" morph="none" pos="word" start_char="9280">this</TOKEN>
<TOKEN end_char="9288" id="token-93-9" morph="none" pos="word" start_char="9285">line</TOKEN>
<TOKEN end_char="9291" id="token-93-10" morph="none" pos="word" start_char="9290">of</TOKEN>
<TOKEN end_char="9299" id="token-93-11" morph="none" pos="word" start_char="9293">inquiry</TOKEN>
<TOKEN end_char="9300" id="token-93-12" morph="none" pos="punct" start_char="9300">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
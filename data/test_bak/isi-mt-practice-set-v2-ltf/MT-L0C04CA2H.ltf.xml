<LCTL_TEXT lang="spa">
<DOC grammar="none" id="L0C04CA2H" lang="spa" raw_text_char_length="3647" raw_text_md5="b799be5de57353ddd208f78d0c069947" tokenization="tokenization_parameters.v5.0">
<TEXT>
<SEG end_char="87" id="segment-0" start_char="1">
<ORIGINAL_TEXT>Fact Check: Were System And Method Of Testing For Coronavirus Testing Patented In 2015?</ORIGINAL_TEXT>
<TOKEN end_char="4" id="token-0-0" morph="none" pos="word" start_char="1">Fact</TOKEN>
<TOKEN end_char="10" id="token-0-1" morph="none" pos="word" start_char="6">Check</TOKEN>
<TOKEN end_char="11" id="token-0-2" morph="none" pos="punct" start_char="11">:</TOKEN>
<TOKEN end_char="16" id="token-0-3" morph="none" pos="word" start_char="13">Were</TOKEN>
<TOKEN end_char="23" id="token-0-4" morph="none" pos="word" start_char="18">System</TOKEN>
<TOKEN end_char="27" id="token-0-5" morph="none" pos="word" start_char="25">And</TOKEN>
<TOKEN end_char="34" id="token-0-6" morph="none" pos="word" start_char="29">Method</TOKEN>
<TOKEN end_char="37" id="token-0-7" morph="none" pos="word" start_char="36">Of</TOKEN>
<TOKEN end_char="45" id="token-0-8" morph="none" pos="word" start_char="39">Testing</TOKEN>
<TOKEN end_char="49" id="token-0-9" morph="none" pos="word" start_char="47">For</TOKEN>
<TOKEN end_char="61" id="token-0-10" morph="none" pos="word" start_char="51">Coronavirus</TOKEN>
<TOKEN end_char="69" id="token-0-11" morph="none" pos="word" start_char="63">Testing</TOKEN>
<TOKEN end_char="78" id="token-0-12" morph="none" pos="word" start_char="71">Patented</TOKEN>
<TOKEN end_char="81" id="token-0-13" morph="none" pos="word" start_char="80">In</TOKEN>
<TOKEN end_char="86" id="token-0-14" morph="none" pos="word" start_char="83">2015</TOKEN>
<TOKEN end_char="87" id="token-0-15" morph="none" pos="punct" start_char="87">?</TOKEN>
</SEG>
<SEG end_char="191" id="segment-1" start_char="92">
<ORIGINAL_TEXT>Many conspiracy theories have been doing the rounds on the internet since the inception of Covid-19.</ORIGINAL_TEXT>
<TOKEN end_char="95" id="token-1-0" morph="none" pos="word" start_char="92">Many</TOKEN>
<TOKEN end_char="106" id="token-1-1" morph="none" pos="word" start_char="97">conspiracy</TOKEN>
<TOKEN end_char="115" id="token-1-2" morph="none" pos="word" start_char="108">theories</TOKEN>
<TOKEN end_char="120" id="token-1-3" morph="none" pos="word" start_char="117">have</TOKEN>
<TOKEN end_char="125" id="token-1-4" morph="none" pos="word" start_char="122">been</TOKEN>
<TOKEN end_char="131" id="token-1-5" morph="none" pos="word" start_char="127">doing</TOKEN>
<TOKEN end_char="135" id="token-1-6" morph="none" pos="word" start_char="133">the</TOKEN>
<TOKEN end_char="142" id="token-1-7" morph="none" pos="word" start_char="137">rounds</TOKEN>
<TOKEN end_char="145" id="token-1-8" morph="none" pos="word" start_char="144">on</TOKEN>
<TOKEN end_char="149" id="token-1-9" morph="none" pos="word" start_char="147">the</TOKEN>
<TOKEN end_char="158" id="token-1-10" morph="none" pos="word" start_char="151">internet</TOKEN>
<TOKEN end_char="164" id="token-1-11" morph="none" pos="word" start_char="160">since</TOKEN>
<TOKEN end_char="168" id="token-1-12" morph="none" pos="word" start_char="166">the</TOKEN>
<TOKEN end_char="178" id="token-1-13" morph="none" pos="word" start_char="170">inception</TOKEN>
<TOKEN end_char="181" id="token-1-14" morph="none" pos="word" start_char="180">of</TOKEN>
<TOKEN end_char="190" id="token-1-15" morph="none" pos="unknown" start_char="183">Covid-19</TOKEN>
<TOKEN end_char="191" id="token-1-16" morph="none" pos="punct" start_char="191">.</TOKEN>
</SEG>
<SEG end_char="289" id="segment-2" start_char="193">
<ORIGINAL_TEXT>Many theories have been circulated about how Covid-19 was a scam or how it was already predicted.</ORIGINAL_TEXT>
<TOKEN end_char="196" id="token-2-0" morph="none" pos="word" start_char="193">Many</TOKEN>
<TOKEN end_char="205" id="token-2-1" morph="none" pos="word" start_char="198">theories</TOKEN>
<TOKEN end_char="210" id="token-2-2" morph="none" pos="word" start_char="207">have</TOKEN>
<TOKEN end_char="215" id="token-2-3" morph="none" pos="word" start_char="212">been</TOKEN>
<TOKEN end_char="226" id="token-2-4" morph="none" pos="word" start_char="217">circulated</TOKEN>
<TOKEN end_char="232" id="token-2-5" morph="none" pos="word" start_char="228">about</TOKEN>
<TOKEN end_char="236" id="token-2-6" morph="none" pos="word" start_char="234">how</TOKEN>
<TOKEN end_char="245" id="token-2-7" morph="none" pos="unknown" start_char="238">Covid-19</TOKEN>
<TOKEN end_char="249" id="token-2-8" morph="none" pos="word" start_char="247">was</TOKEN>
<TOKEN end_char="251" id="token-2-9" morph="none" pos="word" start_char="251">a</TOKEN>
<TOKEN end_char="256" id="token-2-10" morph="none" pos="word" start_char="253">scam</TOKEN>
<TOKEN end_char="259" id="token-2-11" morph="none" pos="word" start_char="258">or</TOKEN>
<TOKEN end_char="263" id="token-2-12" morph="none" pos="word" start_char="261">how</TOKEN>
<TOKEN end_char="266" id="token-2-13" morph="none" pos="word" start_char="265">it</TOKEN>
<TOKEN end_char="270" id="token-2-14" morph="none" pos="word" start_char="268">was</TOKEN>
<TOKEN end_char="278" id="token-2-15" morph="none" pos="word" start_char="272">already</TOKEN>
<TOKEN end_char="288" id="token-2-16" morph="none" pos="word" start_char="280">predicted</TOKEN>
<TOKEN end_char="289" id="token-2-17" morph="none" pos="punct" start_char="289">.</TOKEN>
</SEG>
<SEG end_char="369" id="segment-3" start_char="292">
<ORIGINAL_TEXT>In the backdrop of the same context, another social media post has gone viral.</ORIGINAL_TEXT>
<TOKEN end_char="293" id="token-3-0" morph="none" pos="word" start_char="292">In</TOKEN>
<TOKEN end_char="297" id="token-3-1" morph="none" pos="word" start_char="295">the</TOKEN>
<TOKEN end_char="306" id="token-3-2" morph="none" pos="word" start_char="299">backdrop</TOKEN>
<TOKEN end_char="309" id="token-3-3" morph="none" pos="word" start_char="308">of</TOKEN>
<TOKEN end_char="313" id="token-3-4" morph="none" pos="word" start_char="311">the</TOKEN>
<TOKEN end_char="318" id="token-3-5" morph="none" pos="word" start_char="315">same</TOKEN>
<TOKEN end_char="326" id="token-3-6" morph="none" pos="word" start_char="320">context</TOKEN>
<TOKEN end_char="327" id="token-3-7" morph="none" pos="punct" start_char="327">,</TOKEN>
<TOKEN end_char="335" id="token-3-8" morph="none" pos="word" start_char="329">another</TOKEN>
<TOKEN end_char="342" id="token-3-9" morph="none" pos="word" start_char="337">social</TOKEN>
<TOKEN end_char="348" id="token-3-10" morph="none" pos="word" start_char="344">media</TOKEN>
<TOKEN end_char="353" id="token-3-11" morph="none" pos="word" start_char="350">post</TOKEN>
<TOKEN end_char="357" id="token-3-12" morph="none" pos="word" start_char="355">has</TOKEN>
<TOKEN end_char="362" id="token-3-13" morph="none" pos="word" start_char="359">gone</TOKEN>
<TOKEN end_char="368" id="token-3-14" morph="none" pos="word" start_char="364">viral</TOKEN>
<TOKEN end_char="369" id="token-3-15" morph="none" pos="punct" start_char="369">.</TOKEN>
</SEG>
<SEG end_char="463" id="segment-4" start_char="371">
<ORIGINAL_TEXT>This post says that the system and method for testing Covid-19 were already patented in 2015.</ORIGINAL_TEXT>
<TOKEN end_char="374" id="token-4-0" morph="none" pos="word" start_char="371">This</TOKEN>
<TOKEN end_char="379" id="token-4-1" morph="none" pos="word" start_char="376">post</TOKEN>
<TOKEN end_char="384" id="token-4-2" morph="none" pos="word" start_char="381">says</TOKEN>
<TOKEN end_char="389" id="token-4-3" morph="none" pos="word" start_char="386">that</TOKEN>
<TOKEN end_char="393" id="token-4-4" morph="none" pos="word" start_char="391">the</TOKEN>
<TOKEN end_char="400" id="token-4-5" morph="none" pos="word" start_char="395">system</TOKEN>
<TOKEN end_char="404" id="token-4-6" morph="none" pos="word" start_char="402">and</TOKEN>
<TOKEN end_char="411" id="token-4-7" morph="none" pos="word" start_char="406">method</TOKEN>
<TOKEN end_char="415" id="token-4-8" morph="none" pos="word" start_char="413">for</TOKEN>
<TOKEN end_char="423" id="token-4-9" morph="none" pos="word" start_char="417">testing</TOKEN>
<TOKEN end_char="432" id="token-4-10" morph="none" pos="unknown" start_char="425">Covid-19</TOKEN>
<TOKEN end_char="437" id="token-4-11" morph="none" pos="word" start_char="434">were</TOKEN>
<TOKEN end_char="445" id="token-4-12" morph="none" pos="word" start_char="439">already</TOKEN>
<TOKEN end_char="454" id="token-4-13" morph="none" pos="word" start_char="447">patented</TOKEN>
<TOKEN end_char="457" id="token-4-14" morph="none" pos="word" start_char="456">in</TOKEN>
<TOKEN end_char="462" id="token-4-15" morph="none" pos="word" start_char="459">2015</TOKEN>
<TOKEN end_char="463" id="token-4-16" morph="none" pos="punct" start_char="463">.</TOKEN>
</SEG>
<SEG end_char="630" id="segment-5" start_char="465">
<ORIGINAL_TEXT>The claim is being shared with an image that shows a supplemental application that was filed in 2020, followed by the provisional application which was filed in 2015.</ORIGINAL_TEXT>
<TOKEN end_char="467" id="token-5-0" morph="none" pos="word" start_char="465">The</TOKEN>
<TOKEN end_char="473" id="token-5-1" morph="none" pos="word" start_char="469">claim</TOKEN>
<TOKEN end_char="476" id="token-5-2" morph="none" pos="word" start_char="475">is</TOKEN>
<TOKEN end_char="482" id="token-5-3" morph="none" pos="word" start_char="478">being</TOKEN>
<TOKEN end_char="489" id="token-5-4" morph="none" pos="word" start_char="484">shared</TOKEN>
<TOKEN end_char="494" id="token-5-5" morph="none" pos="word" start_char="491">with</TOKEN>
<TOKEN end_char="497" id="token-5-6" morph="none" pos="word" start_char="496">an</TOKEN>
<TOKEN end_char="503" id="token-5-7" morph="none" pos="word" start_char="499">image</TOKEN>
<TOKEN end_char="508" id="token-5-8" morph="none" pos="word" start_char="505">that</TOKEN>
<TOKEN end_char="514" id="token-5-9" morph="none" pos="word" start_char="510">shows</TOKEN>
<TOKEN end_char="516" id="token-5-10" morph="none" pos="word" start_char="516">a</TOKEN>
<TOKEN end_char="529" id="token-5-11" morph="none" pos="word" start_char="518">supplemental</TOKEN>
<TOKEN end_char="541" id="token-5-12" morph="none" pos="word" start_char="531">application</TOKEN>
<TOKEN end_char="546" id="token-5-13" morph="none" pos="word" start_char="543">that</TOKEN>
<TOKEN end_char="550" id="token-5-14" morph="none" pos="word" start_char="548">was</TOKEN>
<TOKEN end_char="556" id="token-5-15" morph="none" pos="word" start_char="552">filed</TOKEN>
<TOKEN end_char="559" id="token-5-16" morph="none" pos="word" start_char="558">in</TOKEN>
<TOKEN end_char="564" id="token-5-17" morph="none" pos="word" start_char="561">2020</TOKEN>
<TOKEN end_char="565" id="token-5-18" morph="none" pos="punct" start_char="565">,</TOKEN>
<TOKEN end_char="574" id="token-5-19" morph="none" pos="word" start_char="567">followed</TOKEN>
<TOKEN end_char="577" id="token-5-20" morph="none" pos="word" start_char="576">by</TOKEN>
<TOKEN end_char="581" id="token-5-21" morph="none" pos="word" start_char="579">the</TOKEN>
<TOKEN end_char="593" id="token-5-22" morph="none" pos="word" start_char="583">provisional</TOKEN>
<TOKEN end_char="605" id="token-5-23" morph="none" pos="word" start_char="595">application</TOKEN>
<TOKEN end_char="611" id="token-5-24" morph="none" pos="word" start_char="607">which</TOKEN>
<TOKEN end_char="615" id="token-5-25" morph="none" pos="word" start_char="613">was</TOKEN>
<TOKEN end_char="621" id="token-5-26" morph="none" pos="word" start_char="617">filed</TOKEN>
<TOKEN end_char="624" id="token-5-27" morph="none" pos="word" start_char="623">in</TOKEN>
<TOKEN end_char="629" id="token-5-28" morph="none" pos="word" start_char="626">2015</TOKEN>
<TOKEN end_char="630" id="token-5-29" morph="none" pos="punct" start_char="630">.</TOKEN>
</SEG>
<SEG end_char="751" id="segment-6" start_char="633">
<ORIGINAL_TEXT>One of the captions says, "For those of you who don't think that this COVID-19 Scam wasn't planned years in advance ...</ORIGINAL_TEXT>
<TOKEN end_char="635" id="token-6-0" morph="none" pos="word" start_char="633">One</TOKEN>
<TOKEN end_char="638" id="token-6-1" morph="none" pos="word" start_char="637">of</TOKEN>
<TOKEN end_char="642" id="token-6-2" morph="none" pos="word" start_char="640">the</TOKEN>
<TOKEN end_char="651" id="token-6-3" morph="none" pos="word" start_char="644">captions</TOKEN>
<TOKEN end_char="656" id="token-6-4" morph="none" pos="word" start_char="653">says</TOKEN>
<TOKEN end_char="657" id="token-6-5" morph="none" pos="punct" start_char="657">,</TOKEN>
<TOKEN end_char="659" id="token-6-6" morph="none" pos="punct" start_char="659">"</TOKEN>
<TOKEN end_char="662" id="token-6-7" morph="none" pos="word" start_char="660">For</TOKEN>
<TOKEN end_char="668" id="token-6-8" morph="none" pos="word" start_char="664">those</TOKEN>
<TOKEN end_char="671" id="token-6-9" morph="none" pos="word" start_char="670">of</TOKEN>
<TOKEN end_char="675" id="token-6-10" morph="none" pos="word" start_char="673">you</TOKEN>
<TOKEN end_char="679" id="token-6-11" morph="none" pos="word" start_char="677">who</TOKEN>
<TOKEN end_char="685" id="token-6-12" morph="none" pos="word" start_char="681">don't</TOKEN>
<TOKEN end_char="691" id="token-6-13" morph="none" pos="word" start_char="687">think</TOKEN>
<TOKEN end_char="696" id="token-6-14" morph="none" pos="word" start_char="693">that</TOKEN>
<TOKEN end_char="701" id="token-6-15" morph="none" pos="word" start_char="698">this</TOKEN>
<TOKEN end_char="710" id="token-6-16" morph="none" pos="unknown" start_char="703">COVID-19</TOKEN>
<TOKEN end_char="715" id="token-6-17" morph="none" pos="word" start_char="712">Scam</TOKEN>
<TOKEN end_char="722" id="token-6-18" morph="none" pos="word" start_char="717">wasn't</TOKEN>
<TOKEN end_char="730" id="token-6-19" morph="none" pos="word" start_char="724">planned</TOKEN>
<TOKEN end_char="736" id="token-6-20" morph="none" pos="word" start_char="732">years</TOKEN>
<TOKEN end_char="739" id="token-6-21" morph="none" pos="word" start_char="738">in</TOKEN>
<TOKEN end_char="747" id="token-6-22" morph="none" pos="word" start_char="741">advance</TOKEN>
<TOKEN end_char="751" id="token-6-23" morph="none" pos="punct" start_char="749">...</TOKEN>
</SEG>
<SEG end_char="831" id="segment-7" start_char="753">
<ORIGINAL_TEXT>PLEASE explain how a Rothchild filed for a COVID-19 patent BACK IN 2015 ?!?! ..</ORIGINAL_TEXT>
<TOKEN end_char="758" id="token-7-0" morph="none" pos="word" start_char="753">PLEASE</TOKEN>
<TOKEN end_char="766" id="token-7-1" morph="none" pos="word" start_char="760">explain</TOKEN>
<TOKEN end_char="770" id="token-7-2" morph="none" pos="word" start_char="768">how</TOKEN>
<TOKEN end_char="772" id="token-7-3" morph="none" pos="word" start_char="772">a</TOKEN>
<TOKEN end_char="782" id="token-7-4" morph="none" pos="word" start_char="774">Rothchild</TOKEN>
<TOKEN end_char="788" id="token-7-5" morph="none" pos="word" start_char="784">filed</TOKEN>
<TOKEN end_char="792" id="token-7-6" morph="none" pos="word" start_char="790">for</TOKEN>
<TOKEN end_char="794" id="token-7-7" morph="none" pos="word" start_char="794">a</TOKEN>
<TOKEN end_char="803" id="token-7-8" morph="none" pos="unknown" start_char="796">COVID-19</TOKEN>
<TOKEN end_char="810" id="token-7-9" morph="none" pos="word" start_char="805">patent</TOKEN>
<TOKEN end_char="815" id="token-7-10" morph="none" pos="word" start_char="812">BACK</TOKEN>
<TOKEN end_char="818" id="token-7-11" morph="none" pos="word" start_char="817">IN</TOKEN>
<TOKEN end_char="823" id="token-7-12" morph="none" pos="word" start_char="820">2015</TOKEN>
<TOKEN end_char="828" id="token-7-13" morph="none" pos="punct" start_char="825">?!?!</TOKEN>
<TOKEN end_char="831" id="token-7-14" morph="none" pos="punct" start_char="830">..</TOKEN>
</SEG>
<SEG end_char="847" id="segment-8" start_char="833">
<ORIGINAL_TEXT>5 YEARS AGO ?!?</ORIGINAL_TEXT>
<TOKEN end_char="833" id="token-8-0" morph="none" pos="word" start_char="833">5</TOKEN>
<TOKEN end_char="839" id="token-8-1" morph="none" pos="word" start_char="835">YEARS</TOKEN>
<TOKEN end_char="843" id="token-8-2" morph="none" pos="word" start_char="841">AGO</TOKEN>
<TOKEN end_char="847" id="token-8-3" morph="none" pos="punct" start_char="845">?!?</TOKEN>
<TRANSLATED_TEXT>5 YEARS AGO?</TRANSLATED_TEXT><DETECTED_LANGUAGE>so</DETECTED_LANGUAGE></SEG>
<SEG end_char="897" id="segment-9" start_char="849">
<ORIGINAL_TEXT>(Larry Melanchuk is your head still in the sand?"</ORIGINAL_TEXT>
<TOKEN end_char="849" id="token-9-0" morph="none" pos="punct" start_char="849">(</TOKEN>
<TOKEN end_char="854" id="token-9-1" morph="none" pos="word" start_char="850">Larry</TOKEN>
<TOKEN end_char="864" id="token-9-2" morph="none" pos="word" start_char="856">Melanchuk</TOKEN>
<TOKEN end_char="867" id="token-9-3" morph="none" pos="word" start_char="866">is</TOKEN>
<TOKEN end_char="872" id="token-9-4" morph="none" pos="word" start_char="869">your</TOKEN>
<TOKEN end_char="877" id="token-9-5" morph="none" pos="word" start_char="874">head</TOKEN>
<TOKEN end_char="883" id="token-9-6" morph="none" pos="word" start_char="879">still</TOKEN>
<TOKEN end_char="886" id="token-9-7" morph="none" pos="word" start_char="885">in</TOKEN>
<TOKEN end_char="890" id="token-9-8" morph="none" pos="word" start_char="888">the</TOKEN>
<TOKEN end_char="895" id="token-9-9" morph="none" pos="word" start_char="892">sand</TOKEN>
<TOKEN end_char="897" id="token-9-10" morph="none" pos="punct" start_char="896">?"</TOKEN>
</SEG>
<SEG end_char="1020" id="segment-10" start_char="900">
<ORIGINAL_TEXT>A similar claim was shared by Facebook user Ron Lloyd with the caption, "All the obedient sheeple should check the dates!</ORIGINAL_TEXT>
<TOKEN end_char="900" id="token-10-0" morph="none" pos="word" start_char="900">A</TOKEN>
<TOKEN end_char="908" id="token-10-1" morph="none" pos="word" start_char="902">similar</TOKEN>
<TOKEN end_char="914" id="token-10-2" morph="none" pos="word" start_char="910">claim</TOKEN>
<TOKEN end_char="918" id="token-10-3" morph="none" pos="word" start_char="916">was</TOKEN>
<TOKEN end_char="925" id="token-10-4" morph="none" pos="word" start_char="920">shared</TOKEN>
<TOKEN end_char="928" id="token-10-5" morph="none" pos="word" start_char="927">by</TOKEN>
<TOKEN end_char="937" id="token-10-6" morph="none" pos="word" start_char="930">Facebook</TOKEN>
<TOKEN end_char="942" id="token-10-7" morph="none" pos="word" start_char="939">user</TOKEN>
<TOKEN end_char="946" id="token-10-8" morph="none" pos="word" start_char="944">Ron</TOKEN>
<TOKEN end_char="952" id="token-10-9" morph="none" pos="word" start_char="948">Lloyd</TOKEN>
<TOKEN end_char="957" id="token-10-10" morph="none" pos="word" start_char="954">with</TOKEN>
<TOKEN end_char="961" id="token-10-11" morph="none" pos="word" start_char="959">the</TOKEN>
<TOKEN end_char="969" id="token-10-12" morph="none" pos="word" start_char="963">caption</TOKEN>
<TOKEN end_char="970" id="token-10-13" morph="none" pos="punct" start_char="970">,</TOKEN>
<TOKEN end_char="972" id="token-10-14" morph="none" pos="punct" start_char="972">"</TOKEN>
<TOKEN end_char="975" id="token-10-15" morph="none" pos="word" start_char="973">All</TOKEN>
<TOKEN end_char="979" id="token-10-16" morph="none" pos="word" start_char="977">the</TOKEN>
<TOKEN end_char="988" id="token-10-17" morph="none" pos="word" start_char="981">obedient</TOKEN>
<TOKEN end_char="996" id="token-10-18" morph="none" pos="word" start_char="990">sheeple</TOKEN>
<TOKEN end_char="1003" id="token-10-19" morph="none" pos="word" start_char="998">should</TOKEN>
<TOKEN end_char="1009" id="token-10-20" morph="none" pos="word" start_char="1005">check</TOKEN>
<TOKEN end_char="1013" id="token-10-21" morph="none" pos="word" start_char="1011">the</TOKEN>
<TOKEN end_char="1019" id="token-10-22" morph="none" pos="word" start_char="1015">dates</TOKEN>
<TOKEN end_char="1020" id="token-10-23" morph="none" pos="punct" start_char="1020">!</TOKEN>
</SEG>
<SEG end_char="1047" id="segment-11" start_char="1022">
<ORIGINAL_TEXT>You've all been scammed!".</ORIGINAL_TEXT>
<TOKEN end_char="1027" id="token-11-0" morph="none" pos="word" start_char="1022">You've</TOKEN>
<TOKEN end_char="1031" id="token-11-1" morph="none" pos="word" start_char="1029">all</TOKEN>
<TOKEN end_char="1036" id="token-11-2" morph="none" pos="word" start_char="1033">been</TOKEN>
<TOKEN end_char="1044" id="token-11-3" morph="none" pos="word" start_char="1038">scammed</TOKEN>
<TOKEN end_char="1047" id="token-11-4" morph="none" pos="punct" start_char="1045">!".</TOKEN>
</SEG>
<SEG end_char="1174" id="segment-12" start_char="1050">
<ORIGINAL_TEXT>All the obedient sheeple should check the dates‚ùó Youve all been scammed‚ùó üëáüëáüëáüëáPosted by Ron Lloyd on Wednesday, 7 October 2020</ORIGINAL_TEXT>
<TOKEN end_char="1052" id="token-12-0" morph="none" pos="word" start_char="1050">All</TOKEN>
<TOKEN end_char="1056" id="token-12-1" morph="none" pos="word" start_char="1054">the</TOKEN>
<TOKEN end_char="1065" id="token-12-2" morph="none" pos="word" start_char="1058">obedient</TOKEN>
<TOKEN end_char="1073" id="token-12-3" morph="none" pos="word" start_char="1067">sheeple</TOKEN>
<TOKEN end_char="1080" id="token-12-4" morph="none" pos="word" start_char="1075">should</TOKEN>
<TOKEN end_char="1086" id="token-12-5" morph="none" pos="word" start_char="1082">check</TOKEN>
<TOKEN end_char="1090" id="token-12-6" morph="none" pos="word" start_char="1088">the</TOKEN>
<TOKEN end_char="1097" id="token-12-7" morph="none" pos="unknown" start_char="1092">dates‚ùó</TOKEN>
<TOKEN end_char="1103" id="token-12-8" morph="none" pos="word" start_char="1099">Youve</TOKEN>
<TOKEN end_char="1107" id="token-12-9" morph="none" pos="word" start_char="1105">all</TOKEN>
<TOKEN end_char="1112" id="token-12-10" morph="none" pos="word" start_char="1109">been</TOKEN>
<TOKEN end_char="1121" id="token-12-11" morph="none" pos="unknown" start_char="1114">scammed‚ùó</TOKEN>
<TOKEN end_char="1132" id="token-12-12" morph="none" pos="unknown" start_char="1123">üëáüëáüëáüëáPosted</TOKEN>
<TOKEN end_char="1135" id="token-12-13" morph="none" pos="word" start_char="1134">by</TOKEN>
<TOKEN end_char="1139" id="token-12-14" morph="none" pos="word" start_char="1137">Ron</TOKEN>
<TOKEN end_char="1145" id="token-12-15" morph="none" pos="word" start_char="1141">Lloyd</TOKEN>
<TOKEN end_char="1148" id="token-12-16" morph="none" pos="word" start_char="1147">on</TOKEN>
<TOKEN end_char="1158" id="token-12-17" morph="none" pos="word" start_char="1150">Wednesday</TOKEN>
<TOKEN end_char="1159" id="token-12-18" morph="none" pos="punct" start_char="1159">,</TOKEN>
<TOKEN end_char="1161" id="token-12-19" morph="none" pos="word" start_char="1161">7</TOKEN>
<TOKEN end_char="1169" id="token-12-20" morph="none" pos="word" start_char="1163">October</TOKEN>
<TOKEN end_char="1174" id="token-12-21" morph="none" pos="word" start_char="1171">2020</TOKEN>
</SEG>
<SEG end_char="1183" id="segment-13" start_char="1178">
<ORIGINAL_TEXT>Claim:</ORIGINAL_TEXT>
<TOKEN end_char="1182" id="token-13-0" morph="none" pos="word" start_char="1178">Claim</TOKEN>
<TOKEN end_char="1183" id="token-13-1" morph="none" pos="punct" start_char="1183">:</TOKEN>
</SEG>
<SEG end_char="1319" id="segment-14" start_char="1186">
<ORIGINAL_TEXT>The provisional application for system and method for testing Covid-19 was already filed in 2015, 5 years before coronavirus pandemic.</ORIGINAL_TEXT>
<TOKEN end_char="1188" id="token-14-0" morph="none" pos="word" start_char="1186">The</TOKEN>
<TOKEN end_char="1200" id="token-14-1" morph="none" pos="word" start_char="1190">provisional</TOKEN>
<TOKEN end_char="1212" id="token-14-2" morph="none" pos="word" start_char="1202">application</TOKEN>
<TOKEN end_char="1216" id="token-14-3" morph="none" pos="word" start_char="1214">for</TOKEN>
<TOKEN end_char="1223" id="token-14-4" morph="none" pos="word" start_char="1218">system</TOKEN>
<TOKEN end_char="1227" id="token-14-5" morph="none" pos="word" start_char="1225">and</TOKEN>
<TOKEN end_char="1234" id="token-14-6" morph="none" pos="word" start_char="1229">method</TOKEN>
<TOKEN end_char="1238" id="token-14-7" morph="none" pos="word" start_char="1236">for</TOKEN>
<TOKEN end_char="1246" id="token-14-8" morph="none" pos="word" start_char="1240">testing</TOKEN>
<TOKEN end_char="1255" id="token-14-9" morph="none" pos="unknown" start_char="1248">Covid-19</TOKEN>
<TOKEN end_char="1259" id="token-14-10" morph="none" pos="word" start_char="1257">was</TOKEN>
<TOKEN end_char="1267" id="token-14-11" morph="none" pos="word" start_char="1261">already</TOKEN>
<TOKEN end_char="1273" id="token-14-12" morph="none" pos="word" start_char="1269">filed</TOKEN>
<TOKEN end_char="1276" id="token-14-13" morph="none" pos="word" start_char="1275">in</TOKEN>
<TOKEN end_char="1281" id="token-14-14" morph="none" pos="word" start_char="1278">2015</TOKEN>
<TOKEN end_char="1282" id="token-14-15" morph="none" pos="punct" start_char="1282">,</TOKEN>
<TOKEN end_char="1284" id="token-14-16" morph="none" pos="word" start_char="1284">5</TOKEN>
<TOKEN end_char="1290" id="token-14-17" morph="none" pos="word" start_char="1286">years</TOKEN>
<TOKEN end_char="1297" id="token-14-18" morph="none" pos="word" start_char="1292">before</TOKEN>
<TOKEN end_char="1309" id="token-14-19" morph="none" pos="word" start_char="1299">coronavirus</TOKEN>
<TOKEN end_char="1318" id="token-14-20" morph="none" pos="word" start_char="1311">pandemic</TOKEN>
<TOKEN end_char="1319" id="token-14-21" morph="none" pos="punct" start_char="1319">.</TOKEN>
</SEG>
<SEG end_char="1332" id="segment-15" start_char="1322">
<ORIGINAL_TEXT>Fact Check:</ORIGINAL_TEXT>
<TOKEN end_char="1325" id="token-15-0" morph="none" pos="word" start_char="1322">Fact</TOKEN>
<TOKEN end_char="1331" id="token-15-1" morph="none" pos="word" start_char="1327">Check</TOKEN>
<TOKEN end_char="1332" id="token-15-2" morph="none" pos="punct" start_char="1332">:</TOKEN>
</SEG>
<SEG end_char="1441" id="segment-16" start_char="1336">
<ORIGINAL_TEXT>The Logical Indian used the keyword 'system and method for testing Covid-19' and found a patent on Google.</ORIGINAL_TEXT>
<TOKEN end_char="1338" id="token-16-0" morph="none" pos="word" start_char="1336">The</TOKEN>
<TOKEN end_char="1346" id="token-16-1" morph="none" pos="word" start_char="1340">Logical</TOKEN>
<TOKEN end_char="1353" id="token-16-2" morph="none" pos="word" start_char="1348">Indian</TOKEN>
<TOKEN end_char="1358" id="token-16-3" morph="none" pos="word" start_char="1355">used</TOKEN>
<TOKEN end_char="1362" id="token-16-4" morph="none" pos="word" start_char="1360">the</TOKEN>
<TOKEN end_char="1370" id="token-16-5" morph="none" pos="word" start_char="1364">keyword</TOKEN>
<TOKEN end_char="1372" id="token-16-6" morph="none" pos="punct" start_char="1372">'</TOKEN>
<TOKEN end_char="1378" id="token-16-7" morph="none" pos="word" start_char="1373">system</TOKEN>
<TOKEN end_char="1382" id="token-16-8" morph="none" pos="word" start_char="1380">and</TOKEN>
<TOKEN end_char="1389" id="token-16-9" morph="none" pos="word" start_char="1384">method</TOKEN>
<TOKEN end_char="1393" id="token-16-10" morph="none" pos="word" start_char="1391">for</TOKEN>
<TOKEN end_char="1401" id="token-16-11" morph="none" pos="word" start_char="1395">testing</TOKEN>
<TOKEN end_char="1410" id="token-16-12" morph="none" pos="unknown" start_char="1403">Covid-19</TOKEN>
<TOKEN end_char="1411" id="token-16-13" morph="none" pos="punct" start_char="1411">'</TOKEN>
<TOKEN end_char="1415" id="token-16-14" morph="none" pos="word" start_char="1413">and</TOKEN>
<TOKEN end_char="1421" id="token-16-15" morph="none" pos="word" start_char="1417">found</TOKEN>
<TOKEN end_char="1423" id="token-16-16" morph="none" pos="word" start_char="1423">a</TOKEN>
<TOKEN end_char="1430" id="token-16-17" morph="none" pos="word" start_char="1425">patent</TOKEN>
<TOKEN end_char="1433" id="token-16-18" morph="none" pos="word" start_char="1432">on</TOKEN>
<TOKEN end_char="1440" id="token-16-19" morph="none" pos="word" start_char="1435">Google</TOKEN>
<TOKEN end_char="1441" id="token-16-20" morph="none" pos="punct" start_char="1441">.</TOKEN>
</SEG>
<SEG end_char="1574" id="segment-17" start_char="1443">
<ORIGINAL_TEXT>The patent was found on the website, with the application, 'System and Method for Testing for COVID-19' filed by Rothschild Richard.</ORIGINAL_TEXT>
<TOKEN end_char="1445" id="token-17-0" morph="none" pos="word" start_char="1443">The</TOKEN>
<TOKEN end_char="1452" id="token-17-1" morph="none" pos="word" start_char="1447">patent</TOKEN>
<TOKEN end_char="1456" id="token-17-2" morph="none" pos="word" start_char="1454">was</TOKEN>
<TOKEN end_char="1462" id="token-17-3" morph="none" pos="word" start_char="1458">found</TOKEN>
<TOKEN end_char="1465" id="token-17-4" morph="none" pos="word" start_char="1464">on</TOKEN>
<TOKEN end_char="1469" id="token-17-5" morph="none" pos="word" start_char="1467">the</TOKEN>
<TOKEN end_char="1477" id="token-17-6" morph="none" pos="word" start_char="1471">website</TOKEN>
<TOKEN end_char="1478" id="token-17-7" morph="none" pos="punct" start_char="1478">,</TOKEN>
<TOKEN end_char="1483" id="token-17-8" morph="none" pos="word" start_char="1480">with</TOKEN>
<TOKEN end_char="1487" id="token-17-9" morph="none" pos="word" start_char="1485">the</TOKEN>
<TOKEN end_char="1499" id="token-17-10" morph="none" pos="word" start_char="1489">application</TOKEN>
<TOKEN end_char="1500" id="token-17-11" morph="none" pos="punct" start_char="1500">,</TOKEN>
<TOKEN end_char="1502" id="token-17-12" morph="none" pos="punct" start_char="1502">'</TOKEN>
<TOKEN end_char="1508" id="token-17-13" morph="none" pos="word" start_char="1503">System</TOKEN>
<TOKEN end_char="1512" id="token-17-14" morph="none" pos="word" start_char="1510">and</TOKEN>
<TOKEN end_char="1519" id="token-17-15" morph="none" pos="word" start_char="1514">Method</TOKEN>
<TOKEN end_char="1523" id="token-17-16" morph="none" pos="word" start_char="1521">for</TOKEN>
<TOKEN end_char="1531" id="token-17-17" morph="none" pos="word" start_char="1525">Testing</TOKEN>
<TOKEN end_char="1535" id="token-17-18" morph="none" pos="word" start_char="1533">for</TOKEN>
<TOKEN end_char="1544" id="token-17-19" morph="none" pos="unknown" start_char="1537">COVID-19</TOKEN>
<TOKEN end_char="1545" id="token-17-20" morph="none" pos="punct" start_char="1545">'</TOKEN>
<TOKEN end_char="1551" id="token-17-21" morph="none" pos="word" start_char="1547">filed</TOKEN>
<TOKEN end_char="1554" id="token-17-22" morph="none" pos="word" start_char="1553">by</TOKEN>
<TOKEN end_char="1565" id="token-17-23" morph="none" pos="word" start_char="1556">Rothschild</TOKEN>
<TOKEN end_char="1573" id="token-17-24" morph="none" pos="word" start_char="1567">Richard</TOKEN>
<TOKEN end_char="1574" id="token-17-25" morph="none" pos="punct" start_char="1574">.</TOKEN>
</SEG>
<SEG end_char="1758" id="segment-18" start_char="1577">
<ORIGINAL_TEXT>The search was also done on Espacenet and found that the filing is a "Continuation in part" (CIP) application for a US patent, i.e; a partial continuation of an existing application.</ORIGINAL_TEXT>
<TOKEN end_char="1579" id="token-18-0" morph="none" pos="word" start_char="1577">The</TOKEN>
<TOKEN end_char="1586" id="token-18-1" morph="none" pos="word" start_char="1581">search</TOKEN>
<TOKEN end_char="1590" id="token-18-2" morph="none" pos="word" start_char="1588">was</TOKEN>
<TOKEN end_char="1595" id="token-18-3" morph="none" pos="word" start_char="1592">also</TOKEN>
<TOKEN end_char="1600" id="token-18-4" morph="none" pos="word" start_char="1597">done</TOKEN>
<TOKEN end_char="1603" id="token-18-5" morph="none" pos="word" start_char="1602">on</TOKEN>
<TOKEN end_char="1613" id="token-18-6" morph="none" pos="word" start_char="1605">Espacenet</TOKEN>
<TOKEN end_char="1617" id="token-18-7" morph="none" pos="word" start_char="1615">and</TOKEN>
<TOKEN end_char="1623" id="token-18-8" morph="none" pos="word" start_char="1619">found</TOKEN>
<TOKEN end_char="1628" id="token-18-9" morph="none" pos="word" start_char="1625">that</TOKEN>
<TOKEN end_char="1632" id="token-18-10" morph="none" pos="word" start_char="1630">the</TOKEN>
<TOKEN end_char="1639" id="token-18-11" morph="none" pos="word" start_char="1634">filing</TOKEN>
<TOKEN end_char="1642" id="token-18-12" morph="none" pos="word" start_char="1641">is</TOKEN>
<TOKEN end_char="1644" id="token-18-13" morph="none" pos="word" start_char="1644">a</TOKEN>
<TOKEN end_char="1646" id="token-18-14" morph="none" pos="punct" start_char="1646">"</TOKEN>
<TOKEN end_char="1658" id="token-18-15" morph="none" pos="word" start_char="1647">Continuation</TOKEN>
<TOKEN end_char="1661" id="token-18-16" morph="none" pos="word" start_char="1660">in</TOKEN>
<TOKEN end_char="1666" id="token-18-17" morph="none" pos="word" start_char="1663">part</TOKEN>
<TOKEN end_char="1667" id="token-18-18" morph="none" pos="punct" start_char="1667">"</TOKEN>
<TOKEN end_char="1669" id="token-18-19" morph="none" pos="punct" start_char="1669">(</TOKEN>
<TOKEN end_char="1672" id="token-18-20" morph="none" pos="word" start_char="1670">CIP</TOKEN>
<TOKEN end_char="1673" id="token-18-21" morph="none" pos="punct" start_char="1673">)</TOKEN>
<TOKEN end_char="1685" id="token-18-22" morph="none" pos="word" start_char="1675">application</TOKEN>
<TOKEN end_char="1689" id="token-18-23" morph="none" pos="word" start_char="1687">for</TOKEN>
<TOKEN end_char="1691" id="token-18-24" morph="none" pos="word" start_char="1691">a</TOKEN>
<TOKEN end_char="1694" id="token-18-25" morph="none" pos="word" start_char="1693">US</TOKEN>
<TOKEN end_char="1701" id="token-18-26" morph="none" pos="word" start_char="1696">patent</TOKEN>
<TOKEN end_char="1702" id="token-18-27" morph="none" pos="punct" start_char="1702">,</TOKEN>
<TOKEN end_char="1706" id="token-18-28" morph="none" pos="unknown" start_char="1704">i.e</TOKEN>
<TOKEN end_char="1707" id="token-18-29" morph="none" pos="punct" start_char="1707">;</TOKEN>
<TOKEN end_char="1709" id="token-18-30" morph="none" pos="word" start_char="1709">a</TOKEN>
<TOKEN end_char="1717" id="token-18-31" morph="none" pos="word" start_char="1711">partial</TOKEN>
<TOKEN end_char="1730" id="token-18-32" morph="none" pos="word" start_char="1719">continuation</TOKEN>
<TOKEN end_char="1733" id="token-18-33" morph="none" pos="word" start_char="1732">of</TOKEN>
<TOKEN end_char="1736" id="token-18-34" morph="none" pos="word" start_char="1735">an</TOKEN>
<TOKEN end_char="1745" id="token-18-35" morph="none" pos="word" start_char="1738">existing</TOKEN>
<TOKEN end_char="1757" id="token-18-36" morph="none" pos="word" start_char="1747">application</TOKEN>
<TOKEN end_char="1758" id="token-18-37" morph="none" pos="punct" start_char="1758">.</TOKEN>
</SEG>
<SEG end_char="1873" id="segment-19" start_char="1761">
<ORIGINAL_TEXT>Espacenet is a website where one can search for patents and applications developed by the European Patent Office.</ORIGINAL_TEXT>
<TOKEN end_char="1769" id="token-19-0" morph="none" pos="word" start_char="1761">Espacenet</TOKEN>
<TOKEN end_char="1772" id="token-19-1" morph="none" pos="word" start_char="1771">is</TOKEN>
<TOKEN end_char="1774" id="token-19-2" morph="none" pos="word" start_char="1774">a</TOKEN>
<TOKEN end_char="1782" id="token-19-3" morph="none" pos="word" start_char="1776">website</TOKEN>
<TOKEN end_char="1788" id="token-19-4" morph="none" pos="word" start_char="1784">where</TOKEN>
<TOKEN end_char="1792" id="token-19-5" morph="none" pos="word" start_char="1790">one</TOKEN>
<TOKEN end_char="1796" id="token-19-6" morph="none" pos="word" start_char="1794">can</TOKEN>
<TOKEN end_char="1803" id="token-19-7" morph="none" pos="word" start_char="1798">search</TOKEN>
<TOKEN end_char="1807" id="token-19-8" morph="none" pos="word" start_char="1805">for</TOKEN>
<TOKEN end_char="1815" id="token-19-9" morph="none" pos="word" start_char="1809">patents</TOKEN>
<TOKEN end_char="1819" id="token-19-10" morph="none" pos="word" start_char="1817">and</TOKEN>
<TOKEN end_char="1832" id="token-19-11" morph="none" pos="word" start_char="1821">applications</TOKEN>
<TOKEN end_char="1842" id="token-19-12" morph="none" pos="word" start_char="1834">developed</TOKEN>
<TOKEN end_char="1845" id="token-19-13" morph="none" pos="word" start_char="1844">by</TOKEN>
<TOKEN end_char="1849" id="token-19-14" morph="none" pos="word" start_char="1847">the</TOKEN>
<TOKEN end_char="1858" id="token-19-15" morph="none" pos="word" start_char="1851">European</TOKEN>
<TOKEN end_char="1865" id="token-19-16" morph="none" pos="word" start_char="1860">Patent</TOKEN>
<TOKEN end_char="1872" id="token-19-17" morph="none" pos="word" start_char="1867">Office</TOKEN>
<TOKEN end_char="1873" id="token-19-18" morph="none" pos="punct" start_char="1873">.</TOKEN>
</SEG>
<SEG end_char="2019" id="segment-20" start_char="1875">
<ORIGINAL_TEXT>CIP allows an inventor to link a new patent with an older license, as long as the old invention is contributed to the development of the new one.</ORIGINAL_TEXT>
<TOKEN end_char="1877" id="token-20-0" morph="none" pos="word" start_char="1875">CIP</TOKEN>
<TOKEN end_char="1884" id="token-20-1" morph="none" pos="word" start_char="1879">allows</TOKEN>
<TOKEN end_char="1887" id="token-20-2" morph="none" pos="word" start_char="1886">an</TOKEN>
<TOKEN end_char="1896" id="token-20-3" morph="none" pos="word" start_char="1889">inventor</TOKEN>
<TOKEN end_char="1899" id="token-20-4" morph="none" pos="word" start_char="1898">to</TOKEN>
<TOKEN end_char="1904" id="token-20-5" morph="none" pos="word" start_char="1901">link</TOKEN>
<TOKEN end_char="1906" id="token-20-6" morph="none" pos="word" start_char="1906">a</TOKEN>
<TOKEN end_char="1910" id="token-20-7" morph="none" pos="word" start_char="1908">new</TOKEN>
<TOKEN end_char="1917" id="token-20-8" morph="none" pos="word" start_char="1912">patent</TOKEN>
<TOKEN end_char="1922" id="token-20-9" morph="none" pos="word" start_char="1919">with</TOKEN>
<TOKEN end_char="1925" id="token-20-10" morph="none" pos="word" start_char="1924">an</TOKEN>
<TOKEN end_char="1931" id="token-20-11" morph="none" pos="word" start_char="1927">older</TOKEN>
<TOKEN end_char="1939" id="token-20-12" morph="none" pos="word" start_char="1933">license</TOKEN>
<TOKEN end_char="1940" id="token-20-13" morph="none" pos="punct" start_char="1940">,</TOKEN>
<TOKEN end_char="1943" id="token-20-14" morph="none" pos="word" start_char="1942">as</TOKEN>
<TOKEN end_char="1948" id="token-20-15" morph="none" pos="word" start_char="1945">long</TOKEN>
<TOKEN end_char="1951" id="token-20-16" morph="none" pos="word" start_char="1950">as</TOKEN>
<TOKEN end_char="1955" id="token-20-17" morph="none" pos="word" start_char="1953">the</TOKEN>
<TOKEN end_char="1959" id="token-20-18" morph="none" pos="word" start_char="1957">old</TOKEN>
<TOKEN end_char="1969" id="token-20-19" morph="none" pos="word" start_char="1961">invention</TOKEN>
<TOKEN end_char="1972" id="token-20-20" morph="none" pos="word" start_char="1971">is</TOKEN>
<TOKEN end_char="1984" id="token-20-21" morph="none" pos="word" start_char="1974">contributed</TOKEN>
<TOKEN end_char="1987" id="token-20-22" morph="none" pos="word" start_char="1986">to</TOKEN>
<TOKEN end_char="1991" id="token-20-23" morph="none" pos="word" start_char="1989">the</TOKEN>
<TOKEN end_char="2003" id="token-20-24" morph="none" pos="word" start_char="1993">development</TOKEN>
<TOKEN end_char="2006" id="token-20-25" morph="none" pos="word" start_char="2005">of</TOKEN>
<TOKEN end_char="2010" id="token-20-26" morph="none" pos="word" start_char="2008">the</TOKEN>
<TOKEN end_char="2014" id="token-20-27" morph="none" pos="word" start_char="2012">new</TOKEN>
<TOKEN end_char="2018" id="token-20-28" morph="none" pos="word" start_char="2016">one</TOKEN>
<TOKEN end_char="2019" id="token-20-29" morph="none" pos="punct" start_char="2019">.</TOKEN>
</SEG>
<SEG end_char="2253" id="segment-21" start_char="2022">
<ORIGINAL_TEXT>Rothschild patent which is being noticed in the screenshot of the misleading Facebook posts, which says it was filed in 2013, is actually of "acquiring and transmitting biometric data" and not related to Covid-19 as claimed by post.</ORIGINAL_TEXT>
<TOKEN end_char="2031" id="token-21-0" morph="none" pos="word" start_char="2022">Rothschild</TOKEN>
<TOKEN end_char="2038" id="token-21-1" morph="none" pos="word" start_char="2033">patent</TOKEN>
<TOKEN end_char="2044" id="token-21-2" morph="none" pos="word" start_char="2040">which</TOKEN>
<TOKEN end_char="2047" id="token-21-3" morph="none" pos="word" start_char="2046">is</TOKEN>
<TOKEN end_char="2053" id="token-21-4" morph="none" pos="word" start_char="2049">being</TOKEN>
<TOKEN end_char="2061" id="token-21-5" morph="none" pos="word" start_char="2055">noticed</TOKEN>
<TOKEN end_char="2064" id="token-21-6" morph="none" pos="word" start_char="2063">in</TOKEN>
<TOKEN end_char="2068" id="token-21-7" morph="none" pos="word" start_char="2066">the</TOKEN>
<TOKEN end_char="2079" id="token-21-8" morph="none" pos="word" start_char="2070">screenshot</TOKEN>
<TOKEN end_char="2082" id="token-21-9" morph="none" pos="word" start_char="2081">of</TOKEN>
<TOKEN end_char="2086" id="token-21-10" morph="none" pos="word" start_char="2084">the</TOKEN>
<TOKEN end_char="2097" id="token-21-11" morph="none" pos="word" start_char="2088">misleading</TOKEN>
<TOKEN end_char="2106" id="token-21-12" morph="none" pos="word" start_char="2099">Facebook</TOKEN>
<TOKEN end_char="2112" id="token-21-13" morph="none" pos="word" start_char="2108">posts</TOKEN>
<TOKEN end_char="2113" id="token-21-14" morph="none" pos="punct" start_char="2113">,</TOKEN>
<TOKEN end_char="2119" id="token-21-15" morph="none" pos="word" start_char="2115">which</TOKEN>
<TOKEN end_char="2124" id="token-21-16" morph="none" pos="word" start_char="2121">says</TOKEN>
<TOKEN end_char="2127" id="token-21-17" morph="none" pos="word" start_char="2126">it</TOKEN>
<TOKEN end_char="2131" id="token-21-18" morph="none" pos="word" start_char="2129">was</TOKEN>
<TOKEN end_char="2137" id="token-21-19" morph="none" pos="word" start_char="2133">filed</TOKEN>
<TOKEN end_char="2140" id="token-21-20" morph="none" pos="word" start_char="2139">in</TOKEN>
<TOKEN end_char="2145" id="token-21-21" morph="none" pos="word" start_char="2142">2013</TOKEN>
<TOKEN end_char="2146" id="token-21-22" morph="none" pos="punct" start_char="2146">,</TOKEN>
<TOKEN end_char="2149" id="token-21-23" morph="none" pos="word" start_char="2148">is</TOKEN>
<TOKEN end_char="2158" id="token-21-24" morph="none" pos="word" start_char="2151">actually</TOKEN>
<TOKEN end_char="2161" id="token-21-25" morph="none" pos="word" start_char="2160">of</TOKEN>
<TOKEN end_char="2163" id="token-21-26" morph="none" pos="punct" start_char="2163">"</TOKEN>
<TOKEN end_char="2172" id="token-21-27" morph="none" pos="word" start_char="2164">acquiring</TOKEN>
<TOKEN end_char="2176" id="token-21-28" morph="none" pos="word" start_char="2174">and</TOKEN>
<TOKEN end_char="2189" id="token-21-29" morph="none" pos="word" start_char="2178">transmitting</TOKEN>
<TOKEN end_char="2199" id="token-21-30" morph="none" pos="word" start_char="2191">biometric</TOKEN>
<TOKEN end_char="2204" id="token-21-31" morph="none" pos="word" start_char="2201">data</TOKEN>
<TOKEN end_char="2205" id="token-21-32" morph="none" pos="punct" start_char="2205">"</TOKEN>
<TOKEN end_char="2209" id="token-21-33" morph="none" pos="word" start_char="2207">and</TOKEN>
<TOKEN end_char="2213" id="token-21-34" morph="none" pos="word" start_char="2211">not</TOKEN>
<TOKEN end_char="2221" id="token-21-35" morph="none" pos="word" start_char="2215">related</TOKEN>
<TOKEN end_char="2224" id="token-21-36" morph="none" pos="word" start_char="2223">to</TOKEN>
<TOKEN end_char="2233" id="token-21-37" morph="none" pos="unknown" start_char="2226">Covid-19</TOKEN>
<TOKEN end_char="2236" id="token-21-38" morph="none" pos="word" start_char="2235">as</TOKEN>
<TOKEN end_char="2244" id="token-21-39" morph="none" pos="word" start_char="2238">claimed</TOKEN>
<TOKEN end_char="2247" id="token-21-40" morph="none" pos="word" start_char="2246">by</TOKEN>
<TOKEN end_char="2252" id="token-21-41" morph="none" pos="word" start_char="2249">post</TOKEN>
<TOKEN end_char="2253" id="token-21-42" morph="none" pos="punct" start_char="2253">.</TOKEN>
</SEG>
<SEG end_char="2453" id="segment-22" start_char="2255">
<ORIGINAL_TEXT>Later, CIP application was submitted in May 2020 because it was claimed that the biometric data can also be used to "determine whether the user is suffering from a viral infection, such as COVID-19."</ORIGINAL_TEXT>
<TOKEN end_char="2259" id="token-22-0" morph="none" pos="word" start_char="2255">Later</TOKEN>
<TOKEN end_char="2260" id="token-22-1" morph="none" pos="punct" start_char="2260">,</TOKEN>
<TOKEN end_char="2264" id="token-22-2" morph="none" pos="word" start_char="2262">CIP</TOKEN>
<TOKEN end_char="2276" id="token-22-3" morph="none" pos="word" start_char="2266">application</TOKEN>
<TOKEN end_char="2280" id="token-22-4" morph="none" pos="word" start_char="2278">was</TOKEN>
<TOKEN end_char="2290" id="token-22-5" morph="none" pos="word" start_char="2282">submitted</TOKEN>
<TOKEN end_char="2293" id="token-22-6" morph="none" pos="word" start_char="2292">in</TOKEN>
<TOKEN end_char="2297" id="token-22-7" morph="none" pos="word" start_char="2295">May</TOKEN>
<TOKEN end_char="2302" id="token-22-8" morph="none" pos="word" start_char="2299">2020</TOKEN>
<TOKEN end_char="2310" id="token-22-9" morph="none" pos="word" start_char="2304">because</TOKEN>
<TOKEN end_char="2313" id="token-22-10" morph="none" pos="word" start_char="2312">it</TOKEN>
<TOKEN end_char="2317" id="token-22-11" morph="none" pos="word" start_char="2315">was</TOKEN>
<TOKEN end_char="2325" id="token-22-12" morph="none" pos="word" start_char="2319">claimed</TOKEN>
<TOKEN end_char="2330" id="token-22-13" morph="none" pos="word" start_char="2327">that</TOKEN>
<TOKEN end_char="2334" id="token-22-14" morph="none" pos="word" start_char="2332">the</TOKEN>
<TOKEN end_char="2344" id="token-22-15" morph="none" pos="word" start_char="2336">biometric</TOKEN>
<TOKEN end_char="2349" id="token-22-16" morph="none" pos="word" start_char="2346">data</TOKEN>
<TOKEN end_char="2353" id="token-22-17" morph="none" pos="word" start_char="2351">can</TOKEN>
<TOKEN end_char="2358" id="token-22-18" morph="none" pos="word" start_char="2355">also</TOKEN>
<TOKEN end_char="2361" id="token-22-19" morph="none" pos="word" start_char="2360">be</TOKEN>
<TOKEN end_char="2366" id="token-22-20" morph="none" pos="word" start_char="2363">used</TOKEN>
<TOKEN end_char="2369" id="token-22-21" morph="none" pos="word" start_char="2368">to</TOKEN>
<TOKEN end_char="2371" id="token-22-22" morph="none" pos="punct" start_char="2371">"</TOKEN>
<TOKEN end_char="2380" id="token-22-23" morph="none" pos="word" start_char="2372">determine</TOKEN>
<TOKEN end_char="2388" id="token-22-24" morph="none" pos="word" start_char="2382">whether</TOKEN>
<TOKEN end_char="2392" id="token-22-25" morph="none" pos="word" start_char="2390">the</TOKEN>
<TOKEN end_char="2397" id="token-22-26" morph="none" pos="word" start_char="2394">user</TOKEN>
<TOKEN end_char="2400" id="token-22-27" morph="none" pos="word" start_char="2399">is</TOKEN>
<TOKEN end_char="2410" id="token-22-28" morph="none" pos="word" start_char="2402">suffering</TOKEN>
<TOKEN end_char="2415" id="token-22-29" morph="none" pos="word" start_char="2412">from</TOKEN>
<TOKEN end_char="2417" id="token-22-30" morph="none" pos="word" start_char="2417">a</TOKEN>
<TOKEN end_char="2423" id="token-22-31" morph="none" pos="word" start_char="2419">viral</TOKEN>
<TOKEN end_char="2433" id="token-22-32" morph="none" pos="word" start_char="2425">infection</TOKEN>
<TOKEN end_char="2434" id="token-22-33" morph="none" pos="punct" start_char="2434">,</TOKEN>
<TOKEN end_char="2439" id="token-22-34" morph="none" pos="word" start_char="2436">such</TOKEN>
<TOKEN end_char="2442" id="token-22-35" morph="none" pos="word" start_char="2441">as</TOKEN>
<TOKEN end_char="2451" id="token-22-36" morph="none" pos="unknown" start_char="2444">COVID-19</TOKEN>
<TOKEN end_char="2453" id="token-22-37" morph="none" pos="punct" start_char="2452">."</TOKEN>
</SEG>
<SEG end_char="2613" id="segment-23" start_char="2456">
<ORIGINAL_TEXT>According to an AFP report, a European Patent Office spokesperson, Rainer Osterwalder said, "the patent application had no reference to COVID-19 before 2020."</ORIGINAL_TEXT>
<TOKEN end_char="2464" id="token-23-0" morph="none" pos="word" start_char="2456">According</TOKEN>
<TOKEN end_char="2467" id="token-23-1" morph="none" pos="word" start_char="2466">to</TOKEN>
<TOKEN end_char="2470" id="token-23-2" morph="none" pos="word" start_char="2469">an</TOKEN>
<TOKEN end_char="2474" id="token-23-3" morph="none" pos="word" start_char="2472">AFP</TOKEN>
<TOKEN end_char="2481" id="token-23-4" morph="none" pos="word" start_char="2476">report</TOKEN>
<TOKEN end_char="2482" id="token-23-5" morph="none" pos="punct" start_char="2482">,</TOKEN>
<TOKEN end_char="2484" id="token-23-6" morph="none" pos="word" start_char="2484">a</TOKEN>
<TOKEN end_char="2493" id="token-23-7" morph="none" pos="word" start_char="2486">European</TOKEN>
<TOKEN end_char="2500" id="token-23-8" morph="none" pos="word" start_char="2495">Patent</TOKEN>
<TOKEN end_char="2507" id="token-23-9" morph="none" pos="word" start_char="2502">Office</TOKEN>
<TOKEN end_char="2520" id="token-23-10" morph="none" pos="word" start_char="2509">spokesperson</TOKEN>
<TOKEN end_char="2521" id="token-23-11" morph="none" pos="punct" start_char="2521">,</TOKEN>
<TOKEN end_char="2528" id="token-23-12" morph="none" pos="word" start_char="2523">Rainer</TOKEN>
<TOKEN end_char="2540" id="token-23-13" morph="none" pos="word" start_char="2530">Osterwalder</TOKEN>
<TOKEN end_char="2545" id="token-23-14" morph="none" pos="word" start_char="2542">said</TOKEN>
<TOKEN end_char="2546" id="token-23-15" morph="none" pos="punct" start_char="2546">,</TOKEN>
<TOKEN end_char="2548" id="token-23-16" morph="none" pos="punct" start_char="2548">"</TOKEN>
<TOKEN end_char="2551" id="token-23-17" morph="none" pos="word" start_char="2549">the</TOKEN>
<TOKEN end_char="2558" id="token-23-18" morph="none" pos="word" start_char="2553">patent</TOKEN>
<TOKEN end_char="2570" id="token-23-19" morph="none" pos="word" start_char="2560">application</TOKEN>
<TOKEN end_char="2574" id="token-23-20" morph="none" pos="word" start_char="2572">had</TOKEN>
<TOKEN end_char="2577" id="token-23-21" morph="none" pos="word" start_char="2576">no</TOKEN>
<TOKEN end_char="2587" id="token-23-22" morph="none" pos="word" start_char="2579">reference</TOKEN>
<TOKEN end_char="2590" id="token-23-23" morph="none" pos="word" start_char="2589">to</TOKEN>
<TOKEN end_char="2599" id="token-23-24" morph="none" pos="unknown" start_char="2592">COVID-19</TOKEN>
<TOKEN end_char="2606" id="token-23-25" morph="none" pos="word" start_char="2601">before</TOKEN>
<TOKEN end_char="2611" id="token-23-26" morph="none" pos="word" start_char="2608">2020</TOKEN>
<TOKEN end_char="2613" id="token-23-27" morph="none" pos="punct" start_char="2612">."</TOKEN>
</SEG>
<SEG end_char="2686" id="segment-24" start_char="2616">
<ORIGINAL_TEXT>"In the first disclosed registration of 2016, this is about video data.</ORIGINAL_TEXT>
<TOKEN end_char="2616" id="token-24-0" morph="none" pos="punct" start_char="2616">"</TOKEN>
<TOKEN end_char="2618" id="token-24-1" morph="none" pos="word" start_char="2617">In</TOKEN>
<TOKEN end_char="2622" id="token-24-2" morph="none" pos="word" start_char="2620">the</TOKEN>
<TOKEN end_char="2628" id="token-24-3" morph="none" pos="word" start_char="2624">first</TOKEN>
<TOKEN end_char="2638" id="token-24-4" morph="none" pos="word" start_char="2630">disclosed</TOKEN>
<TOKEN end_char="2651" id="token-24-5" morph="none" pos="word" start_char="2640">registration</TOKEN>
<TOKEN end_char="2654" id="token-24-6" morph="none" pos="word" start_char="2653">of</TOKEN>
<TOKEN end_char="2659" id="token-24-7" morph="none" pos="word" start_char="2656">2016</TOKEN>
<TOKEN end_char="2660" id="token-24-8" morph="none" pos="punct" start_char="2660">,</TOKEN>
<TOKEN end_char="2665" id="token-24-9" morph="none" pos="word" start_char="2662">this</TOKEN>
<TOKEN end_char="2668" id="token-24-10" morph="none" pos="word" start_char="2667">is</TOKEN>
<TOKEN end_char="2674" id="token-24-11" morph="none" pos="word" start_char="2670">about</TOKEN>
<TOKEN end_char="2680" id="token-24-12" morph="none" pos="word" start_char="2676">video</TOKEN>
<TOKEN end_char="2685" id="token-24-13" morph="none" pos="word" start_char="2682">data</TOKEN>
<TOKEN end_char="2686" id="token-24-14" morph="none" pos="punct" start_char="2686">.</TOKEN>
</SEG>
<SEG end_char="2808" id="segment-25" start_char="2688">
<ORIGINAL_TEXT>In the subsequent applications of 2016/2017, the collection, processing and transmission of biometric data was specified.</ORIGINAL_TEXT>
<TOKEN end_char="2689" id="token-25-0" morph="none" pos="word" start_char="2688">In</TOKEN>
<TOKEN end_char="2693" id="token-25-1" morph="none" pos="word" start_char="2691">the</TOKEN>
<TOKEN end_char="2704" id="token-25-2" morph="none" pos="word" start_char="2695">subsequent</TOKEN>
<TOKEN end_char="2717" id="token-25-3" morph="none" pos="word" start_char="2706">applications</TOKEN>
<TOKEN end_char="2720" id="token-25-4" morph="none" pos="word" start_char="2719">of</TOKEN>
<TOKEN end_char="2730" id="token-25-5" morph="none" pos="unknown" start_char="2722">2016/2017</TOKEN>
<TOKEN end_char="2731" id="token-25-6" morph="none" pos="punct" start_char="2731">,</TOKEN>
<TOKEN end_char="2735" id="token-25-7" morph="none" pos="word" start_char="2733">the</TOKEN>
<TOKEN end_char="2746" id="token-25-8" morph="none" pos="word" start_char="2737">collection</TOKEN>
<TOKEN end_char="2747" id="token-25-9" morph="none" pos="punct" start_char="2747">,</TOKEN>
<TOKEN end_char="2758" id="token-25-10" morph="none" pos="word" start_char="2749">processing</TOKEN>
<TOKEN end_char="2762" id="token-25-11" morph="none" pos="word" start_char="2760">and</TOKEN>
<TOKEN end_char="2775" id="token-25-12" morph="none" pos="word" start_char="2764">transmission</TOKEN>
<TOKEN end_char="2778" id="token-25-13" morph="none" pos="word" start_char="2777">of</TOKEN>
<TOKEN end_char="2788" id="token-25-14" morph="none" pos="word" start_char="2780">biometric</TOKEN>
<TOKEN end_char="2793" id="token-25-15" morph="none" pos="word" start_char="2790">data</TOKEN>
<TOKEN end_char="2797" id="token-25-16" morph="none" pos="word" start_char="2795">was</TOKEN>
<TOKEN end_char="2807" id="token-25-17" morph="none" pos="word" start_char="2799">specified</TOKEN>
<TOKEN end_char="2808" id="token-25-18" morph="none" pos="punct" start_char="2808">.</TOKEN>
</SEG>
<SEG end_char="3048" id="segment-26" start_char="2810">
<ORIGINAL_TEXT>The first application from 2015 that you mentioned was never disclosed, but for patent law reasons it must also have referred to the mentioned inventions (otherwise a 'continuation in part' would not be possible)," Osterwalder said to AFP.</ORIGINAL_TEXT>
<TOKEN end_char="2812" id="token-26-0" morph="none" pos="word" start_char="2810">The</TOKEN>
<TOKEN end_char="2818" id="token-26-1" morph="none" pos="word" start_char="2814">first</TOKEN>
<TOKEN end_char="2830" id="token-26-2" morph="none" pos="word" start_char="2820">application</TOKEN>
<TOKEN end_char="2835" id="token-26-3" morph="none" pos="word" start_char="2832">from</TOKEN>
<TOKEN end_char="2840" id="token-26-4" morph="none" pos="word" start_char="2837">2015</TOKEN>
<TOKEN end_char="2845" id="token-26-5" morph="none" pos="word" start_char="2842">that</TOKEN>
<TOKEN end_char="2849" id="token-26-6" morph="none" pos="word" start_char="2847">you</TOKEN>
<TOKEN end_char="2859" id="token-26-7" morph="none" pos="word" start_char="2851">mentioned</TOKEN>
<TOKEN end_char="2863" id="token-26-8" morph="none" pos="word" start_char="2861">was</TOKEN>
<TOKEN end_char="2869" id="token-26-9" morph="none" pos="word" start_char="2865">never</TOKEN>
<TOKEN end_char="2879" id="token-26-10" morph="none" pos="word" start_char="2871">disclosed</TOKEN>
<TOKEN end_char="2880" id="token-26-11" morph="none" pos="punct" start_char="2880">,</TOKEN>
<TOKEN end_char="2884" id="token-26-12" morph="none" pos="word" start_char="2882">but</TOKEN>
<TOKEN end_char="2888" id="token-26-13" morph="none" pos="word" start_char="2886">for</TOKEN>
<TOKEN end_char="2895" id="token-26-14" morph="none" pos="word" start_char="2890">patent</TOKEN>
<TOKEN end_char="2899" id="token-26-15" morph="none" pos="word" start_char="2897">law</TOKEN>
<TOKEN end_char="2907" id="token-26-16" morph="none" pos="word" start_char="2901">reasons</TOKEN>
<TOKEN end_char="2910" id="token-26-17" morph="none" pos="word" start_char="2909">it</TOKEN>
<TOKEN end_char="2915" id="token-26-18" morph="none" pos="word" start_char="2912">must</TOKEN>
<TOKEN end_char="2920" id="token-26-19" morph="none" pos="word" start_char="2917">also</TOKEN>
<TOKEN end_char="2925" id="token-26-20" morph="none" pos="word" start_char="2922">have</TOKEN>
<TOKEN end_char="2934" id="token-26-21" morph="none" pos="word" start_char="2927">referred</TOKEN>
<TOKEN end_char="2937" id="token-26-22" morph="none" pos="word" start_char="2936">to</TOKEN>
<TOKEN end_char="2941" id="token-26-23" morph="none" pos="word" start_char="2939">the</TOKEN>
<TOKEN end_char="2951" id="token-26-24" morph="none" pos="word" start_char="2943">mentioned</TOKEN>
<TOKEN end_char="2962" id="token-26-25" morph="none" pos="word" start_char="2953">inventions</TOKEN>
<TOKEN end_char="2964" id="token-26-26" morph="none" pos="punct" start_char="2964">(</TOKEN>
<TOKEN end_char="2973" id="token-26-27" morph="none" pos="word" start_char="2965">otherwise</TOKEN>
<TOKEN end_char="2975" id="token-26-28" morph="none" pos="word" start_char="2975">a</TOKEN>
<TOKEN end_char="2977" id="token-26-29" morph="none" pos="punct" start_char="2977">'</TOKEN>
<TOKEN end_char="2989" id="token-26-30" morph="none" pos="word" start_char="2978">continuation</TOKEN>
<TOKEN end_char="2992" id="token-26-31" morph="none" pos="word" start_char="2991">in</TOKEN>
<TOKEN end_char="2997" id="token-26-32" morph="none" pos="word" start_char="2994">part</TOKEN>
<TOKEN end_char="2998" id="token-26-33" morph="none" pos="punct" start_char="2998">'</TOKEN>
<TOKEN end_char="3004" id="token-26-34" morph="none" pos="word" start_char="3000">would</TOKEN>
<TOKEN end_char="3008" id="token-26-35" morph="none" pos="word" start_char="3006">not</TOKEN>
<TOKEN end_char="3011" id="token-26-36" morph="none" pos="word" start_char="3010">be</TOKEN>
<TOKEN end_char="3020" id="token-26-37" morph="none" pos="word" start_char="3013">possible</TOKEN>
<TOKEN end_char="3023" id="token-26-38" morph="none" pos="punct" start_char="3021">),"</TOKEN>
<TOKEN end_char="3035" id="token-26-39" morph="none" pos="word" start_char="3025">Osterwalder</TOKEN>
<TOKEN end_char="3040" id="token-26-40" morph="none" pos="word" start_char="3037">said</TOKEN>
<TOKEN end_char="3043" id="token-26-41" morph="none" pos="word" start_char="3042">to</TOKEN>
<TOKEN end_char="3047" id="token-26-42" morph="none" pos="word" start_char="3045">AFP</TOKEN>
<TOKEN end_char="3048" id="token-26-43" morph="none" pos="punct" start_char="3048">.</TOKEN>
</SEG>
<SEG end_char="3170" id="segment-27" start_char="3051">
<ORIGINAL_TEXT>Also, on Google patent, it was observed that the recent application for patent is in pending state and not yet approved.</ORIGINAL_TEXT>
<TOKEN end_char="3054" id="token-27-0" morph="none" pos="word" start_char="3051">Also</TOKEN>
<TOKEN end_char="3055" id="token-27-1" morph="none" pos="punct" start_char="3055">,</TOKEN>
<TOKEN end_char="3058" id="token-27-2" morph="none" pos="word" start_char="3057">on</TOKEN>
<TOKEN end_char="3065" id="token-27-3" morph="none" pos="word" start_char="3060">Google</TOKEN>
<TOKEN end_char="3072" id="token-27-4" morph="none" pos="word" start_char="3067">patent</TOKEN>
<TOKEN end_char="3073" id="token-27-5" morph="none" pos="punct" start_char="3073">,</TOKEN>
<TOKEN end_char="3076" id="token-27-6" morph="none" pos="word" start_char="3075">it</TOKEN>
<TOKEN end_char="3080" id="token-27-7" morph="none" pos="word" start_char="3078">was</TOKEN>
<TOKEN end_char="3089" id="token-27-8" morph="none" pos="word" start_char="3082">observed</TOKEN>
<TOKEN end_char="3094" id="token-27-9" morph="none" pos="word" start_char="3091">that</TOKEN>
<TOKEN end_char="3098" id="token-27-10" morph="none" pos="word" start_char="3096">the</TOKEN>
<TOKEN end_char="3105" id="token-27-11" morph="none" pos="word" start_char="3100">recent</TOKEN>
<TOKEN end_char="3117" id="token-27-12" morph="none" pos="word" start_char="3107">application</TOKEN>
<TOKEN end_char="3121" id="token-27-13" morph="none" pos="word" start_char="3119">for</TOKEN>
<TOKEN end_char="3128" id="token-27-14" morph="none" pos="word" start_char="3123">patent</TOKEN>
<TOKEN end_char="3131" id="token-27-15" morph="none" pos="word" start_char="3130">is</TOKEN>
<TOKEN end_char="3134" id="token-27-16" morph="none" pos="word" start_char="3133">in</TOKEN>
<TOKEN end_char="3142" id="token-27-17" morph="none" pos="word" start_char="3136">pending</TOKEN>
<TOKEN end_char="3148" id="token-27-18" morph="none" pos="word" start_char="3144">state</TOKEN>
<TOKEN end_char="3152" id="token-27-19" morph="none" pos="word" start_char="3150">and</TOKEN>
<TOKEN end_char="3156" id="token-27-20" morph="none" pos="word" start_char="3154">not</TOKEN>
<TOKEN end_char="3160" id="token-27-21" morph="none" pos="word" start_char="3158">yet</TOKEN>
<TOKEN end_char="3169" id="token-27-22" morph="none" pos="word" start_char="3162">approved</TOKEN>
<TOKEN end_char="3170" id="token-27-23" morph="none" pos="punct" start_char="3170">.</TOKEN>
</SEG>
<SEG end_char="3323" id="segment-28" start_char="3173">
<ORIGINAL_TEXT>Thus, it was clear that an old patent by Rothschild is being linked to the current situation of Covid-19 and is being presented with the wrong context.</ORIGINAL_TEXT>
<TOKEN end_char="3176" id="token-28-0" morph="none" pos="word" start_char="3173">Thus</TOKEN>
<TOKEN end_char="3177" id="token-28-1" morph="none" pos="punct" start_char="3177">,</TOKEN>
<TOKEN end_char="3180" id="token-28-2" morph="none" pos="word" start_char="3179">it</TOKEN>
<TOKEN end_char="3184" id="token-28-3" morph="none" pos="word" start_char="3182">was</TOKEN>
<TOKEN end_char="3190" id="token-28-4" morph="none" pos="word" start_char="3186">clear</TOKEN>
<TOKEN end_char="3195" id="token-28-5" morph="none" pos="word" start_char="3192">that</TOKEN>
<TOKEN end_char="3198" id="token-28-6" morph="none" pos="word" start_char="3197">an</TOKEN>
<TOKEN end_char="3202" id="token-28-7" morph="none" pos="word" start_char="3200">old</TOKEN>
<TOKEN end_char="3209" id="token-28-8" morph="none" pos="word" start_char="3204">patent</TOKEN>
<TOKEN end_char="3212" id="token-28-9" morph="none" pos="word" start_char="3211">by</TOKEN>
<TOKEN end_char="3223" id="token-28-10" morph="none" pos="word" start_char="3214">Rothschild</TOKEN>
<TOKEN end_char="3226" id="token-28-11" morph="none" pos="word" start_char="3225">is</TOKEN>
<TOKEN end_char="3232" id="token-28-12" morph="none" pos="word" start_char="3228">being</TOKEN>
<TOKEN end_char="3239" id="token-28-13" morph="none" pos="word" start_char="3234">linked</TOKEN>
<TOKEN end_char="3242" id="token-28-14" morph="none" pos="word" start_char="3241">to</TOKEN>
<TOKEN end_char="3246" id="token-28-15" morph="none" pos="word" start_char="3244">the</TOKEN>
<TOKEN end_char="3254" id="token-28-16" morph="none" pos="word" start_char="3248">current</TOKEN>
<TOKEN end_char="3264" id="token-28-17" morph="none" pos="word" start_char="3256">situation</TOKEN>
<TOKEN end_char="3267" id="token-28-18" morph="none" pos="word" start_char="3266">of</TOKEN>
<TOKEN end_char="3276" id="token-28-19" morph="none" pos="unknown" start_char="3269">Covid-19</TOKEN>
<TOKEN end_char="3280" id="token-28-20" morph="none" pos="word" start_char="3278">and</TOKEN>
<TOKEN end_char="3283" id="token-28-21" morph="none" pos="word" start_char="3282">is</TOKEN>
<TOKEN end_char="3289" id="token-28-22" morph="none" pos="word" start_char="3285">being</TOKEN>
<TOKEN end_char="3299" id="token-28-23" morph="none" pos="word" start_char="3291">presented</TOKEN>
<TOKEN end_char="3304" id="token-28-24" morph="none" pos="word" start_char="3301">with</TOKEN>
<TOKEN end_char="3308" id="token-28-25" morph="none" pos="word" start_char="3306">the</TOKEN>
<TOKEN end_char="3314" id="token-28-26" morph="none" pos="word" start_char="3310">wrong</TOKEN>
<TOKEN end_char="3322" id="token-28-27" morph="none" pos="word" start_char="3316">context</TOKEN>
<TOKEN end_char="3323" id="token-28-28" morph="none" pos="punct" start_char="3323">.</TOKEN>
</SEG>
<SEG end_char="3349" id="segment-29" start_char="3325">
<ORIGINAL_TEXT>Thus, the claim is false.</ORIGINAL_TEXT>
<TOKEN end_char="3328" id="token-29-0" morph="none" pos="word" start_char="3325">Thus</TOKEN>
<TOKEN end_char="3329" id="token-29-1" morph="none" pos="punct" start_char="3329">,</TOKEN>
<TOKEN end_char="3333" id="token-29-2" morph="none" pos="word" start_char="3331">the</TOKEN>
<TOKEN end_char="3339" id="token-29-3" morph="none" pos="word" start_char="3335">claim</TOKEN>
<TOKEN end_char="3342" id="token-29-4" morph="none" pos="word" start_char="3341">is</TOKEN>
<TOKEN end_char="3348" id="token-29-5" morph="none" pos="word" start_char="3344">false</TOKEN>
<TOKEN end_char="3349" id="token-29-6" morph="none" pos="punct" start_char="3349">.</TOKEN>
</SEG>
<SEG end_char="3491" id="segment-30" start_char="3352">
<ORIGINAL_TEXT>If you have any news that you believe needs to be fact-checked, please email us at factcheck@thelogicalindian.com or WhatsApp at 6364000343.</ORIGINAL_TEXT>
<TOKEN end_char="3353" id="token-30-0" morph="none" pos="word" start_char="3352">If</TOKEN>
<TOKEN end_char="3357" id="token-30-1" morph="none" pos="word" start_char="3355">you</TOKEN>
<TOKEN end_char="3362" id="token-30-2" morph="none" pos="word" start_char="3359">have</TOKEN>
<TOKEN end_char="3366" id="token-30-3" morph="none" pos="word" start_char="3364">any</TOKEN>
<TOKEN end_char="3371" id="token-30-4" morph="none" pos="word" start_char="3368">news</TOKEN>
<TOKEN end_char="3376" id="token-30-5" morph="none" pos="word" start_char="3373">that</TOKEN>
<TOKEN end_char="3380" id="token-30-6" morph="none" pos="word" start_char="3378">you</TOKEN>
<TOKEN end_char="3388" id="token-30-7" morph="none" pos="word" start_char="3382">believe</TOKEN>
<TOKEN end_char="3394" id="token-30-8" morph="none" pos="word" start_char="3390">needs</TOKEN>
<TOKEN end_char="3397" id="token-30-9" morph="none" pos="word" start_char="3396">to</TOKEN>
<TOKEN end_char="3400" id="token-30-10" morph="none" pos="word" start_char="3399">be</TOKEN>
<TOKEN end_char="3413" id="token-30-11" morph="none" pos="unknown" start_char="3402">fact-checked</TOKEN>
<TOKEN end_char="3414" id="token-30-12" morph="none" pos="punct" start_char="3414">,</TOKEN>
<TOKEN end_char="3421" id="token-30-13" morph="none" pos="word" start_char="3416">please</TOKEN>
<TOKEN end_char="3427" id="token-30-14" morph="none" pos="word" start_char="3423">email</TOKEN>
<TOKEN end_char="3430" id="token-30-15" morph="none" pos="word" start_char="3429">us</TOKEN>
<TOKEN end_char="3433" id="token-30-16" morph="none" pos="word" start_char="3432">at</TOKEN>
<TOKEN end_char="3464" id="token-30-17" morph="none" pos="unknown" start_char="3435">factcheck@thelogicalindian.com</TOKEN>
<TOKEN end_char="3467" id="token-30-18" morph="none" pos="word" start_char="3466">or</TOKEN>
<TOKEN end_char="3476" id="token-30-19" morph="none" pos="word" start_char="3469">WhatsApp</TOKEN>
<TOKEN end_char="3479" id="token-30-20" morph="none" pos="word" start_char="3478">at</TOKEN>
<TOKEN end_char="3490" id="token-30-21" morph="none" pos="word" start_char="3481">6364000343</TOKEN>
<TOKEN end_char="3491" id="token-30-22" morph="none" pos="punct" start_char="3491">.</TOKEN>
</SEG>
<SEG end_char="3643" id="segment-31" start_char="3495">
<ORIGINAL_TEXT>Claim Review : The provisional application for system and method for testing Covid-19 was already filed in 2015, 5 years before coronavirus pandemic.</ORIGINAL_TEXT>
<TOKEN end_char="3499" id="token-31-0" morph="none" pos="word" start_char="3495">Claim</TOKEN>
<TOKEN end_char="3506" id="token-31-1" morph="none" pos="word" start_char="3501">Review</TOKEN>
<TOKEN end_char="3508" id="token-31-2" morph="none" pos="punct" start_char="3508">:</TOKEN>
<TOKEN end_char="3512" id="token-31-3" morph="none" pos="word" start_char="3510">The</TOKEN>
<TOKEN end_char="3524" id="token-31-4" morph="none" pos="word" start_char="3514">provisional</TOKEN>
<TOKEN end_char="3536" id="token-31-5" morph="none" pos="word" start_char="3526">application</TOKEN>
<TOKEN end_char="3540" id="token-31-6" morph="none" pos="word" start_char="3538">for</TOKEN>
<TOKEN end_char="3547" id="token-31-7" morph="none" pos="word" start_char="3542">system</TOKEN>
<TOKEN end_char="3551" id="token-31-8" morph="none" pos="word" start_char="3549">and</TOKEN>
<TOKEN end_char="3558" id="token-31-9" morph="none" pos="word" start_char="3553">method</TOKEN>
<TOKEN end_char="3562" id="token-31-10" morph="none" pos="word" start_char="3560">for</TOKEN>
<TOKEN end_char="3570" id="token-31-11" morph="none" pos="word" start_char="3564">testing</TOKEN>
<TOKEN end_char="3579" id="token-31-12" morph="none" pos="unknown" start_char="3572">Covid-19</TOKEN>
<TOKEN end_char="3583" id="token-31-13" morph="none" pos="word" start_char="3581">was</TOKEN>
<TOKEN end_char="3591" id="token-31-14" morph="none" pos="word" start_char="3585">already</TOKEN>
<TOKEN end_char="3597" id="token-31-15" morph="none" pos="word" start_char="3593">filed</TOKEN>
<TOKEN end_char="3600" id="token-31-16" morph="none" pos="word" start_char="3599">in</TOKEN>
<TOKEN end_char="3605" id="token-31-17" morph="none" pos="word" start_char="3602">2015</TOKEN>
<TOKEN end_char="3606" id="token-31-18" morph="none" pos="punct" start_char="3606">,</TOKEN>
<TOKEN end_char="3608" id="token-31-19" morph="none" pos="word" start_char="3608">5</TOKEN>
<TOKEN end_char="3614" id="token-31-20" morph="none" pos="word" start_char="3610">years</TOKEN>
<TOKEN end_char="3621" id="token-31-21" morph="none" pos="word" start_char="3616">before</TOKEN>
<TOKEN end_char="3633" id="token-31-22" morph="none" pos="word" start_char="3623">coronavirus</TOKEN>
<TOKEN end_char="3642" id="token-31-23" morph="none" pos="word" start_char="3635">pandemic</TOKEN>
<TOKEN end_char="3643" id="token-31-24" morph="none" pos="punct" start_char="3643">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>